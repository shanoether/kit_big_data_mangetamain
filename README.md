# ğŸ½ï¸ Mangetamain, Garde l'Autre Pour Demain

> *"Eat this one, save the other for tomorrow!"*

**Mangetamain** is a leader in B2C recipe recommendations powered by massive data analytics. We're sharing our best insights with the world through an interactive web platform where everyone can discover what makes recipes delicious... or not!
ğŸŒ **[Visit Our Live Application](http://34.1.14.43:8501/)**

This **Streamlit web application** provides comprehensive **data analysis and visualization of cooking recipes and user interactions** based on the [Food.com Kaggle dataset](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions/data). Explore how recipes are rated, discover food trends, and understand user behavior through interactive dashboards powered by big data tools and advanced visualization techniques.

---

## ğŸš€ Features

- **ğŸ“Š Recipe Analysis**: Extract and visualize key recipe metrics (ingredients, steps, cooking time, nutritional info)
- **ğŸ‘¥ User Interaction Insights**: Analyze user ratings, reviews, and behavioral patterns
- **ğŸ”„ Automated Data Processing**: Efficient data cleaning and transformation into parquet format
- **ğŸ“ˆ Interactive Dashboards**: Built with **Streamlit** for real-time data exploration
- **ğŸ¨ Advanced NLP Visualizations**:
  - Word clouds (frequency-based and TF-IDF)
  - Polar plots for ingredient analysis
  - Venn diagrams for method comparison
- **ğŸ—ï¸ Modular Architecture**: Clean separation of backend, frontend, and utilities
- **ğŸ³ Docker Support**: Containerized deployment for easy scaling
- **âš¡ High Performance**: Leverages Polars for 10x faster DataFrame operations

---

## ğŸ§  Usage

Once the application is running, you can access multiple analytical pages:

### ğŸ“± Available Dashboards

1. **Overview** - High-level statistics and key metrics
2. **Recipe Analysis** - Deep dive into recipe characteristics and ingredients
3. **User Analysis** - User behavior patterns and engagement metrics
4. **Trends** - Temporal trends and popular recipe categories
5. **Ratings Dashboard** - Rating distributions and popularity analysis

### ğŸ¯ Key Capabilities

- Filter recipes by preparation time, rating, or specific ingredients
- Visualize rating distributions and popularity trends over time
- Explore word clouds of recipe reviews and ingredient frequencies
- Compare different NLP analysis methods (frequency vs. TF-IDF)
- Interactive polar plots for ingredient category analysis

---

## ğŸ› ï¸ Installation

### Prerequisites

- Python 3.12+
- [UV](https://docs.astral.sh/uv/) package manager
- Docker (optional, for containerized deployment)

### ğŸ“¦ Option 1: Local Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/shanoether/kit_big_data_mangetamain.git
   cd kit_big_data_mangetamain
   ```

2. **Set up the environment**
   ```bash
   # Install dependencies
   uv sync

   # Install development dependencies (optional)
   uv sync --group dev
   ```

3. **Download spaCy language model**
   ```bash
   uv run hatch run dev:download-spacy
   ```

4. **Download the dataset**

   Download from [Kaggle Food.com dataset](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions/data):
   - `RAW_interactions.csv`
   - `RAW_recipes.csv`

   Place both files in `/data/raw/`

5. **Run data preprocessing**
   ```bash
   uv run python src/mangetamain/backend/data_processor.py
   ```

   This will generate processed `.parquet` and `.pkl` files in `data/processed/`

6. **Launch the Streamlit application**
   ```bash
   uv run streamlit run src/mangetamain/streamlit_ui.py
   ```

7. **Access the application**

   Open your browser at: `http://localhost:8501`

### ğŸ³ Option 2: Docker Installation

Build and run using Docker Compose:

```bash
# Build and start all services
docker-compose -f docker-compose-local.yml up

# Access the application at http://localhost:8501

# Stop the services
docker-compose -f docker-compose-local.yml down
```

This will:
1. Build the Docker image locally
2. Spawn a preprocessing container to process the data
3. Launch the Streamlit webapp in a container

---

## ğŸ“ Project Structure

```
kit_big_data_mangetamain/
â”œâ”€â”€ ğŸ“‚ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml              # CI/CD pipeline for automated deployment
â”œâ”€â”€ ğŸ“‚ config/                       # Configuration files
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ processed/                   # Processed data (.parquet, .pkl)
â”‚   â”‚   â”œâ”€â”€ processed_interactions.parquet
â”‚   â”‚   â”œâ”€â”€ processed_recipes.parquet
â”‚   â”‚   â””â”€â”€ recipe_analyzer.pkl
â”‚   â””â”€â”€ raw/                         # Raw CSV data from Kaggle
â”‚       â”œâ”€â”€ RAW_interactions.csv
â”‚       â””â”€â”€ RAW_recipes.csv
â”œâ”€â”€ ğŸ“‚ docs/                         # MkDocs documentation and playbooks
â”‚   â”œâ”€â”€ index.md
â”‚   â”œâ”€â”€ playbook-env.md             # Environment setup guide
â”‚   â”œâ”€â”€ playbook-precommit.md       # Pre-commit workflow guide
â”‚   â”œâ”€â”€ playbook-troubleshooting.md # Troubleshooting tips
â”‚   â”œâ”€â”€ playbook-deployment-gcloud.md # GCP deployment guide
â”‚   â””â”€â”€ reference/                   # Auto-generated API docs
â”œâ”€â”€ ğŸ“‚ issue_template/               # GitHub issue templates
â”‚   â”œâ”€â”€ bug_report.md
â”‚   â””â”€â”€ feature_request.md
â”œâ”€â”€ ğŸ“‚ logs/                         # Application logs
â”‚   â””â”€â”€ app/
â”œâ”€â”€ ğŸ“‚ notebook/                     # Jupyter notebooks for exploration
â”œâ”€â”€ ğŸ“‚ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ pre-commit.sh               # Pre-commit checks
â”‚   â”œâ”€â”€ build_docker.sh             # Docker build script
â”‚   â””â”€â”€ gloud-setup.sh              # GCP setup script
â”œâ”€â”€ ğŸ“‚ src/mangetamain/              # Main application code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ streamlit_ui.py             # Main Streamlit application
â”‚   â”œâ”€â”€ ğŸ“‚ backend/
â”‚   â”‚   â”œâ”€â”€ data_processor.py       # ETL pipeline (DataProcessor class)
â”‚   â”‚   â”œâ”€â”€ recipe_analyzer.py      # NLP analysis (RecipeAnalyzer class)
â”‚   â”‚   â””â”€â”€ helper.py               # Helper functions
â”‚   â”œâ”€â”€ ğŸ“‚ frontend/pages/          # Streamlit page modules
â”‚   â”‚   â”œâ”€â”€ dashboard.py            # Ratings dashboard
â”‚   â”‚   â”œâ”€â”€ overview.py             # Overview page
â”‚   â”‚   â”œâ”€â”€ recipes_analysis.py     # Recipe analysis page
â”‚   â”‚   â”œâ”€â”€ trends.py               # Trends page
â”‚   â”‚   â””â”€â”€ users_analysis.py       # User analysis page
â”‚   â””â”€â”€ ğŸ“‚ utils/
â”‚       â”œâ”€â”€ logger.py               # Custom logging utility
â”‚       â””â”€â”€ helper.py               # General utilities
â”œâ”€â”€ ğŸ“‚ tests/                        # Test suite
â”‚   â””â”€â”€ unit/
â”‚       â””â”€â”€ mangetamain/
â”‚           â”œâ”€â”€ backend/
â”‚           â”‚   â”œâ”€â”€ test_data_processor.py
â”‚           â”‚   â”œâ”€â”€ test_recipe_analyzer.py
â”‚           â”‚   â””â”€â”€ test_helper.py
â”‚           â”œâ”€â”€ utils/
â”‚           â”‚   â””â”€â”€ test_logger.py
â”‚           â””â”€â”€ test_streamlit_ui.py
â”œâ”€â”€ ğŸ“„ .pre-commit-config.yaml       # Pre-commit hooks configuration
â”œâ”€â”€ ğŸ“„ docker-compose.yml            # Production Docker Compose
â”œâ”€â”€ ğŸ“„ docker-compose-local.yml      # Local Docker Compose
â”œâ”€â”€ ğŸ“„ Dockerfile                    # Multi-stage Docker build
â”œâ”€â”€ ğŸ“„ mkdocs.yml                    # MkDocs configuration
â”œâ”€â”€ ğŸ“„ pyproject.toml                # Project metadata & dependencies
â””â”€â”€ ğŸ“„ README.md                     # This file
```

---

## ğŸ’» Development Functionalities

### ğŸ¯ Object-Oriented Programming (OOP)

The project follows **OOP best practices** with a clean, modular architecture. However the object oriented approach is very basic as `Streamlit`implements its onw logic and is not made to be run into different classes.

#### Core Classes

1. **`DataProcessor`** (`src/mangetamain/backend/data_processor.py`)
   - **Purpose**: ETL pipeline for data cleaning and transformation
   - **Key Methods**:
     - `load_data()`: Load and validate raw CSV/ZIP files
     - `drop_na()`: Remove rows with missing or unrealistic values
     - `split_minutes()`: Categorize recipes by cooking time
     - `merge_data()`: Join interactions with recipe metadata
     - `save_data()`: Export processed data to Parquet format
   - **Features**: Type hints, comprehensive docstrings, error handling

2. **`RecipeAnalyzer`** (`src/mangetamain/backend/recipe_analyzer.py`)
   - **Purpose**: NLP analysis and visualization of recipe data
   - **Key Methods**:
     - `preprocess_text()`: Batch spaCy processing for 5-10x performance
     - `frequency_wordcloud()`: Generate frequency-based word clouds
     - `tfidf_wordcloud()`: Generate TF-IDF weighted word clouds
     - `compare_frequency_and_tfidf()`: Venn diagram comparison
     - `plot_top_ingredients()`: Polar plots for ingredient analysis
   - **Features**: LRU caching, figure memoization, streaming support
   - **Serialization**: Supports `save()` and `load()` for pickle persistence

3. **`BaseLogger`** (`src/mangetamain/utils/logger.py`)
   - **Purpose**: Centralized logging with colored console output
   - **Features**:
     - Rotating file handlers (5MB max, 3 backups)
     - ANSI color codes for different log levels
     - Thread-safe singleton pattern
     - Separate log files per session

#### Design Patterns Used

- **Factory Pattern**: Logger instantiation via `get_logger()`
- **Singleton Pattern**: Single logger instance per module
- **Strategy Pattern**: Different word cloud generation strategies (frequency vs. TF-IDF)
- **Caching Pattern**: LRU cache decorators for expensive operations

---

### ğŸ—ï¸ Frontend/Backend Architecture

The application follows a **separation of concerns** architecture with distinct backend and frontend components:

#### Two-Stage Container Architecture

Our Docker deployment uses a **sequential container orchestration** pattern:

##### Stage 1: Backend Processing Container

- **Purpose**: Heavy data preprocessing and transformation
- **Process**: Runs `data_processor.py` to:
  - Load raw CSV datasets from Kaggle
  - Clean and validate data (remove nulls, filter outliers)
  - Transform data into optimized formats
  - Generate `.parquet` files for fast columnar storage
  - Serialize NLP models to `.pkl` files
- **Lifecycle**: Automatically shuts down after successful completion
- **Output**: Persisted files in `data/processed/` volume

##### Stage 2: Frontend Application Container

- **Purpose**: Lightweight web interface for data visualization
- **Process**: Runs Streamlit application
- **Data Access**: Reads preprocessed `.parquet` and `.pkl` files
- **Lifecycle**: Runs continuously to serve the web application
- **Resources**: Minimal CPU/memory footprint

#### Architecture Benefits

âœ… **Separation of Concerns**:
- Backend handles computationally expensive ETL operations
- Frontend focuses solely on visualization and user interaction

âœ… **Improved Stability**:
- Frontend never performs heavy preprocessing
- No risk of UI crashes during data processing
- Graceful failure isolation

âœ… **Resource Efficiency**:
- Backend container only runs when data updates are needed
- Frontend container remains lightweight and responsive
- Optimized resource allocation per workload type

âœ… **Faster Startup**:
- Frontend launches instantly with preprocessed data
- No waiting for data processing on application start
- Better user experience

---

### ğŸ”„ Continuous Integration (Pre-Commit)

We maintain code quality through **automated pre-commit checks**:

#### Pre-Commit Hooks

Our `.pre-commit-config.yaml` includes:

1. **Code Quality**
   - âœ… `ruff`: Fast Python linter and formatter
   - âœ… `ruff-format`: Code formatting (PEP 8 compliant)
   - âœ… `mypy`: Static type checking

2. **File Integrity**
   - âœ… `trailing-whitespace`: Remove trailing whitespace
   - âœ… `end-of-file-fixer`: Ensure files end with newline
   - âœ… `check-merge-conflict`: Detect merge conflict markers
   - âœ… `check-toml`: Validate TOML syntax
   - âœ… `check-yaml`: Validate YAML syntax

3. **Testing**
   - âœ… `pytest`: Run unit tests before commit

#### Running Pre-Commit Checks

```bash
# Install pre-commit hooks (one-time setup)
uv run pre-commit install

# Run on all files manually
uv run pre-commit run --all-files

# Run specific hooks
uv run ruff check .
uv run ruff format .
uv run mypy src
uv run pytest
```

#### Complete Pre-Commit Workflow

For detailed pre-commit procedures, see our comprehensive guide:

ğŸ“– **[Pre-Commit Workflow Playbook](docs/playbook-precommit.md)**

This playbook covers:
- Code documentation with Pyment
- Linting and formatting commands
- Testing and coverage
- Documentation generation with MkDocs
- Full pre-commit checklist

---

### ğŸ“š Documentation

We use **MkDocs** with the **Material theme** for comprehensive documentation.

#### Documentation Structure

- **API Reference**: Auto-generated from docstrings using `mkdocstrings`
- **Playbooks**: Step-by-step guides for common tasks
  - Environment setup
  - Pre-commit workflow
  - Troubleshooting
  - GCP deployment
- **User Guides**: How to use the application features

#### Serving Documentation Locally

```bash
# Start documentation server
uv run hatch run docs:serve

# Access at http://127.0.0.1:8000
```

#### Building Documentation

```bash
# Build static documentation site
uv run hatch run docs:build

# Deploy to GitHub Pages
uv run hatch run docs:deploy
```

#### Documentation Configuration

- **Tool**: MkDocs with Material theme
- **Plugins**:
  - `mkdocstrings`: API reference generation
  - `gen-files`: Dynamic content generation
  - `section-index`: Automatic section indexing
  - `include-markdown`: Markdown file inclusion
- **Auto-generation**: `scripts/gen_ref_pages.py` generates API docs from source code

ğŸ“– **[View Full Documentation](http://127.0.0.1:8000)** (after running `docs:serve`)

---

### ğŸ“ Logger

Our custom logging system provides **structured, colorful logs** for better debugging:

#### Features

- **Color-Coded Output**: Different colors for DEBUG, INFO, WARNING, ERROR, CRITICAL
- **File Rotation**: Automatic log rotation (5MB files, 3 backups)
- **Dual Output**: Console and file handlers
- **Session-Based**: Separate log files per application run
- **Thread-Safe**: Safe for concurrent operations

#### Usage

```python
from mangetamain.utils.logger import get_logger

logger = get_logger()

logger.debug("Debugging information")
logger.info("Processing started")
logger.warning("Deprecated feature used")
logger.error("Failed to load data")
logger.critical("System shutdown required")
```

#### Log Storage

Logs are stored in `logs/app/` with timestamped filenames:
```
logs/app/
â”œâ”€â”€ app_20251028_143052.log
â”œâ”€â”€ app_20251028_143052.log.1
â””â”€â”€ app_20251028_143052.log.2
```

---

### ğŸš€ Continuous Deployment

We use **GitHub Actions** for automated deployment to our Google Cloud VM.

#### CI/CD Pipeline

Our `.github/workflows/deploy.yml` implements a **two-stage pipeline**:

##### Stage 1: Security Scan

- **Tool**: [Safety CLI](https://github.com/pyupio/safety)
- **Purpose**: Scan dependencies for known security vulnerabilities
- **Trigger**: Every push to `main` branch
- **Action**: Fails pipeline if vulnerabilities found

```yaml
security:
  runs-on: ubuntu-latest
  steps:
    - uses: pyupio/safety-action@v1
      with:
        api-key: ${{ secrets.SAFETY_API_KEY }}
```

##### Stage 2: Build & Deploy

Only runs if security scan passes:

1. **Build Docker Image**
   - Multi-stage build for optimized image size
   - Tags: `latest` and `sha-<commit-sha>`
   - Push to GitHub Container Registry (GHCR)

2. **Deploy to VM**
   - SSH into Google Cloud VM
   - Pull latest code and Docker images
   - Run `docker compose up -d`
   - Zero-downtime deployment with health checks

3. **Deploy Documentation**
   - Build documentation with MkDocs
   - Deploy to GitHub Pages automatically
   - Available at: `https://shanoether.github.io/kit_big_data_mangetamain/`

#### Deployment Flow

```
Push to main â†’ Security Scan â†’ Build Docker â†’ Push to GHCR â†’ Deploy to VM â†’ Deploy Docs to GitHub Pages
```

#### Environment Variables & Secrets

Required GitHub secrets:
- `SAFETY_API_KEY`: Safety CLI API key
- `SSH_KEY`: Private SSH key for VM access
- `GHCR_PAT`: GitHub Personal Access Token
- `SSH_HOST`: VM IP address (environment variable)
- `SSH_USER`: VM SSH username (environment variable)

#### Manual Deployment

For manual deployment to Google Cloud, see:

ğŸ“– **[GCP Deployment Playbook](docs/playbook-deployment-gcloud.md)**

---

### ğŸ§ª Tests

We maintain **comprehensive test coverage** across all modules.

#### Test Coverage

- **Overall Coverage**: ~97%+ across core modules
- **Backend**: 100% coverage on `DataProcessor` and `RecipeAnalyzer`
- **Utils**: 100% coverage on `logger` and helper functions
- **Frontend**: Core Streamlit functions tested with mocking

#### Running Tests

```bash
# Run all tests
uv run pytest

# Run with coverage report
uv run coverage run -m pytest
uv run coverage report -m

# Run specific test file
uv run pytest tests/unit/mangetamain/backend/test_recipe_analyzer.py

# Run specific test class
uv run pytest tests/unit/mangetamain/backend/test_recipe_analyzer.py::TestRecipeAnalyzer

# Run specific test method
uv run pytest tests/unit/mangetamain/backend/test_recipe_analyzer.py::TestRecipeAnalyzer::test_preprocess_text
```

Test folder structur mirrors the structure of `src/mangetamain`

#### Testing Tools

- **pytest**: Test framework
- **pytest-cov**: Coverage reporting
- **unittest.mock**: Mocking framework for isolating tests
- **MagicMock**: Mock objects with type hints

#### Coverage Requirements

- **Minimum**: 90% overall coverage (configured in `pyproject.toml`)
- **Target**: 95%+ for production code
- **Docstring Coverage**: 80% minimum (enforced by `interrogate`)

---

### ğŸ”’ Security

Security is a **top priority** in our deployment and development process.

#### Security Measures Implemented

##### 1. Dependency Scanning

- **Tool**: [Safety CLI](https://pyup.io/safety/)
- **Frequency**: Every commit to `main` branch
- **Action**: Automated scan of all Python dependencies
- **Result**: Pipeline fails if vulnerabilities detected

```yaml
# From .github/workflows/deploy.yml
- name: Run Safety CLI to check for vulnerabilities
  uses: pyupio/safety-action@v1
  with:
    api-key: ${{ secrets.SAFETY_API_KEY }}
```

##### 2. VM Security

**Firewall Configuration**:
- Only port 8501 (Streamlit) exposed to public internet
- All other ports blocked by default

**SSH Key Authentication**:
- Password authentication disabled
- Only ED25519 SSH key authentication allowed
- Separate deployment keys for CI/CD

**VM Hardening**:
```bash
# SSH configuration
PermitRootLogin no
PasswordAuthentication no
PubkeyAuthentication yes
```

##### 3. Secrets Management

- **GitHub Secrets**: All sensitive credentials stored securely
- **Environment Variables**: No hardcoded secrets in code
- `.env` files in `.gitignore`

##### 4. Docker Security

- **Non-root user**: Container runs as non-privileged user
- **Multi-stage build**: Minimizes attack surface
- **Minimal base image**: Debian slim for smaller footprint
- **No secrets in images**: All secrets mounted at runtime

##### 5. Code Security

- **Type checking**: `mypy` enforces type safety
- **Input validation**: All user inputs validated
- **SQL injection prevention**: Uses Polars DataFrames (no raw SQL)
- **XSS prevention**: Streamlit handles output sanitization

#### Security Audit Results

Latest security scan (generated with Safety CLI):

- TODO

#### Reporting Security Issues

Found a security vulnerability? Please report it responsibly:

1. **DO NOT** open a public GitHub issue
2. Email: gardelautrepourdemain@mangetamain.ai
3. Include:
   - Description of the vulnerability
   - Steps to reproduce
   - Potential impact
   - Suggested fix (if any)

---

### âš¡ Performance

We've optimized the application for **fast data processing and rendering**.

#### Performance Optimizations

##### 1. Data Processing

**Polars instead of Pandas**:
- **10-30x faster** for large datasets
- Lazy evaluation for query optimization
- Parallel processing by default
- Lower memory footprint

##### 2. NLP Processing

**Batch Processing with spaCy**:
- Process texts in batches of 100 for **5-10x speedup**
- Pipeline optimization (`disable=["parser", "ner"]`)
- Only lemmatization and POS tagging enabled

```python
# Batch processing in RecipeAnalyzer
for doc in nlp.pipe(texts, batch_size=100, disable=["parser", "ner"]):
    # Process...
```

##### 3. Caching Strategy

**LRU Cache for Expensive Operations**:
- `@lru_cache` on figure generation methods
- Cached preprocessing results
- Memoized word clouds and plots

```python
@lru_cache(maxsize=128)
def frequency_wordcloud(self, recipe_count: int, ...) -> Figure:
    # Expensive operation cached
```

**Streamlit Cache**:
- `@st.cache_data` for data loading
- `@st.cache_resource` for model loading
- Persistent cache across user sessions

##### 4. Lazy Loading

- Data loaded only when needed
- Progressive rendering of dashboards
- Deferred visualization generation

#### Profiling

We use multiple tools for performance profiling:

```bash
# Install profiling tools
uv sync --group dev

# Profile with py-spy (sampling profiler)
uv run py-spy record -o profile.svg -- python src/mangetamain/backend/data_processor.py

# Profile with snakeviz (visualizer)
uv run python -m cProfile -o output.prof src/mangetamain/backend/data_processor.py
uv run snakeviz output.prof
```

Profiling results available in `docs/profiling.md`.

#### Performance Monitoring

- **Application logs**: Track processing times
- **Streamlit metrics**: Monitor page load times
- **Docker stats**: Monitor container resource usage

```bash
# Monitor container performance
docker stats mangetamain
```

---


## ğŸ§‘â€ğŸ’» Contributing

We welcome contributions! Please follow our contribution guidelines to maintain code quality and consistency.

### ğŸ“‹ Using Issue Templates

Before starting work, please create an issue using one of our templates:

#### ğŸ› Bug Report

Found a bug? Use our **[Bug Report Template](issue_template/bug_report.md)**:

1. Go to **Issues** â†’ **New Issue**
2. Select **Bug report** template
3. Fill in:
   - **Description**: Clear description of the bug
   - **To Reproduce**: Step-by-step reproduction steps
   - **Expected behavior**: What should happen
   - **Environment**: OS, browser, version
   - **Screenshots**: If applicable

#### âœ¨ Feature Request

Have an idea? Use our **[Feature Request Template](issue_template/feature_request.md)**:

1. Go to **Issues** â†’ **New Issue**
2. Select **Feature request** template
3. Fill in:
   - **Problem description**: What problem does this solve?
   - **Proposed solution**: How should it work?
   - **Alternatives considered**: Other approaches
   - **Additional context**: Screenshots, mockups, etc.

---

## ğŸŒ± Future Improvements

We're continuously working to improve Mangetamain. Here are planned enhancements:

- ğŸ” **Recipe Clustering**: ML-based similarity analysis to discover recipe patterns and group similar recipes
- ğŸ“„ **PDF Export**: Export dashboards and analysis reports to PDF for offline viewing
- ğŸ“Š **Advanced Visualizations**:
  - Interactive 3D plots for multi-dimensional analysis
  - Network graphs for recipe ingredient relationships
  - Heatmaps for user behavior patterns
- âš™ï¸ **Enhanced CI/CD Pipeline**:
  - Add automated testing stage
  - Manual approval gate before production deployment
- ğŸ§® **Advanced Analytics**:
  - Time-series forecasting for recipe trends
  - Sentiment analysis on user reviews
  - Anomaly detection for unusual rating patterns
- ğŸ—„ï¸ **Database Migration**:
  - Move from CSV to PostgreSQL for better scalability
  - Implement data versioning
  - Add data backup and recovery procedures

---

## ğŸ“Š Project Metrics

- **Test Coverage**: 97%+
- **Number of Tests**: 80+
- **Dependencies**: 40+
- **Supported Python Version**: 3.12+
- **Docker Image Size**: ~850MB (optimized)

---

## ğŸ™ Acknowledgments

- **Dataset**: [Food.com Recipes and User Interactions](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions/data) from Kaggle
- **Framework**: [Streamlit](https://streamlit.io/) for the interactive web interface
- **Data Processing**: [Polars](https://www.pola.rs/) for high-performance data operations
- **NLP**: [spaCy](https://spacy.io/) for natural language processing
- **Deployment**: [Google Cloud Platform](https://cloud.google.com/) for hosting

---

## ğŸ“š Additional Resources

- ğŸ“˜ **[Environment Setup Playbook](docs/playbook-env.md)** - Detailed environment configuration
- ğŸ“— **[Pre-Commit Playbook](docs/playbook-precommit.md)** - Code quality workflow
- ğŸ“™ **[Troubleshooting Guide](docs/playbook-troubleshooting.md)** - Common issues and solutions
- ğŸ“• **[GCP Deployment Guide](docs/playbook-deployment-gcloud.md)** - Production deployment

---

## ğŸ“ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“ Contact & Support

- ğŸ› **Bug Reports and Feature Request**: [GitHub Issues](https://github.com/shanoether/kit_big_data_mangetamain/issues)

- ğŸ“§ **Email**: gardelautrepourdemain@mangetamain.ai
- ğŸŒ **Live App**: [http://34.1.14.43:8501/](http://34.1.14.43:8501/)
