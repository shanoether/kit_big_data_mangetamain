{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udf7d\ufe0f Mangetamain, Garde l'Autre Pour Demain","text":"<p>\"Eat this one, save the other for tomorrow!\"</p> <p>Mangetamain is a leader in B2C recipe recommendations powered by massive data analytics. We're sharing our best insights with the world through an interactive web platform where everyone can discover what makes recipes delicious... or not! \ud83c\udf10 Visit Our Live Application</p> <p>This Streamlit web application provides comprehensive data analysis and visualization of cooking recipes and user interactions based on the Food.com Kaggle dataset. Explore how recipes are rated, discover food trends, and understand user behavior through interactive dashboards powered by big data tools and advanced visualization techniques.</p>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>\ud83d\udcca Recipe Analysis: Extract and visualize key recipe metrics (ingredients, steps, cooking time, nutritional info)</li> <li>\ud83d\udc65 User Interaction Insights: Analyze user ratings, reviews, and behavioral patterns</li> <li>\ud83d\udd04 Automated Data Processing: Efficient data cleaning and transformation into parquet format</li> <li>\ud83d\udcc8 Interactive Dashboards: Built with Streamlit for real-time data exploration</li> <li>\ud83c\udfa8 Advanced NLP Visualizations:</li> <li>Word clouds (frequency-based and TF-IDF)</li> <li>Polar plots for ingredient analysis</li> <li>Venn diagrams for method comparison</li> <li>\ud83c\udfd7\ufe0f Modular Architecture: Clean separation of backend, frontend, and utilities</li> <li>\ud83d\udc33 Docker Support: Containerized deployment for easy scaling</li> <li>\u26a1 High Performance: Leverages Polars for 10x faster DataFrame operations</li> </ul>"},{"location":"#usage","title":"\ud83e\udde0 Usage","text":"<p>Once the application is running, you can access multiple analytical pages:</p>"},{"location":"#available-dashboards","title":"\ud83d\udcf1 Available Dashboards","text":"<ol> <li>Ratings Dashboard - Rating distributions and popularity analysis</li> <li>Trends - Temporal trends and popular recipe categories</li> <li>User Analysis - User behavior patterns and engagement metrics</li> <li>Recipe Analysis - Deep dive into recipe characteristics and ingredients</li> </ol>"},{"location":"#key-capabilities","title":"\ud83c\udfaf Key Capabilities","text":"<ul> <li>Filter recipes by preparation time, rating, or specific ingredients</li> <li>Visualize rating distributions and popularity trends over time</li> <li>Explore word clouds of recipe reviews and ingredient frequencies</li> <li>Compare different NLP analysis methods (frequency vs. TF-IDF)</li> <li>Interactive polar plots for ingredient category analysis</li> </ul>"},{"location":"#installation","title":"\ud83d\udee0\ufe0f Installation","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>UV package manager</li> <li>Docker (optional, for containerized deployment)</li> </ul>"},{"location":"#local-setup","title":"Local Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/shanoether/kit_big_data_mangetamain.git\ncd kit_big_data_mangetamain\n\n# Install dependencies\nuv sync\n\n# Download spaCy model\nuv run python -m spacy download en_core_web_sm\n</code></pre>"},{"location":"#data-preparation","title":"Data Preparation","text":"<ol> <li>Download Food.com dataset</li> <li>Place <code>RAW_interactions.csv</code> and <code>RAW_recipes.csv</code> in <code>data/raw/</code></li> <li>Run preprocessing:</li> </ol> <pre><code>uv run python src/mangetamain/backend/data_processor.py\n</code></pre> <p>This generates optimized <code>.parquet</code> files and a cached <code>recipe_analyzer.pkl</code> in <code>data/processed/</code>.</p>"},{"location":"#launch-application","title":"Launch Application","text":"<pre><code>uv run streamlit run src/mangetamain/streamlit_ui.py\n</code></pre> <p>Access at <code>http://localhost:8501</code></p>"},{"location":"#docker-deployment","title":"Docker Deployment","text":"<pre><code># Build and start all services\ndocker-compose -f docker-compose-local.yml up\n# Access the application at http://localhost:8501\n# Stop the services\ndocker-compose -f docker-compose-local.yml down\n</code></pre> <p>This will: 1. Build the Docker image locally 2. Spawn a preprocessing container to process the data 3. Launch the Streamlit webapp in a container</p>"},{"location":"#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<pre><code>kit_big_data_mangetamain/\n\u251c\u2500\u2500 src/mangetamain/\n\u2502   \u251c\u2500\u2500 streamlit_ui.py              # Main application entry\n\u2502   \u251c\u2500\u2500 backend/\n\u2502   \u2502   \u251c\u2500\u2500 data_processor.py        # ETL pipeline\n\u2502   \u2502   \u2514\u2500\u2500 recipe_analyzer.py       # NLP analysis\n\u2502   \u251c\u2500\u2500 frontend/pages/              # Streamlit pages\n\u2502   \u2502   \u251c\u2500\u2500 overview.py\n\u2502   \u2502   \u251c\u2500\u2500 recipes_analysis.py\n\u2502   \u2502   \u251c\u2500\u2500 users_analysis.py\n\u2502   \u2502   \u251c\u2500\u2500 trends.py\n\u2502   \u2502   \u2514\u2500\u2500 dashboard.py\n\u2502   \u2514\u2500\u2500 utils/\n\u2502       \u251c\u2500\u2500 logger.py                # Custom logging\n\u2502       \u2514\u2500\u2500 helper.py                # Data loading utilities\n\u251c\u2500\u2500 tests/unit/                      # Unit tests\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/                         # CSV input files\n\u2502   \u2514\u2500\u2500 processed/                   # Parquet &amp; pickle outputs\n\u251c\u2500\u2500 docs/                            # MkDocs documentation\n\u251c\u2500\u2500 .github/workflows/deploy.yml     # CI/CD pipeline\n\u251c\u2500\u2500 docker-compose.yml               # Production deployment\n\u251c\u2500\u2500 Dockerfile                       # Multi-stage build\n\u2514\u2500\u2500 pyproject.toml                   # Dependencies &amp; config\n</code></pre>"},{"location":"#development-functionalities","title":"\ud83d\udcbb Development Functionalities","text":""},{"location":"#object-oriented-programming-oop","title":"\ud83c\udfaf Object-Oriented Programming (OOP)","text":"<p>The project follows OOP best practices with a clean, modular architecture. However, the object-oriented approach is very basic as <code>Streamlit</code> implements its own logic and is not made to be run into different classes.</p>"},{"location":"#core-classes","title":"Core Classes","text":"<ol> <li><code>DataProcessor</code> (<code>src/mangetamain/backend/data_processor.py</code>)</li> <li>Purpose: ETL pipeline for data cleaning and transformation</li> <li>Key Methods:<ul> <li><code>load_data()</code>: Load and validate raw CSV/ZIP files</li> <li><code>drop_na()</code>: Remove rows with missing or unrealistic values</li> <li><code>split_minutes()</code>: Categorize recipes by cooking time</li> <li><code>merge_data()</code>: Join interactions with recipe metadata</li> <li><code>save_data()</code>: Export processed data to Parquet format</li> </ul> </li> <li> <p>Features: Type hints, comprehensive docstrings, error handling</p> </li> <li> <p><code>RecipeAnalyzer</code> (<code>src/mangetamain/backend/recipe_analyzer.py</code>)</p> </li> <li>Purpose: NLP analysis and visualization of recipe data</li> <li>Key Methods:<ul> <li><code>preprocess_text()</code>: Batch spaCy processing for 5-10x performance</li> <li><code>frequency_wordcloud()</code>: Generate frequency-based word clouds</li> <li><code>tfidf_wordcloud()</code>: Generate TF-IDF weighted word clouds</li> <li><code>compare_frequency_and_tfidf()</code>: Venn diagram comparison</li> <li><code>plot_top_ingredients()</code>: Polar plots for ingredient analysis</li> </ul> </li> <li>Features: LRU caching, figure memoization, streaming support</li> <li> <p>Serialization: Supports <code>save()</code> and <code>load()</code> for pickle persistence</p> </li> <li> <p><code>BaseLogger</code> (<code>src/mangetamain/utils/logger.py</code>)</p> </li> <li>Purpose: Centralized logging with colored console output</li> <li>Features:<ul> <li>Rotating file handlers (5MB max, 3 backups)</li> <li>ANSI color codes for different log levels</li> <li>Thread-safe singleton pattern</li> <li>Separate log files per session</li> </ul> </li> </ol>"},{"location":"#design-patterns-used","title":"Design Patterns Used","text":"<ul> <li>Factory Pattern: Logger instantiation via <code>get_logger()</code></li> <li>Singleton Pattern: Single logger instance per module</li> <li>Strategy Pattern: Different word cloud generation strategies (frequency vs. TF-IDF)</li> <li>Caching Pattern: LRU cache decorators for expensive operations</li> </ul>"},{"location":"#frontendbackend-architecture","title":"\ud83c\udfd7\ufe0f Frontend/Backend Architecture","text":"<p>The application follows a separation of concerns architecture with distinct backend and frontend components:</p>"},{"location":"#two-stage-container-architecture","title":"Two-Stage Container Architecture","text":"<p>Our Docker deployment uses a sequential container orchestration pattern:</p>"},{"location":"#stage-1-backend-processing-container","title":"Stage 1: Backend Processing Container","text":"<ul> <li>Purpose: Heavy data preprocessing and transformation</li> <li>Process: Runs <code>data_processor.py</code> to:</li> <li>Load raw CSV datasets from Kaggle</li> <li>Clean and validate data (remove nulls, filter outliers)</li> <li>Transform data into optimized formats</li> <li>Generate <code>.parquet</code> files for fast columnar storage</li> <li>Serialize NLP models to <code>.pkl</code> files</li> <li>Lifecycle: Automatically shuts down after successful completion</li> <li>Output: Persisted files in <code>data/processed/</code> volume</li> </ul>"},{"location":"#stage-2-frontend-application-container","title":"Stage 2: Frontend Application Container","text":"<ul> <li>Purpose: Lightweight web interface for data visualization</li> <li>Process: Runs Streamlit application</li> <li>Data Access: Reads preprocessed <code>.parquet</code> and <code>.pkl</code> files</li> <li>Lifecycle: Runs continuously to serve the web application</li> <li>Resources: Minimal CPU/memory footprint</li> </ul>"},{"location":"#architecture-benefits","title":"Architecture Benefits","text":"<ul> <li>Separation of Concerns</li> <li>Backend handles computationally expensive ETL operations</li> <li>Frontend focuses solely on visualization and user interaction</li> <li>Improved Stability: </li> <li>Frontend never performs heavy preprocessing</li> <li>No risk of UI crashes during data processing</li> <li> <p>Graceful failure isolation</p> </li> <li> <p>Resource Efficiency:</p> </li> <li>Backend container only runs when data updates are needed</li> <li>Frontend container remains lightweight and responsive</li> <li>Optimized resource allocation per workload type</li> <li>Faster Startup:</li> <li>Frontend launches instantly with preprocessed data</li> <li>No waiting for data processing on application start</li> <li>Better user experience</li> </ul>"},{"location":"#continuous-integration-pre-commit","title":"\ud83d\udd04 Continuous Integration (Pre-Commit)","text":"<p>We maintain code quality through automated pre-commit checks:</p>"},{"location":"#pre-commit-hooks","title":"Pre-Commit Hooks","text":"<p>Our <code>.pre-commit-config.yaml</code> includes:</p> <ol> <li>Code Quality</li> <li>\u2705 <code>ruff</code>: Fast Python linter and formatter</li> <li>\u2705 <code>ruff-format</code>: Code formatting (PEP 8 compliant)</li> <li> <p>\u2705 <code>mypy</code>: Static type checking</p> </li> <li> <p>File Integrity</p> </li> <li>\u2705 <code>trailing-whitespace</code>: Remove trailing whitespace</li> <li>\u2705 <code>end-of-file-fixer</code>: Ensure files end with newline</li> <li>\u2705 <code>check-merge-conflict</code>: Detect merge conflict markers</li> <li>\u2705 <code>check-toml</code>: Validate TOML syntax</li> <li> <p>\u2705 <code>check-yaml</code>: Validate YAML syntax</p> </li> <li> <p>Testing</p> </li> <li>\u2705 <code>pytest</code>: Run unit tests before commit</li> </ol>"},{"location":"#running-pre-commit-checks","title":"Running Pre-Commit Checks","text":"<pre><code># Pre-commit checks\nuv run pre-commit install\n\n# Run on all files manually\nuv run pre-commit run --all-files\n\n# Linting &amp; formatting\nuv run ruff check .\nuv run ruff format .\n\n# Type checking\nuv run mypy src\n</code></pre> <p>For detailed pre-commit procedures, see our comprehensive guide:</p> <p>\ud83d\udcd6 Pre-Commit Workflow Playbook</p>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>We use MkDocs with the Material theme for comprehensive documentation.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Available on GitHub: Documentation is automatically updated during deployment and published on GitHub Pages.</li> <li>API Reference: Auto-generated from docstrings using <code>mkdocstrings</code></li> <li>Playbooks: Step-by-step guides for common tasks</li> <li>Environment setup</li> <li>Pre-commit workflow</li> <li>Troubleshooting</li> <li>GCP deployment</li> <li>User Guides: How to use the application features</li> </ul>"},{"location":"#serving-documentation-locally","title":"Serving Documentation Locally","text":"<pre><code># Start documentation server\nuv run hatch run docs:serve\n\n# Access at http://127.0.0.1:8000\n</code></pre>"},{"location":"#building-documentation","title":"Building Documentation","text":"<pre><code># Build static documentation site\nuv run hatch run docs:build\n\n# Deploy to GitHub Pages\nuv run hatch run docs:deploy\n</code></pre>"},{"location":"#documentation-configuration","title":"Documentation Configuration","text":"<ul> <li>Tool: MkDocs with Material theme</li> <li>Plugins:</li> <li><code>mkdocstrings</code>: API reference generation</li> <li><code>gen-files</code>: Dynamic content generation</li> <li><code>section-index</code>: Automatic section indexing</li> <li><code>include-markdown</code>: Markdown file inclusion</li> <li>Auto-generation: <code>scripts/gen_ref_pages.py</code> generates API docs from source code</li> </ul> <p>\ud83d\udcd6 View Full Documentation (after running <code>docs:serve</code>)</p>"},{"location":"#logger","title":"\ud83d\udcdd Logger","text":"<p>Our custom logging system provides structured, colorful logs for better debugging:</p>"},{"location":"#features_1","title":"Features","text":"<ul> <li>Color-Coded Output: Different colors for DEBUG, INFO, WARNING, ERROR, CRITICAL</li> <li>File Rotation: Automatic log rotation (5MB files, 3 backups)</li> <li>Dual Output: Console and file handlers</li> <li>Session-Based: Separate log files per application run</li> <li>Thread-Safe: Safe for concurrent operations</li> </ul>"},{"location":"#usage_1","title":"Usage","text":"<pre><code>from mangetamain.utils.logger import get_logger\n\nlogger = get_logger()\n\nlogger.debug(\"Debugging information\")\nlogger.info(\"Processing started\")\nlogger.warning(\"Deprecated feature used\")\nlogger.error(\"Failed to load data\")\nlogger.critical(\"System shutdown required\")\n</code></pre>"},{"location":"#log-storage","title":"Log Storage","text":"<p>Logs are stored in <code>logs/app/</code> with timestamped filenames: <pre><code>logs/app/\n\u251c\u2500\u2500 app_20251028_143052.log\n\u251c\u2500\u2500 app_20251028_143052.log.1\n\u2514\u2500\u2500 app_20251028_143052.log.2\n</code></pre></p>"},{"location":"#continuous-deployment","title":"\ud83d\ude80 Continuous Deployment","text":"<p>We use GitHub Actions for automated deployment to our Google Cloud VM.</p>"},{"location":"#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>Our <code>.github/workflows/deploy.yml</code> implements a two-stage pipeline:</p>"},{"location":"#stage-1-security-scan","title":"Stage 1: Security Scan","text":"<ul> <li>Tool: Safety CLI</li> <li>Purpose: Scan dependencies for known security vulnerabilities</li> <li>Trigger: Every push to <code>main</code> branch</li> <li>Action: Fails pipeline if vulnerabilities found</li> </ul>"},{"location":"#stage-2-build-deploy","title":"Stage 2: Build &amp; Deploy","text":"<p>Only runs if security scan passes:</p> <ol> <li>Build Docker Image</li> <li>Multi-stage build for optimized image size</li> <li>Tags: <code>latest</code> and <code>sha-&lt;commit-sha&gt;</code></li> <li> <p>Push to GitHub Container Registry (GHCR)</p> </li> <li> <p>Deploy to VM</p> </li> <li>SSH into Google Cloud VM</li> <li>Pull latest code and Docker images</li> <li>Run <code>docker compose up -d</code></li> <li> <p>Zero-downtime deployment with health checks</p> </li> <li> <p>Deploy Documentation</p> </li> <li>Build documentation with MkDocs</li> <li>Deploy to GitHub Pages automatically</li> <li>Available at: <code>https://shanoether.github.io/kit_big_data_mangetamain/</code></li> </ol>"},{"location":"#deployment-flow","title":"Deployment Flow","text":"<pre><code>Push to main \u2192 Security Scan \u2192 Build Docker \u2192 Push to GHCR \u2192 Deploy to VM \u2192 Deploy Docs to GitHub Pages\n</code></pre>"},{"location":"#environment-variables-secrets","title":"Environment Variables &amp; Secrets","text":"<p>Required GitHub secrets: - <code>SAFETY_API_KEY</code>: Safety CLI API key - <code>SSH_KEY</code>: Private SSH key for VM access - <code>GHCR_PAT</code>: GitHub Personal Access Token - <code>SSH_HOST</code>: VM IP address (environment variable) - <code>SSH_USER</code>: VM SSH username (environment variable)</p>"},{"location":"#manual-deployment","title":"Manual Deployment","text":"<p>For manual deployment to Google Cloud, see:</p> <p>\ud83d\udcd6 GCP Deployment Playbook</p>"},{"location":"#tests","title":"\ud83e\uddea Tests","text":"<p>We maintain comprehensive test coverage across all modules.</p>"},{"location":"#test-coverage","title":"Test Coverage","text":"<ul> <li>Overall Coverage: ~90%+ across core modules</li> <li>Backend: 100% coverage on <code>DataProcessor</code> and <code>RecipeAnalyzer</code></li> <li>Utils: 100% coverage on <code>logger</code> and helper functions</li> <li>Frontend: Core Streamlit functions tested with mocking</li> </ul>"},{"location":"#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# With coverage\nuv run pytest --cov=src --cov-report=html\n\n# Specific test\nuv run pytest tests/unit/mangetamain/backend/test_recipe_analyzer.py\n</code></pre>"},{"location":"#security","title":"\ud83d\udd12 Security","text":"<ul> <li>Dependency Scanning: Automated Safety CLI checks on every commit</li> <li>Firewall: Only port 443 exposed</li> <li>SSH: Key-based authentication only, no passwords</li> <li>Docker: Non-root user, minimal base image</li> <li>Secrets: GitHub Secrets for credentials, no hardcoded values</li> <li>Errors Page: Users do not get exposed to precise error messages but generic ones to avoid exploitation.</li> <li>HTTPS Connection: Secure connection through HTTPS with certificate generated with <code>letsencrypt</code>.</li> </ul>"},{"location":"#performance","title":"\u26a1 Performance","text":"<p>We currently have some unresolved performance issues. The loading of different pages is slow despite the various techniques we set up to reduce lagging.</p> <p>Optimizations: - Polars: 10-30x faster than Pandas for large datasets - Batch Processing: spaCy processes 100 texts at a time - Caching: <code>@st.cache_data</code> for data, <code>@st.cache_resource</code> for models - Lazy Loading: Data loaded only when needed</p> <p>Profiling: <pre><code>uv run py-spy record -o profile.svg -- python src/mangetamain/backend/data_processor.py\n</code></pre></p>"},{"location":"#contributing","title":"\ud83e\uddd1\u200d\ud83d\udcbb Contributing","text":"<p>Use our issue templates for bug reports or feature requests:</p> <ul> <li>Bug Report: <code>issue_template/bug_report.md</code></li> <li>Feature Request: <code>issue_template/feature_request.md</code></li> </ul>"},{"location":"#future-improvements","title":"\ud83c\udf31 Future Improvements","text":"<p>We're continuously working to improve Mangetamain. Here are planned enhancements:</p> <ul> <li>\ud83d\udd0d Recipe Clustering: ML-based similarity analysis to discover recipe patterns and group similar recipes</li> <li> <p>\ud83d\udcca Advanced Visualizations:</p> </li> <li> <p>Network graphs for recipe ingredient relationships</p> </li> <li>Heatmaps for user behavior patterns</li> <li>\u2699\ufe0f Enhanced CI/CD Pipeline:</li> <li>Add automated testing stage</li> <li>Manual approval gate before production deployment</li> <li>\ud83e\uddee Advanced Analytics:</li> <li>Time-series forecasting for recipe trends</li> <li>Sentiment analysis on user reviews</li> <li>Anomaly detection for unusual rating patterns</li> <li>\ud83d\uddc4\ufe0f Database Migration:</li> <li>Move from Parquet to PostgreSQL for better scalability with an API endpoint for the frontend to connect to</li> <li>Implement data versioning</li> <li>Add data backup and recovery procedures</li> </ul>"},{"location":"#project-metrics","title":"\ud83d\udcca Project Metrics","text":"<ul> <li>Test Coverage: 90%+</li> <li>Python Version: 3.12+</li> <li>Docker Image: ~1.5GB (multi-stage optimized)</li> <li>Lines of Code: ~5,000</li> </ul>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<ul> <li>Dataset: Food.com Recipes and User Interactions from Kaggle</li> <li>Framework: Streamlit for the interactive web interface</li> <li>Data Processing: Polars for high-performance data operations</li> <li>NLP: spaCy for natural language processing</li> <li>Deployment: Google Cloud Platform for hosting</li> </ul>"},{"location":"#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>\ud83d\udcd8 Environment Setup Playbook - Detailed environment configuration</li> <li>\ud83d\udcd7 Pre-Commit Playbook - Code quality workflow</li> <li>\ud83d\udcd9 Troubleshooting Guide - Common issues and solutions</li> <li>\ud83d\udcd5 GCP Deployment Guide - Production deployment</li> </ul>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>MIT License - see LICENSE file</p>"},{"location":"#contact","title":"\ud83d\udcde Contact","text":"<ul> <li>Issues: GitHub Issues</li> <li>Email: gardelautrepourdemain@mangetamain.ai</li> <li>Live App: https://mangetamain.duckdns.org/</li> </ul>"},{"location":"playbook-deployment-gcloud/","title":"Google Cloud Deployment Playbook","text":"<p>This playbook covers Docker containerization, Google Cloud Platform setup, and deployment workflows for the Mangetamain project.</p>"},{"location":"playbook-deployment-gcloud/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Docker Setup</li> <li>Building Docker Images</li> <li>Google Cloud Platform Setup</li> <li>VM Configuration</li> <li>Deployment Workflow</li> <li>Troubleshooting Deployment</li> </ul>"},{"location":"playbook-deployment-gcloud/#docker-setup","title":"Docker Setup","text":""},{"location":"playbook-deployment-gcloud/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop (macOS/Windows) or Colima (macOS)</li> <li>Docker Compose</li> <li>BuildKit support</li> </ul>"},{"location":"playbook-deployment-gcloud/#colima-setup-macos-recommended","title":"Colima Setup (macOS Recommended)","text":"<p>Colima is a lightweight Docker runtime for macOS that's more efficient than Docker Desktop.</p> <pre><code># Install Colima (if not installed)\nbrew install colima\n\n# Start Colima with sufficient resources\ncolima start --memory 8 --cpu 4\n\n# Switch Docker context to Colima\ndocker context use colima\n\n# Verify Colima is running\ndocker ps\n</code></pre>"},{"location":"playbook-deployment-gcloud/#docker-buildkit-configuration","title":"Docker BuildKit Configuration","text":"<p>BuildKit enables advanced Docker features like multi-platform builds.</p> <pre><code># Create and use a buildx builder (one-time setup)\ndocker buildx create --name colima-builder \\\n  --use \\\n  --driver docker-container\n\n# Initialize the builder\ndocker buildx inspect --bootstrap\n\n# Verify builder is ready\ndocker buildx ls\n</code></pre>"},{"location":"playbook-deployment-gcloud/#building-docker-images","title":"Building Docker Images","text":""},{"location":"playbook-deployment-gcloud/#simple-docker-build-development","title":"Simple Docker Build (Development)","text":"<p>For quick local testing:</p> <pre><code># Build the image\ndocker build -t mangetamain .\n\n# Run the container\ndocker run -p 8501:8501 -v ./data:/app/data mangetamain\n</code></pre> <p>Access the application: Open <code>http://localhost:8501</code> in your browser.</p>"},{"location":"playbook-deployment-gcloud/#production-build-with-buildkit","title":"Production Build with BuildKit","text":"<p>For deployment to Google Cloud or other platforms:</p> <pre><code># Ensure Colima and buildx are configured\ncolima start\ndocker context use colima\ndocker buildx use colima-builder\n\n# Build for linux/amd64 platform (required for GCP)\nDOCKER_BUILDKIT=1 docker buildx build \\\n  --platform linux/amd64 \\\n  -t mangetamain-processor:latest \\\n  --load .\n</code></pre> <p>Important: GCP VMs typically run on <code>linux/amd64</code> architecture. Always build for this platform when deploying to GCP.</p>"},{"location":"playbook-deployment-gcloud/#docker-compose-build","title":"Docker Compose Build","text":"<p>For multi-container applications:</p> <pre><code># Build and start all services\ndocker compose up --build\n\n# Alternative with BuildKit\nDOCKER_BUILDKIT=1 docker-compose up --build\n</code></pre> <p>Note: <code>docker-compose.yml</code> should be configured in your project root.</p>"},{"location":"playbook-deployment-gcloud/#interactive-docker-testing","title":"Interactive Docker Testing","text":"<p>Test your container before deployment:</p> <pre><code># Run container with interactive bash shell\ndocker run --rm -it \\\n  --entrypoint /bin/bash \\\n  -v \"$(pwd)/data:/app/data\" \\\n  mangetamain-processor:latest\n\n# Inside container, test imports:\npython -c \"import sys; print(sys.path); import mangetamain; print('OK')\"\n</code></pre> <p>This is useful for debugging import issues, path problems, or missing dependencies.</p>"},{"location":"playbook-deployment-gcloud/#google-cloud-platform-setup","title":"Google Cloud Platform Setup","text":""},{"location":"playbook-deployment-gcloud/#install-google-cloud-cli","title":"Install Google Cloud CLI","text":"<pre><code># Install gcloud CLI (macOS)\nbrew install --cask gcloud-cli\n\n# Alternative: Download from Google\n# https://cloud.google.com/sdk/docs/install\n</code></pre>"},{"location":"playbook-deployment-gcloud/#initial-authentication-and-configuration","title":"Initial Authentication and Configuration","text":"<pre><code># Login to Google Cloud\ngcloud auth login\n\n# Set project and zone using environment variables\nset -a &amp;&amp; source .env &amp;&amp; set +a\ngcloud config set project ${PROJECT_ID}\ngcloud config set compute/zone ${ZONE}\n\n# Verify configuration\ngcloud config list\n</code></pre>"},{"location":"playbook-deployment-gcloud/#create-env-file","title":"Create <code>.env</code> File","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># .env\nPROJECT_ID=your-gcp-project-id\nZONE=us-central1-a\nVM_NAME=mangetamain-vm\nVM_EXTERNAL_IP=xx.xx.xx.xx\n</code></pre> <p>Note: Add <code>.env</code> to <code>.gitignore</code> to keep credentials secure.</p>"},{"location":"playbook-deployment-gcloud/#vm-configuration","title":"VM Configuration","text":""},{"location":"playbook-deployment-gcloud/#create-and-access-vm","title":"Create and Access VM","text":"<pre><code># Create VM instance (example)\ngcloud compute instances create ${VM_NAME} \\\n  --zone=${ZONE} \\\n  --machine-type=e2-medium \\\n  --image-family=debian-11 \\\n  --image-project=debian-cloud \\\n  --boot-disk-size=20GB\n\n# SSH into VM\ngcloud compute ssh ${VM_NAME}\n</code></pre>"},{"location":"playbook-deployment-gcloud/#ssh-key-setup-for-non-interactive-access","title":"SSH Key Setup for Non-Interactive Access","text":"<p>Generate dedicated SSH key for deployment:</p> <pre><code># Generate ED25519 SSH key (more secure than RSA)\nssh-keygen -t ed25519 \\\n  -f ~/.ssh/deploy_key_kitbigdata \\\n  -C \"deploy@kitbigdata\" \\\n  -N \"\"\n\n# Add public key to GCP VM metadata or authorized_keys\n\n# Connect using the key\nssh -i ~/.ssh/deploy_key_kitbigdata deploy@$VM_EXTERNAL_IP\n</code></pre>"},{"location":"playbook-deployment-gcloud/#vm-startup-script","title":"VM Startup Script","text":"<p>Configure your VM to run setup commands on boot.</p> <p>Location: GCP Console \u2192 VM Instance \u2192 Edit \u2192 Automation \u2192 Startup script</p> <p>Example startup script:</p> <pre><code>#!/bin/bash\n# Install Docker if not present\nif ! command -v docker &amp;&gt; /dev/null; then\n    apt-get update\n    apt-get install -y docker.io docker-compose\nfi\n\n# Pull latest image\ndocker pull gcr.io/${PROJECT_ID}/mangetamain:latest\n\n# Run container\ndocker run -d \\\n  --name mangetamain \\\n  --restart unless-stopped \\\n  -p 8501:8501 \\\n  -v /data:/app/data \\\n  gcr.io/${PROJECT_ID}/mangetamain:latest\n</code></pre>"},{"location":"playbook-deployment-gcloud/#deployment-workflow","title":"Deployment Workflow","text":""},{"location":"playbook-deployment-gcloud/#complete-deployment-pipeline","title":"Complete Deployment Pipeline","text":""},{"location":"playbook-deployment-gcloud/#1-prepare-the-application","title":"1. Prepare the Application","text":"<pre><code># Sync dependencies\nuv sync\n\n# Run tests\nuv run pytest\n\n# Build distribution\nuv build\n</code></pre>"},{"location":"playbook-deployment-gcloud/#2-build-docker-image-for-gcp","title":"2. Build Docker Image for GCP","text":"<pre><code># Start Colima\ncolima start\n\n# Build multi-platform image\nDOCKER_BUILDKIT=1 docker buildx build \\\n  --platform linux/amd64 \\\n  -t gcr.io/${PROJECT_ID}/mangetamain:latest \\\n  --load .\n</code></pre>"},{"location":"playbook-deployment-gcloud/#3-push-to-google-container-registry","title":"3. Push to Google Container Registry","text":"<pre><code># Configure Docker for GCR authentication\ngcloud auth configure-docker\n\n# Tag image for GCR\ndocker tag mangetamain:latest gcr.io/${PROJECT_ID}/mangetamain:latest\n\n# Push to GCR\ndocker push gcr.io/${PROJECT_ID}/mangetamain:latest\n</code></pre>"},{"location":"playbook-deployment-gcloud/#4-deploy-to-vm","title":"4. Deploy to VM","text":"<pre><code># SSH into VM\ngcloud compute ssh ${VM_NAME}\n\n# On VM: Pull and run the image\ndocker pull gcr.io/${PROJECT_ID}/mangetamain:latest\n\n# Stop old container (if running)\ndocker stop mangetamain || true\ndocker rm mangetamain || true\n\n# Run new container\ndocker run -d \\\n  --name mangetamain \\\n  --restart unless-stopped \\\n  -p 8501:8501 \\\n  -v /home/deploy/data:/app/data \\\n  gcr.io/${PROJECT_ID}/mangetamain:latest\n\n# Verify it's running\ndocker ps\ndocker logs mangetamain\n</code></pre>"},{"location":"playbook-deployment-gcloud/#5-configure-firewall","title":"5. Configure Firewall","text":"<pre><code># Allow incoming traffic on port 8501\ngcloud compute firewall-rules create allow-streamlit \\\n  --allow tcp:8501 \\\n  --source-ranges 0.0.0.0/0 \\\n  --description \"Allow Streamlit traffic\"\n</code></pre>"},{"location":"playbook-deployment-gcloud/#6-access-the-application","title":"6. Access the Application","text":"<pre><code>http://VM_EXTERNAL_IP:8501\n</code></pre>"},{"location":"playbook-deployment-gcloud/#troubleshooting-deployment","title":"Troubleshooting Deployment","text":""},{"location":"playbook-deployment-gcloud/#docker-build-fails-on-gcp","title":"Docker Build Fails on GCP","text":"<p>Problem: Image works locally but fails on GCP VM.</p> <p>Solution: Ensure platform compatibility: <pre><code># Build specifically for linux/amd64\ndocker buildx build --platform linux/amd64 -t image:tag .\n</code></pre></p>"},{"location":"playbook-deployment-gcloud/#cannot-ssh-into-vm","title":"Cannot SSH into VM","text":"<p>See Troubleshooting Playbook - Locked Out of VM.</p> <p>Quick fix: Add startup script to restore SSH access: <pre><code># In GCP Console: VM \u2192 Edit \u2192 Startup Script\ncat &gt;/etc/ssh/sshd_config.d/30-google-ssh.conf &lt;&lt;'EOF'\nUsePAM yes\nPubkeyAuthentication yes\nAuthorizedKeysCommand /usr/bin/google_authorized_keys\nAuthorizedKeysCommandUser root\nEOF\nsystemctl restart ssh\n</code></pre></p>"},{"location":"playbook-deployment-gcloud/#container-exits-with-code-137","title":"Container Exits with Code 137","text":"<p>Problem: Container is killed due to memory exhaustion.</p> <p>Solutions:</p> <ol> <li> <p>Increase VM memory:    <pre><code># Stop VM\ngcloud compute instances stop ${VM_NAME}\n\n# Change machine type to one with more RAM\ngcloud compute instances set-machine-type ${VM_NAME} \\\n  --machine-type=e2-standard-2\n\n# Start VM\ngcloud compute instances start ${VM_NAME}\n</code></pre></p> </li> <li> <p>Optimize Docker image:</p> </li> <li>Use multi-stage builds</li> <li>Remove unnecessary dependencies</li> <li> <p>Clean up build artifacts</p> </li> <li> <p>Add memory limits:    <pre><code>docker run --memory=\"2g\" --memory-swap=\"2g\" your-image\n</code></pre></p> </li> </ol>"},{"location":"playbook-deployment-gcloud/#port-8501-not-accessible","title":"Port 8501 Not Accessible","text":"<p>Symptoms: Cannot access application from browser.</p> <p>Solution:</p> <ol> <li> <p>Check firewall rules:    <pre><code>gcloud compute firewall-rules list\n</code></pre></p> </li> <li> <p>Create firewall rule if missing:    <pre><code>gcloud compute firewall-rules create allow-streamlit \\\n  --allow tcp:8501 \\\n  --source-ranges 0.0.0.0/0\n</code></pre></p> </li> <li> <p>Verify container is running:    <pre><code>docker ps\ndocker logs mangetamain\n</code></pre></p> </li> <li> <p>Check VM external IP:    <pre><code>gcloud compute instances describe ${VM_NAME} \\\n  --format='get(networkInterfaces[0].accessConfigs[0].natIP)'\n</code></pre></p> </li> </ol>"},{"location":"playbook-deployment-gcloud/#apt-lock-issues-during-setup","title":"APT Lock Issues During Setup","text":"<p>See Troubleshooting Playbook - APT Lock Issues.</p> <p>Quick fix: <pre><code># Find and kill stuck processes\nsudo lsof /var/lib/dpkg/lock-frontend\nsudo kill -9 &lt;PID&gt;\n\n# Force remove stuck package\nsudo dpkg --remove --force-remove-reinstreq google-cloud-cli\n\n# Reconfigure\nsudo dpkg --configure -a\n</code></pre></p>"},{"location":"playbook-deployment-gcloud/#advanced-deployment","title":"Advanced Deployment","text":""},{"location":"playbook-deployment-gcloud/#automated-deployment-with-cloud-build","title":"Automated Deployment with Cloud Build","text":"<p>Create <code>cloudbuild.yaml</code>:</p> <pre><code>steps:\n  # Build the container image\n  - name: 'gcr.io/cloud-builders/docker'\n    args: ['build', '-t', 'gcr.io/$PROJECT_ID/mangetamain:$COMMIT_SHA', '.']\n\n  # Push the container image to GCR\n  - name: 'gcr.io/cloud-builders/docker'\n    args: ['push', 'gcr.io/$PROJECT_ID/mangetamain:$COMMIT_SHA']\n\n  # Deploy to VM\n  - name: 'gcr.io/cloud-builders/gcloud'\n    args:\n      - 'compute'\n      - 'ssh'\n      - '${_VM_NAME}'\n      - '--command=docker pull gcr.io/$PROJECT_ID/mangetamain:$COMMIT_SHA &amp;&amp; docker stop mangetamain || true &amp;&amp; docker run -d --name mangetamain -p 8501:8501 gcr.io/$PROJECT_ID/mangetamain:$COMMIT_SHA'\n\nimages:\n  - 'gcr.io/$PROJECT_ID/mangetamain:$COMMIT_SHA'\n</code></pre> <p>Trigger build: <pre><code>gcloud builds submit --config cloudbuild.yaml\n</code></pre></p>"},{"location":"playbook-deployment-gcloud/#using-docker-compose-on-gcp-vm","title":"Using Docker Compose on GCP VM","text":"<ol> <li> <p>Create <code>docker-compose.yml</code> on VM:    <pre><code>version: '3.8'\nservices:\n  mangetamain:\n    image: gcr.io/${PROJECT_ID}/mangetamain:latest\n    ports:\n      - \"8501:8501\"\n    volumes:\n      - ./data:/app/data\n    restart: unless-stopped\n</code></pre></p> </li> <li> <p>Deploy:    <pre><code># On VM\ndocker-compose pull\ndocker-compose up -d\n</code></pre></p> </li> </ol>"},{"location":"playbook-deployment-gcloud/#continuous-deployment-with-github-actions","title":"Continuous Deployment with GitHub Actions","text":"<p>Create <code>.github/workflows/deploy.yml</code>:</p> <pre><code>name: Deploy to GCP\n\non:\n  push:\n    branches: [main]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Cloud SDK\n        uses: google-github-actions/setup-gcloud@v1\n        with:\n          service_account_key: ${{ secrets.GCP_SA_KEY }}\n          project_id: ${{ secrets.GCP_PROJECT_ID }}\n\n      - name: Build and Push\n        run: |\n          gcloud auth configure-docker\n          docker build -t gcr.io/${{ secrets.GCP_PROJECT_ID }}/mangetamain:latest .\n          docker push gcr.io/${{ secrets.GCP_PROJECT_ID }}/mangetamain:latest\n\n      - name: Deploy to VM\n        run: |\n          gcloud compute ssh ${{ secrets.VM_NAME }} --command=\"\n            docker pull gcr.io/${{ secrets.GCP_PROJECT_ID }}/mangetamain:latest &amp;&amp;\n            docker stop mangetamain || true &amp;&amp;\n            docker run -d --name mangetamain -p 8501:8501 gcr.io/${{ secrets.GCP_PROJECT_ID }}/mangetamain:latest\n          \"\n</code></pre>"},{"location":"playbook-deployment-gcloud/#best-practices","title":"Best Practices","text":"<ol> <li>Always build for <code>linux/amd64</code> when deploying to GCP</li> <li>Use Container Registry (GCR) or Artifact Registry for image storage</li> <li>Tag images with version numbers or commit SHAs</li> <li>Set resource limits in docker run commands</li> <li>Use startup scripts for VM initialization</li> <li>Monitor logs: <code>docker logs -f mangetamain</code></li> <li>Enable automatic restarts: <code>--restart unless-stopped</code></li> <li>Back up data volumes regularly</li> <li>Use <code>.dockerignore</code> to reduce image size</li> <li>Test locally with same platform: <code>--platform linux/amd64</code></li> </ol>"},{"location":"playbook-deployment-gcloud/#security-considerations","title":"Security Considerations","text":"<ol> <li>Don't commit secrets to Git (use <code>.env</code> and <code>.gitignore</code>)</li> <li>Use IAM roles for service account permissions</li> <li>Restrict firewall rules to specific IP ranges when possible</li> <li>Rotate SSH keys regularly</li> <li>Keep Docker images updated with security patches</li> <li>Use Google Secret Manager for sensitive configuration</li> <li>Enable VPC firewall logging for audit trails</li> <li>Use private IPs for internal communication</li> </ol>"},{"location":"playbook-deployment-gcloud/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":""},{"location":"playbook-deployment-gcloud/#check-application-health","title":"Check Application Health","text":"<pre><code># View container logs\ndocker logs -f mangetamain\n\n# Check resource usage\ndocker stats mangetamain\n\n# Check container health\ndocker inspect mangetamain\n</code></pre>"},{"location":"playbook-deployment-gcloud/#update-deployed-application","title":"Update Deployed Application","text":"<pre><code># Pull latest image\ndocker pull gcr.io/${PROJECT_ID}/mangetamain:latest\n\n# Recreate container with new image\ndocker stop mangetamain\ndocker rm mangetamain\ndocker run -d --name mangetamain -p 8501:8501 gcr.io/${PROJECT_ID}/mangetamain:latest\n</code></pre>"},{"location":"playbook-deployment-gcloud/#backup-data","title":"Backup Data","text":"<pre><code># Create backup of data volume\ndocker run --rm \\\n  -v mangetamain_data:/data \\\n  -v $(pwd)/backup:/backup \\\n  ubuntu tar czf /backup/data-$(date +%Y%m%d).tar.gz /data\n</code></pre>"},{"location":"playbook-env/","title":"Environment Setup Playbook","text":"<p>This playbook covers environment setup, configuration, and troubleshooting for the Mangetamain project.</p>"},{"location":"playbook-env/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Initial Environment Setup</li> <li>Virtual Environment with UV</li> <li>Jupyter Kernel Configuration</li> <li>Dependency Management</li> <li>Troubleshooting</li> </ul>"},{"location":"playbook-env/#initial-environment-setup","title":"Initial Environment Setup","text":""},{"location":"playbook-env/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12 (required to avoid TensorFlow incompatibility)</li> <li>UV package manager</li> <li>VS Code (recommended)</li> </ul>"},{"location":"playbook-env/#create-project-structure","title":"Create Project Structure","text":"<pre><code># Initialize project with package structure\nuv init --package  # Creates src/ and pyproject.toml\n\n# Create virtual environment with Python 3.12\nuv venv --python 3.12\n</code></pre>"},{"location":"playbook-env/#install-development-dependencies","title":"Install Development Dependencies","text":"<pre><code># Add essential development tools\nuv add --dev ruff mypy pytest black ipykernel jupyterlab ipython\n\n# Sync all dependencies\nuv sync\n</code></pre>"},{"location":"playbook-env/#virtual-environment-with-uv","title":"Virtual Environment with UV","text":""},{"location":"playbook-env/#basic-uv-commands","title":"Basic UV Commands","text":"<pre><code># Sync dependencies from pyproject.toml and lock file\nuv sync\n\n# Sync including development dependencies\nuv sync --group dev\n\n# Build distribution packages (wheel + sdist)\nuv build\n</code></pre>"},{"location":"playbook-env/#quality-assurance-tools","title":"Quality Assurance Tools","text":"<pre><code># Check code formatting issues\nuv run ruff check .\n\n# Auto-fix formatting errors\nuv run ruff format .\n\n# Type checking for consistency\nuv run mypy .\n\n# Run unit tests\nuv run pytest\n</code></pre>"},{"location":"playbook-env/#jupyter-kernel-configuration","title":"Jupyter Kernel Configuration","text":""},{"location":"playbook-env/#install-jupyter-kernel-for-vs-code","title":"Install Jupyter Kernel for VS Code","text":"<p>Create a custom kernel for the project:</p> <pre><code>uv run ipython kernel install --user \\\n  --name=venv-3.12-mangetamain \\\n  --display-name \"Python 3.12 (mangetamain)\"\n</code></pre>"},{"location":"playbook-env/#useful-kernel-management-commands","title":"Useful Kernel Management Commands","text":"<pre><code># List all available Jupyter kernels\nuv run jupyter kernelspec list\n\n# Remove old/unused kernel\nuv run jupyter kernelspec uninstall python3\n</code></pre>"},{"location":"playbook-env/#configuration-notes","title":"Configuration Notes","text":"<ul> <li>Adapt dependencies in <code>pyproject.toml</code> after kernel installation</li> <li>The kernel name <code>venv-3.12-mangetamain</code> should match your project structure</li> <li>Reference: Create Virtual Environments with UV for Jupyter in VS Code</li> </ul>"},{"location":"playbook-env/#dependency-management","title":"Dependency Management","text":""},{"location":"playbook-env/#python-version-configuration","title":"Python Version Configuration","text":"<p>Edit <code>pyproject.toml</code>:</p> <pre><code>[tool.uv]\npython = \"3.12.*\"\n</code></pre>"},{"location":"playbook-env/#standard-dependencies","title":"Standard Dependencies","text":"<pre><code># Add runtime dependencies\nuv add package-name\n\n# Add development-only dependencies\nuv add --dev package-name\n\n# Remove dependencies\nuv remove package-name\n</code></pre>"},{"location":"playbook-env/#troubleshooting","title":"Troubleshooting","text":""},{"location":"playbook-env/#pyarrow-installation-issues","title":"PyArrow Installation Issues","text":"<p>If you encounter PyArrow compatibility issues:</p> <p>Problem: PyArrow binary build failures or version conflicts</p> <p>Solution:</p> <ol> <li> <p>Modify <code>pyproject.toml</code>:    <pre><code>[tool.uv]\npython = \"3.12.*\"\n</code></pre></p> </li> <li> <p>Force binary-only installation:    <pre><code># Set environment variable to use pre-built binaries only\nexport UV_PIP_ONLY_BINARY=\":all:\"\n# Alternative:\nexport PIP_ONLY_BINARY=\":all:\"\n</code></pre></p> </li> <li> <p>Install compatible PyArrow version:    <pre><code># Recreate virtual environment\nuv venv --recreate\n\n# Install PyArrow with version constraint\nuv add \"pyarrow&gt;=16,&lt;18\" --no-sync\n\n# Sync all dependencies\nuv sync\n</code></pre></p> </li> </ol>"},{"location":"playbook-env/#common-environment-issues","title":"Common Environment Issues","text":""},{"location":"playbook-env/#issue-dependencies-not-found-after-installation","title":"Issue: Dependencies Not Found After Installation","text":"<p>Solution: <pre><code># Clear cache and reinstall\nuv cache clean\nuv sync --reinstall\n</code></pre></p>"},{"location":"playbook-env/#issue-virtual-environment-not-activated","title":"Issue: Virtual Environment Not Activated","text":"<p>Solution: <pre><code># Check if virtual environment exists\nls .venv\n\n# Activate manually (if needed)\nsource .venv/bin/activate  # Unix/macOS\n# or\n.venv\\Scripts\\activate  # Windows\n</code></pre></p>"},{"location":"playbook-env/#issue-python-version-mismatch","title":"Issue: Python Version Mismatch","text":"<p>Solution: <pre><code># Recreate venv with specific Python version\nuv venv --recreate --python 3.12\n\n# Verify Python version\nuv run python --version\n</code></pre></p>"},{"location":"playbook-env/#issue-jupyter-kernel-not-appearing-in-vs-code","title":"Issue: Jupyter Kernel Not Appearing in VS Code","text":"<p>Solution: <pre><code># Reinstall kernel\nuv run jupyter kernelspec uninstall venv-3.12-mangetamain\nuv run ipython kernel install --user \\\n  --name=venv-3.12-mangetamain \\\n  --display-name \"Python 3.12 (mangetamain)\"\n\n# Restart VS Code\n# Select the correct kernel in notebook interface\n</code></pre></p>"},{"location":"playbook-env/#environment-variables","title":"Environment Variables","text":"<p>When working with Google Cloud or other services requiring credentials:</p> <pre><code># Load environment variables from .env file\nset -a &amp;&amp; source .env &amp;&amp; set +a\n</code></pre>"},{"location":"playbook-env/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always sync after modifying dependencies:    <pre><code>uv sync\n</code></pre></p> </li> <li> <p>Use specific Python versions to avoid compatibility issues</p> </li> <li> <p>Keep development dependencies separate using <code>--dev</code> flag</p> </li> <li> <p>Regularly update dependencies:    <pre><code>uv sync --upgrade\n</code></pre></p> </li> <li> <p>Use virtual environments to isolate project dependencies</p> </li> <li> <p>Document custom requirements in <code>pyproject.toml</code> comments</p> </li> </ol>"},{"location":"playbook-https-setup/","title":"Quick HTTPS Setup for mangetamain.duckdns.org","text":""},{"location":"playbook-https-setup/#port-requirements-explained","title":"\ud83d\udccc Port Requirements Explained","text":"<ul> <li>Port 80 (HTTP): Only needed temporarily during Let's Encrypt certificate setup for domain validation. Automatically closed after setup.</li> <li>Port 443 (HTTPS): Standard HTTPS port. Docker maps external 443 to internal 8501 where Streamlit runs.</li> <li>Port 8501: Internal Streamlit port (not exposed externally).</li> </ul> <p>Your final URL: <code>https://mangetamain.duckdns.org</code> (no port number needed!)</p>"},{"location":"playbook-https-setup/#automatic-deployment-via-cicd","title":"\ud83d\udd04 Automatic Deployment via CI/CD","text":"<p>The HTTPS setup is integrated into the GitHub Actions deployment workflow: - \u2705 Automatically runs on first deployment if certificate doesn't exist - \u2705 Skips setup on subsequent deployments (checks for existing certificate) - \u2705 Cron job for renewal is only added once (prevents duplicates) - \u2705 Port 80 is automatically closed after certificate setup - \u2705 Certificate auto-renews every 2 months</p> <p>Manual setup is only needed if CI/CD hasn't run yet.</p>"},{"location":"playbook-https-setup/#quick-start-3-steps","title":"\ud83d\ude80 Quick Start (3 Steps)","text":""},{"location":"playbook-https-setup/#step-1-update-gcp-firewall-run-from-local-machine","title":"Step 1: Update GCP Firewall (Run from local machine)","text":"<pre><code># Allow HTTP for Let's Encrypt validation (temporary, only during setup)\ngcloud compute firewall-rules create allow-http \\\n    --project=KitBigData-mangetamain \\\n    --allow tcp:80 \\\n    --source-ranges 0.0.0.0/0 \\\n    --description \"Allow HTTP for Let's Encrypt validation\"\n\n# Allow standard HTTPS port (permanent)\ngcloud compute firewall-rules create allow-https \\\n    --project=KitBigData-mangetamain \\\n    --allow tcp:443 \\\n    --source-ranges 0.0.0.0/0 \\\n    --description \"Allow HTTPS on port 443\"\n\n# Optional: Close port 80 after setup if you don't need it\n# gcloud compute firewall-rules delete allow-http --project=KitBigData-mangetamain\n</code></pre>"},{"location":"playbook-https-setup/#step-2-copy-setup-script-to-vm","title":"Step 2: Copy Setup Script to VM","text":"<pre><code># From your local machine, in the project root\ncd /Users/corentinmergny/Documents/code_local/data_ai700_kitbigdata_telecomparis_p1/kit_big_data_mangetamain\n\n# Copy the setup script to VM\ngcloud compute scp scripts/setup_https_mangetamain.sh kit-big-data-1:/tmp/ \\\n    --zone=europe-west9-b\n</code></pre>"},{"location":"playbook-https-setup/#step-3-run-setup-on-vm","title":"Step 3: Run Setup on VM","text":"<pre><code># SSH into your VM\ngcloud compute ssh kit-big-data-1 --zone=europe-west9-b\n\n# Run the setup script\nchmod +x /tmp/setup_https_mangetamain.sh\nsudo /tmp/setup_https_mangetamain.sh\n\n# That's it! The script will:\n# - Install Certbot\n# - Get Let's Encrypt certificate\n# - Configure Streamlit for HTTPS\n# - Update Docker Compose\n# - Setup auto-renewal\n# - Start your app with HTTPS\n</code></pre>"},{"location":"playbook-https-setup/#after-setup","title":"\u2705 After Setup","text":"<p>Your app will be available at: https://mangetamain.duckdns.org (standard HTTPS, no port needed!)</p> <p>The Docker container maps: - External port 443 (HTTPS) \u2192 Internal port 8501 (Streamlit)</p>"},{"location":"playbook-https-setup/#verify-setup","title":"\ud83d\udd0d Verify Setup","text":"<pre><code># Check if certificate was obtained\nsudo certbot certificates\n\n# Check if Docker is running\nsudo docker ps\n\n# View Streamlit logs\nsudo docker logs streamlit-app\n\n# Test HTTPS connection\ncurl -I https://mangetamain.duckdns.org:8501\n</code></pre>"},{"location":"playbook-https-setup/#configuration-files","title":"\ud83d\udccb Configuration Files","text":"<p>After setup, these files will exist on your VM:</p> <pre><code>/opt/app/\n\u251c\u2500\u2500 .streamlit/\n\u2502   \u2514\u2500\u2500 config.toml              # Streamlit HTTPS config\n\u251c\u2500\u2500 docker-compose.yml           # Updated with certificate mounts\n\u251c\u2500\u2500 docker-compose.yml.backup    # Original backup\n\u2514\u2500\u2500 data/\n\n/etc/letsencrypt/live/mangetamain.duckdns.org/\n\u251c\u2500\u2500 fullchain.pem               # SSL certificate\n\u2514\u2500\u2500 privkey.pem                 # Private key\n</code></pre>"},{"location":"playbook-https-setup/#certificate-auto-renewal","title":"\ud83d\udd04 Certificate Auto-Renewal","text":"<ul> <li>Certificates renew automatically every 2 months</li> <li>Cron job: <code>0 0 1 */2 *</code> (1st day of every 2nd month at midnight)</li> <li>Logs: <code>/var/log/certbot-renewal.log</code></li> </ul>"},{"location":"playbook-https-setup/#manual-renewal","title":"Manual Renewal","text":"<pre><code># Test renewal (dry run)\nsudo certbot renew --dry-run\n\n# Force renewal now\nsudo certbot renew --force-renewal\n\n# Restart app after manual renewal\ncd /opt/app &amp;&amp; sudo docker compose restart streamlit\n</code></pre>"},{"location":"playbook-https-setup/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"playbook-https-setup/#port-80443-already-in-use","title":"Port 80/443 already in use","text":"<pre><code># Check what's using the ports\nsudo lsof -i :80\nsudo lsof -i :443\n\n# Stop Docker if needed\ncd /opt/app\nsudo docker compose down\n\n# Try setup again\nsudo /tmp/setup_https_mangetamain.sh\n</code></pre>"},{"location":"playbook-https-setup/#certificate-not-working","title":"Certificate not working","text":"<pre><code># Check certificate\nsudo certbot certificates\n\n# Check Streamlit config\ncat /opt/app/.streamlit/config.toml\n\n# Check Docker volumes\nsudo docker inspect streamlit-app | grep -A 10 Mounts\n\n# Restart Docker\ncd /opt/app\nsudo docker compose restart\n</code></pre>"},{"location":"playbook-https-setup/#browser-shows-not-secure","title":"Browser shows \"Not Secure\"","text":"<ol> <li>Make sure you're accessing <code>https://</code> not <code>http://</code></li> <li>Check certificate expiry: <code>sudo certbot certificates</code></li> <li>Verify domain points to correct IP: <code>nslookup mangetamain.duckdns.org</code></li> </ol>"},{"location":"playbook-https-setup/#manual-rollback-if-needed","title":"\ud83d\udd27 Manual Rollback (if needed)","text":"<p>If something goes wrong:</p> <pre><code># Restore original docker-compose\ncd /opt/app\nsudo mv docker-compose.yml.backup docker-compose.yml\n\n# Remove SSL config\nsudo rm -rf /opt/app/.streamlit/\n\n# Restart without HTTPS\nsudo docker compose down\nsudo docker compose up -d\n</code></pre>"},{"location":"playbook-https-setup/#support-commands","title":"\ud83d\udcde Support Commands","text":"<pre><code># View all logs\nsudo docker compose logs -f\n\n# Check cron jobs\ncrontab -l\n\n# Check firewall\nsudo ufw status\n\n# View certificate files\nsudo ls -la /etc/letsencrypt/live/mangetamain.duckdns.org/\n</code></pre>"},{"location":"playbook-https-setup/#expected-result","title":"\ud83c\udfaf Expected Result","text":"<p>After successful setup:</p> <ul> <li>\u2705 App accessible at <code>https://mangetamain.duckdns.org:8501</code></li> <li>\u2705 Valid SSL certificate (no browser warnings)</li> <li>\u2705 Auto-renewal configured</li> <li>\u2705 HTTP traffic works (redirected to HTTPS)</li> <li>\u2705 Certificates stored at <code>/etc/letsencrypt/live/mangetamain.duckdns.org/</code></li> </ul>"},{"location":"playbook-precommit/","title":"Pre-Commit Workflow Playbook","text":"<p>This playbook provides a comprehensive guide for running linting, formatting, testing, and documentation commands before committing code.</p>"},{"location":"playbook-precommit/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Quick Start</li> <li>Code Documentation</li> <li>Linting and Formatting</li> <li>Testing and Coverage</li> <li>Documentation Generation</li> <li>Pre-Commit Hooks</li> <li>Workflow Summary</li> </ul>"},{"location":"playbook-precommit/#quick-start","title":"Quick Start","text":""},{"location":"playbook-precommit/#essential-commands","title":"Essential Commands","text":"<pre><code># 1. Sync dependencies\nuv sync\nuv sync --group dev\n\n# 2. Format code\nuv run ruff format .\nuv run isort .\nuv run black .\n\n# 3. Check for issues\nuv run ruff check . --fix\n\n# 4. Run tests\nuv run pytest\n\n# 5. Generate documentation\nuv run hatch run docs:serve\n</code></pre>"},{"location":"playbook-precommit/#code-documentation","title":"Code Documentation","text":""},{"location":"playbook-precommit/#generate-docstrings-with-pyment","title":"Generate Docstrings with Pyment","text":"<p>\u26a0\ufe0f WARNING: Pyment writes directly to your files. Commit your changes before running!</p> <pre><code># Generate Google-style docstrings for entire src/ directory\nuv run pyment -w -o google src/\n</code></pre> <p>After running Pyment: 1. Review all modified files carefully 2. Check that docstrings are accurate and complete 3. Use GitHub Copilot to improve docstrings if needed 4. Fix any formatting issues</p>"},{"location":"playbook-precommit/#check-documentation-coverage","title":"Check Documentation Coverage","text":"<pre><code># Run documentation coverage check\nuv run hatch run dev:docs-cov\n</code></pre> <p>This command analyzes how much of your code has proper documentation.</p>"},{"location":"playbook-precommit/#linting-and-formatting","title":"Linting and Formatting","text":""},{"location":"playbook-precommit/#code-quality-checks","title":"Code Quality Checks","text":"<pre><code># Check for linting issues with statistics\nuv run ruff check . --statistics\n\n# Auto-fix linting issues\nuv run ruff check . --fix\n\n# Fix issues with Hatch dev environment\nuv run hatch run dev:lint --fix\n</code></pre>"},{"location":"playbook-precommit/#code-formatting","title":"Code Formatting","text":"<pre><code># Format code with Ruff (recommended)\nuv run ruff format .\n\n# Sort imports\nuv run isort .\n\n# Format with Black\nuv run black .\n</code></pre>"},{"location":"playbook-precommit/#advanced-ruff-usage","title":"Advanced Ruff Usage","text":"<p>With folder exclusions (if folders are excluded in <code>pyproject.toml</code>): <pre><code>uv run ruff check .\n</code></pre></p> <p>Manual folder exclusions: <pre><code>uv run ruff check . \\\n  --exclude notebook \\\n  --exclude src/utils \\\n  --exclude src/mypkg \\\n  --exclude src/archive \\\n  --exclude .venv \\\n  --fix \\\n  --unsafe-fixes\n</code></pre></p>"},{"location":"playbook-precommit/#linting-order-of-operations","title":"Linting Order of Operations","text":"<p>Recommended sequence for clean code:</p> <ol> <li> <p>Format first:    <pre><code>uv run ruff format .\nuv run isort .\nuv run black .\n</code></pre></p> </li> <li> <p>Then check and fix issues:    <pre><code>uv run ruff check . --fix\n</code></pre></p> </li> <li> <p>Finally, verify:    <pre><code>uv run ruff check . --statistics\n</code></pre></p> </li> </ol>"},{"location":"playbook-precommit/#testing-and-coverage","title":"Testing and Coverage","text":""},{"location":"playbook-precommit/#run-unit-tests","title":"Run Unit Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run with coverage report\nuv run coverage run -m pytest &amp;&amp; uv run coverage report -m\n\n# Alternative syntax (if 'ur' alias is configured)\nur coverage run -m pytest &amp;&amp; ur coverage report -m\n</code></pre>"},{"location":"playbook-precommit/#run-specific-tests","title":"Run Specific Tests","text":"<pre><code># Run a specific test file\nuv run pytest tests/unit/mangetamain/backend/test_recipe_analyzer.py\n\n# Run a specific test class\nuv run pytest tests/unit/mangetamain/backend/test_recipe_analyzer.py::TestRecipeAnalyzer\n\n# Run a specific test method\nuv run pytest tests/unit/mangetamain/backend/test_recipe_analyzer.py::TestRecipeAnalyzer::test_compare_frequency_and_tfidf_returns_figure\n</code></pre>"},{"location":"playbook-precommit/#clear-test-cache","title":"Clear Test Cache","text":"<p>If tests are failing unexpectedly:</p> <pre><code># Remove pytest cache\nrm -rf tests/unit/mangetamain/backend/__pycache__\n\n# Rerun tests\nuv run coverage run -m pytest &amp;&amp; uv run coverage report -m\n</code></pre>"},{"location":"playbook-precommit/#coverage-best-practices","title":"Coverage Best Practices","text":"<ul> <li>Aim for 80%+ coverage on core business logic</li> <li>Check coverage report to identify untested code</li> <li>Add tests for edge cases and error handling</li> </ul>"},{"location":"playbook-precommit/#documentation-generation","title":"Documentation Generation","text":""},{"location":"playbook-precommit/#install-mkdocs-dependencies","title":"Install MkDocs Dependencies","text":"<p>If any dependencies are missing:</p> <pre><code># Core documentation packages\nuv add mkdocs-material\nuv add mkdocstrings\nuv add mkdocstrings-python\nuv add mkdocs-gen-files\nuv add mkdocs-literate-nav\nuv add mkdocs-section-index\nuv add pymdownx\n</code></pre> <p>Note: Try different variations if package names differ: - <code>mkdocs-sections-index</code> - <code>mkdocs-section-index</code></p>"},{"location":"playbook-precommit/#serve-documentation-locally","title":"Serve Documentation Locally","text":"<pre><code># Start local documentation server\nuv run hatch run docs:serve\n</code></pre> <p>This will typically serve documentation at <code>http://127.0.0.1:8000</code>.</p>"},{"location":"playbook-precommit/#build-documentation","title":"Build Documentation","text":"<pre><code># Build static documentation site\nuv run mkdocs build\n\n# Deploy to GitHub Pages\nuv run mkdocs gh-deploy\n</code></pre>"},{"location":"playbook-precommit/#pre-commit-hooks","title":"Pre-Commit Hooks","text":""},{"location":"playbook-precommit/#install-pre-commit","title":"Install Pre-Commit","text":"<pre><code># Add pre-commit to project\nuv add pre-commit\n\n# Install Git hooks\npre-commit install\n</code></pre>"},{"location":"playbook-precommit/#run-pre-commit","title":"Run Pre-Commit","text":"<pre><code># Run on all files\npre-commit run --all-files\n\n# Run automatically on git commit\ngit commit -m \"your message\"\n</code></pre>"},{"location":"playbook-precommit/#bypass-pre-commit-use-sparingly","title":"Bypass Pre-Commit (Use Sparingly)","text":"<pre><code># Skip pre-commit hooks (not recommended)\ngit commit -n -m \"emergency fix\"\n# or\ngit commit --no-verify -m \"emergency fix\"\n</code></pre> <p>\u26a0\ufe0f Only bypass pre-commit for: - Critical hotfixes - Emergency production issues - Work-in-progress commits on feature branches</p>"},{"location":"playbook-precommit/#workflow-summary","title":"Workflow Summary","text":""},{"location":"playbook-precommit/#full-pre-commit-checklist","title":"Full Pre-Commit Checklist","text":"<p>Run these commands in order before committing:</p> <pre><code># 1. Sync dependencies\nuv sync --group dev\n\n# 2. Generate/update docstrings (once or after adding files)\n# \u26a0\ufe0f Commit before running!\nuv run pyment -w -o google src/\n\n# 3. Review docstrings manually\n# Use GitHub Copilot for improvements\n\n# 4. Check documentation coverage\nuv run hatch run dev:docs-cov\n\n# 5. Format code\nuv run ruff format .\nuv run isort .\nuv run black .\n\n# 6. Fix linting issues\nuv run ruff check . --fix\nuv run hatch run dev:lint --fix\n\n# 7. Run tests with coverage\nuv run coverage run -m pytest &amp;&amp; uv run coverage report -m\n\n# 8. Verify no linting issues remain\nuv run ruff check . --statistics\n\n# 9. Serve documentation to verify\nuv run hatch run docs:serve\n\n# 10. Commit changes\ngit add .\ngit commit -m \"descriptive commit message\"\n</code></pre>"},{"location":"playbook-precommit/#quick-pre-commit-minimal","title":"Quick Pre-Commit (Minimal)","text":"<p>For smaller changes:</p> <pre><code># Format and fix\nuv run ruff format .\nuv run ruff check . --fix\n\n# Test\nuv run pytest\n\n# Commit\ngit commit -m \"message\"\n</code></pre>"},{"location":"playbook-precommit/#configuration-files","title":"Configuration Files","text":""},{"location":"playbook-precommit/#pyprojecttoml","title":"pyproject.toml","text":"<p>Ensure your <code>pyproject.toml</code> includes:</p> <pre><code>[tool.ruff]\nexclude = [\n    \".venv\",\n    \"notebook\",\n    \"src/archive\",\n]\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"I\"]  # Error, Flake8, Import sorting\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\n</code></pre>"},{"location":"playbook-precommit/#install-pre-commit-hook","title":"Install pre-commit hook","text":""},{"location":"playbook-precommit/#then-install-the-precommit","title":"Then install the precommit","text":"<pre><code>uv install pre-commit\nuv add pre-commit\npre-commit install\npre-commit run --all-files\n</code></pre>"},{"location":"playbook-precommit/#avoid-precommit-with","title":"avoid precommit with","text":"<pre><code>git commit -nm 'my message'\n````\n\n### .pre-commit-config.yaml\n\nExample pre-commit configuration:\n\n```yaml\nrepos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.1.0\n    hooks:\n      - id: ruff\n        args: [--fix]\n      - id: ruff-format\n\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.5.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n</code></pre>"},{"location":"playbook-precommit/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ol> <li> <p>Run linters frequently during development, not just before commits</p> </li> <li> <p>Use auto-fix carefully: Review changes made by <code>--fix</code> flags</p> </li> <li> <p>Keep tests fast: Slow tests discourage running them frequently</p> </li> <li> <p>Document complex logic: Focus docstring efforts on non-obvious code</p> </li> <li> <p>Commit formatting separately: Makes code review easier    <pre><code>git commit -m \"style: format code with ruff and black\"\ngit commit -m \"feat: add new feature X\"\n</code></pre></p> </li> <li> <p>Use coverage to find gaps: Don't just aim for high numbers; test meaningful scenarios</p> </li> <li> <p>Update documentation: Keep MkDocs content in sync with code changes</p> </li> </ol>"},{"location":"playbook-troubleshooting/","title":"Troubleshooting Playbook","text":"<p>This playbook contains tips, tricks, and solutions for common issues encountered in the Mangetamain project.</p>"},{"location":"playbook-troubleshooting/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Python Environment Issues</li> <li>Docker Issues</li> <li>Google Cloud Platform Issues</li> <li>Testing Issues</li> <li>Dependency Issues</li> <li>SSH and Access Issues</li> </ul>"},{"location":"playbook-troubleshooting/#python-environment-issues","title":"Python Environment Issues","text":""},{"location":"playbook-troubleshooting/#pyarrow-installation-failures","title":"PyArrow Installation Failures","text":"<p>Symptoms: - Build errors when installing PyArrow - \"Failed building wheel for pyarrow\" - Version conflicts with other packages</p> <p>Solution:</p> <ol> <li> <p>Configure Python version in <code>pyproject.toml</code>:    <pre><code>[tool.uv]\npython = \"3.12.*\"\n</code></pre></p> </li> <li> <p>Force binary-only installation to avoid compilation:    <pre><code>export UV_PIP_ONLY_BINARY=\":all:\"\n# Alternative environment variable:\nexport PIP_ONLY_BINARY=\":all:\"\n</code></pre></p> </li> <li> <p>Install compatible PyArrow version:    <pre><code># Recreate virtual environment\nuv venv --recreate\n\n# Install PyArrow with version constraints\nuv add \"pyarrow&gt;=16,&lt;18\" --no-sync\n\n# Sync all dependencies\nuv sync\n</code></pre></p> </li> </ol> <p>Why it works: Using pre-built binary wheels avoids compilation issues, and the version constraint ensures compatibility with other packages.</p>"},{"location":"playbook-troubleshooting/#virtual-environment-not-activating","title":"Virtual Environment Not Activating","text":"<p>Symptoms: - Commands not found after installation - Wrong Python version being used</p> <p>Solution: <pre><code># Verify virtual environment exists\nls -la .venv\n\n# Manually activate (if needed)\nsource .venv/bin/activate\n\n# Verify activation\nwhich python\npython --version\n</code></pre></p>"},{"location":"playbook-troubleshooting/#jupyter-kernel-not-appearing-in-vs-code","title":"Jupyter Kernel Not Appearing in VS Code","text":"<p>Symptoms: - Kernel not listed in VS Code notebook interface - \"No kernel found\" error</p> <p>Solution: <pre><code># List existing kernels\nuv run jupyter kernelspec list\n\n# Remove old kernel if it exists\nuv run jupyter kernelspec uninstall venv-3.12-mangetamain\n\n# Reinstall kernel\nuv run ipython kernel install --user \\\n  --name=venv-3.12-mangetamain \\\n  --display-name \"Python 3.12 (mangetamain)\"\n\n# Restart VS Code completely\n</code></pre></p>"},{"location":"playbook-troubleshooting/#docker-issues","title":"Docker Issues","text":""},{"location":"playbook-troubleshooting/#docker-exit-code-137-out-of-memory","title":"Docker Exit Code 137 (Out of Memory)","text":"<p>Symptoms: - Container exits with code 137 - \"Killed\" message in logs - Docker stops unexpectedly during build or runtime</p> <p>Problem: Container is using too much memory and is killed by the system.</p> <p>Solutions:</p> <ol> <li> <p>Increase Docker memory limit:    <pre><code># For Docker Desktop (macOS)\n# Go to Docker Desktop \u2192 Settings \u2192 Resources \u2192 Memory\n# Increase to at least 4GB (8GB recommended)\n\n# For Colima\ncolima stop\ncolima start --memory 8\n</code></pre></p> </li> <li> <p>Optimize Dockerfile:</p> </li> <li>Use multi-stage builds</li> <li>Clean up unnecessary files</li> <li>Reduce layer size</li> <li> <p>Use <code>.dockerignore</code></p> </li> <li> <p>Monitor memory usage:    <pre><code># Check container memory usage\ndocker stats\n\n# Run container with memory limit\ndocker run --memory=\"2g\" --memory-swap=\"2g\" your-image\n</code></pre></p> </li> </ol>"},{"location":"playbook-troubleshooting/#colima-docker-context-issues","title":"Colima Docker Context Issues","text":"<p>Symptoms: - \"Cannot connect to Docker daemon\" - Docker commands fail</p> <p>Solution: <pre><code># Start Colima\ncolima start\n\n# Switch to Colima context\ndocker context use colima\n\n# Verify context\ndocker context ls\ndocker ps\n</code></pre></p>"},{"location":"playbook-troubleshooting/#docker-buildkit-issues","title":"Docker BuildKit Issues","text":"<p>Symptoms: - Build failures with multi-platform images - \"builder not found\" error</p> <p>Solution: <pre><code># Ensure Colima is running\ncolima start\n\n# Create buildx builder (one-time setup)\ndocker buildx create --name colima-builder \\\n  --use \\\n  --driver docker-container \\\n  --use\n\n# Initialize builder\ndocker buildx inspect --bootstrap\n\n# Verify builder\ndocker buildx ls\n\n# Build with BuildKit\nDOCKER_BUILDKIT=1 docker buildx build \\\n  --platform linux/amd64 \\\n  -t mangetamain-processor:latest \\\n  --load .\n</code></pre></p>"},{"location":"playbook-troubleshooting/#google-cloud-platform-issues","title":"Google Cloud Platform Issues","text":""},{"location":"playbook-troubleshooting/#locked-out-of-vm-no-ssh-access","title":"Locked Out of VM (No SSH Access)","text":"<p>Symptoms: - Cannot SSH into VM - Google Cloud Console SSH not working - \"Permission denied\" errors</p> <p>Solution: Add startup script to restore SSH access</p> <ol> <li>Go to Google Cloud Console:</li> <li>Navigate to Compute Engine \u2192 VM Instances</li> <li> <p>Select your VM \u2192 Edit</p> </li> <li> <p>Add this startup script:    <pre><code># Restore settings that GCE Web SSH relies on (metadata-injected SSH keys)\ncat &gt;/etc/ssh/sshd_config.d/30-google-ssh.conf &lt;&lt;'EOF'\n# Allow Google to inject temporary SSH keys\nUsePAM yes\nPubkeyAuthentication yes\nPasswordAuthentication no\nChallengeResponseAuthentication no\nAuthorizedKeysCommand /usr/bin/google_authorized_keys\nAuthorizedKeysCommandUser root\nPermitRootLogin prohibit-password\nEOF\n\n# Remove hard \"AllowUsers\" restrictions\nsed -i '/^AllowUsers/d' /etc/ssh/sshd_config || true\n\n# Restart services\nsystemctl restart google-guest-agent || true\nsystemctl restart ssh || systemctl restart sshd || true\n\necho \"Startup script finished\"\n</code></pre></p> </li> <li> <p>Save and restart the VM</p> </li> </ol> <p>Why it works: This restores the default Google Cloud SSH configuration that allows authentication via metadata-injected keys.</p>"},{"location":"playbook-troubleshooting/#apt-lock-issues-on-google-cloud-vm","title":"APT Lock Issues on Google Cloud VM","text":"<p>Symptoms: - <code>dpkg</code> or <code>apt</code> commands hang - \"Unable to acquire dpkg frontend lock\" - Package installation fails</p> <p>Solution:</p> <ol> <li> <p>Check for stuck processes:    <pre><code># List processes holding the lock\nsudo lsof /var/lib/dpkg/lock-frontend /var/lib/dpkg/lock 2&gt;/dev/null\n</code></pre></p> </li> <li> <p>Identify and kill stuck process:    <pre><code># Example: Check specific PID\nps -p 283985 -o pid,ppid,stat,etime,cmd\n\n# Kill stuck processes (use actual PIDs from lsof)\nsudo kill -9 283985 283984\n</code></pre></p> </li> <li> <p>Force remove problematic package:    <pre><code># If google-cloud-cli is stuck\nsudo dpkg --remove --force-remove-reinstreq google-cloud-cli\n</code></pre></p> </li> <li> <p>Clean up and retry:    <pre><code># Remove lock files (if processes are truly dead)\nsudo rm /var/lib/dpkg/lock-frontend\nsudo rm /var/lib/dpkg/lock\nsudo rm /var/cache/apt/archives/lock\n\n# Reconfigure packages\nsudo dpkg --configure -a\n\n# Update package lists\nsudo apt update\n</code></pre></p> </li> </ol>"},{"location":"playbook-troubleshooting/#google-cloud-authentication-issues","title":"Google Cloud Authentication Issues","text":"<p>Symptoms: - <code>gcloud</code> commands fail with authentication errors - Cannot access GCP resources</p> <p>Solution: <pre><code># Login to Google Cloud\ngcloud auth login\n\n# Login for application default credentials\ngcloud auth application-default login\n\n# Set project\ngcloud config set project YOUR_PROJECT_ID\n\n# Verify configuration\ngcloud config list\n</code></pre></p>"},{"location":"playbook-troubleshooting/#testing-issues","title":"Testing Issues","text":""},{"location":"playbook-troubleshooting/#tests-fail-after-passing-previously","title":"Tests Fail After Passing Previously","text":"<p>Symptoms: - Tests pass individually but fail together - Inconsistent test results</p> <p>Solution: <pre><code># Clear pytest cache\nrm -rf .pytest_cache\nrm -rf tests/**/__pycache__\n\n# Specific directory cleanup\nrm -rf tests/unit/mangetamain/backend/__pycache__\n\n# Run tests again\nuv run pytest -v\n</code></pre></p>"},{"location":"playbook-troubleshooting/#mock-objects-not-working-correctly","title":"Mock Objects Not Working Correctly","text":"<p>Symptoms: - <code>AttributeError</code> in mocked objects - Tests fail with \"MagicMock has no attribute X\"</p> <p>Solution: <pre><code># Use proper mock configuration\nfrom unittest.mock import MagicMock, patch\n\n# Configure mock return values\nmock_obj = MagicMock()\nmock_obj.method.return_value = expected_value\n\n# Configure mock attributes\nmock_obj.attribute = expected_value\n\n# Use spec to match real object\nmock_obj = MagicMock(spec=RealClass)\n</code></pre></p>"},{"location":"playbook-troubleshooting/#coverage-report-doesnt-match-expected","title":"Coverage Report Doesn't Match Expected","text":"<p>Symptoms: - Coverage shows lines as uncovered that should be covered - Coverage percentage is lower than expected</p> <p>Solution: <pre><code># Clean previous coverage data\nrm -rf .coverage\nrm -rf htmlcov/\n\n# Run coverage with verbose output\nuv run coverage run -m pytest -v\nuv run coverage report -m\n\n# Generate HTML report for detailed view\nuv run coverage html\n# Open htmlcov/index.html in browser\n</code></pre></p>"},{"location":"playbook-troubleshooting/#dependency-issues","title":"Dependency Issues","text":""},{"location":"playbook-troubleshooting/#conflicting-package-versions","title":"Conflicting Package Versions","text":"<p>Symptoms: - <code>pip</code> or <code>uv</code> reports version conflicts - Import errors after installation</p> <p>Solution: <pre><code># Check dependency tree\nuv pip tree\n\n# Force reinstall with fresh lockfile\nrm uv.lock\nuv sync\n\n# Use specific version constraints in pyproject.toml\n# Example:\n# dependencies = [\n#     \"package&gt;=1.0,&lt;2.0\",\n# ]\n</code></pre></p>"},{"location":"playbook-troubleshooting/#import-errors-after-installing-package","title":"Import Errors After Installing Package","text":"<p>Symptoms: - <code>ModuleNotFoundError</code> despite package being installed - Package shows in <code>uv pip list</code> but can't be imported</p> <p>Solution: <pre><code># Verify you're using the right Python environment\nuv run python -c \"import sys; print(sys.executable)\"\n\n# Check if package is in site-packages\nuv run python -c \"import sys; print(sys.path)\"\n\n# Reinstall package\nuv remove package-name\nuv add package-name\nuv sync\n</code></pre></p>"},{"location":"playbook-troubleshooting/#ssh-and-access-issues","title":"SSH and Access Issues","text":""},{"location":"playbook-troubleshooting/#ssh-key-authentication-fails","title":"SSH Key Authentication Fails","text":"<p>Symptoms: - \"Permission denied (publickey)\" - SSH asks for password when it shouldn't</p> <p>Solution: <pre><code># Generate new SSH key\nssh-keygen -t ed25519 -f ~/.ssh/deploy_key_kitbigdata \\\n  -C \"deploy@kitbigdata\" -N \"\"\n\n# Check key permissions (must be 600)\nchmod 600 ~/.ssh/deploy_key_kitbigdata\nchmod 644 ~/.ssh/deploy_key_kitbigdata.pub\n\n# Add key to SSH agent\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/deploy_key_kitbigdata\n\n# Test connection with verbose output\nssh -v -i ~/.ssh/deploy_key_kitbigdata user@host\n</code></pre></p>"},{"location":"playbook-troubleshooting/#general-tips","title":"General Tips","text":""},{"location":"playbook-troubleshooting/#enable-verbose-logging","title":"Enable Verbose Logging","text":"<p>For debugging any command: <pre><code># Python\npython -v script.py\n\n# SSH\nssh -v user@host\n\n# Docker\ndocker build --progress=plain .\n\n# gcloud\ngcloud compute ssh VM_NAME --verbosity=debug\n</code></pre></p>"},{"location":"playbook-troubleshooting/#check-system-resources","title":"Check System Resources","text":"<pre><code># Disk space\ndf -h\n\n# Memory usage\nfree -h\n\n# Running processes\ntop\nhtop  # if installed\n\n# Docker resources\ndocker system df\ndocker system prune  # Clean up unused resources\n</code></pre>"},{"location":"playbook-troubleshooting/#environment-variable-debugging","title":"Environment Variable Debugging","text":"<pre><code># Print all environment variables\nenv\n\n# Print specific variable\necho $VARIABLE_NAME\n\n# Check if variable is set\n[[ -z \"$VARIABLE_NAME\" ]] &amp;&amp; echo \"Not set\" || echo \"Set: $VARIABLE_NAME\"\n</code></pre>"},{"location":"playbook-troubleshooting/#getting-help","title":"Getting Help","text":"<p>When reporting issues, include:</p> <ol> <li>Error message (full output)</li> <li>Command executed (exact command)</li> <li>Environment info:    <pre><code>python --version\nuv --version\ndocker --version\nuname -a\n</code></pre></li> <li>Steps to reproduce the issue</li> <li>What you've already tried</li> </ol> <p>Useful diagnostic commands: <pre><code># Check Python environment\nuv run python -c \"import sys; print(sys.version); print(sys.executable); print(sys.path)\"\n\n# Check installed packages\nuv pip list\n\n# Check Docker setup\ndocker info\ndocker context ls\n\n# Check gcloud configuration\ngcloud config list\ngcloud auth list\n</code></pre></p>"},{"location":"profiling/","title":"Profiling Guide","text":""},{"location":"profiling/#how-to-use","title":"How to use","text":""},{"location":"profiling/#profiling-using-cprofile","title":"Profiling using cProfile","text":"<ul> <li> <p>run</p> <pre><code>streamlit run src/mangetamain/streamlit_ui.py profile\n</code></pre> </li> <li> <p>after starting application, test the web as you wish and stop after finish</p> </li> <li> <p>visualize</p> <pre><code>snakeviz docs/streamlit_profile.prof\n</code></pre> </li> <li> <p>open the visualization using this link</p> </li> </ul>"},{"location":"profiling/#profiling-in-realtime","title":"Profiling in realtime","text":"<ul> <li> <p>run the application first     <pre><code>streamlit run src/mangetamain/streamlit_ui.py\n</code></pre></p> </li> <li> <p>find the pid of the running process</p> <pre><code>ps aux | grep \"[s]treamlit run src/mangetamain/streamlit_ui.py\" | awk '{print $2}'\n</code></pre> </li> <li> <p>if want to run realtime profiling</p> <pre><code>py-spy top --pid &lt;PID&gt;\n</code></pre> </li> <li> <p>if want to save and visualize instead</p> <pre><code>py-spy record -o profile.svg --pid &lt;PID&gt;\n</code></pre> </li> <li> <p>add sudo env \"PATH=$PATH\" if has errors:</p> <pre><code>sudo env \"PATH=$PATH\" py-spy top --pid &lt;PID&gt;\nsudo env \"PATH=$PATH\" py-spy record -o docs/streamlit_profile.svg --pid &lt;PID&gt;\n</code></pre> </li> <li> <p>test the application before quitting</p> </li> <li> <p>the result of visualization will be saved to streamlit_profile.svg</p> </li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>mangetamain<ul> <li>backend<ul> <li>data_processor</li> <li>recipe_analyzer</li> </ul> </li> <li>streamlit_ui</li> <li>utils<ul> <li>helper</li> <li>logger</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mangetamain/__init__/","title":"mangetamain","text":"<p>Mangetamain package.</p> <p>This package provides the backend data processing, frontend Streamlit UI and utility helpers for the Mangetamain project. The package is small and organized into <code>backend</code>, <code>frontend</code> and <code>utils</code> subpackages.</p> <p>Modules should be imported from the <code>mangetamain</code> package (for example: <code>from mangetamain.backend.data_processor import DataProcessor</code>).</p>"},{"location":"reference/mangetamain/streamlit_ui/","title":"streamlit_ui","text":"<p>Main entry point for the Streamlit UI application and doing the dispatch between the pages.</p>"},{"location":"reference/mangetamain/streamlit_ui/#mangetamain.streamlit_ui.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Entry point for the Streamlit front-end application.</p> <p>This function prepares the navigation pages and ensures the application data is loaded into the Streamlit session state before starting the page navigation.</p> <p>The data loading is optimized with @st.cache_resource: - First user: ~90s to load all data - All subsequent users: &lt;0.01s (instant cache hit)</p> Source code in <code>src/mangetamain/streamlit_ui.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Entry point for the Streamlit front-end application.\n\n    This function prepares the navigation pages and ensures the\n    application data is loaded into the Streamlit session state before\n    starting the page navigation.\n\n    The data loading is optimized with @st.cache_resource:\n    - First user: ~90s to load all data\n    - All subsequent users: &lt;0.01s (instant cache hit)\n    \"\"\"\n    # Always call the cached function (it returns instantly after first call)\n    cache_start = time.time()\n    (\n        df_interactions,\n        df_interactions_nna,\n        df_recipes,\n        df_recipes_nna,\n        df_total_nt,\n        df_total,\n        df_total_court,\n        proportion_m,\n        proportion_s,\n        recipe_analyzer,\n        data_loaded,\n    ) = load_data_from_parquet_and_pickle()\n    cache_time = time.time() - cache_start\n\n    logger.info(f\"\u26a1 Cache access took {cache_time:.4f}s\")\n\n    # Store in session_state ONCE per user for convenience\n    if \"data_loaded\" not in st.session_state:\n        with st.spinner(\"\ud83d\udd04 Loading application data...\"):\n            logger.info(\"Storing data in session_state for this user.\")\n            st.session_state.df_interactions = df_interactions\n            st.session_state.df_interactions_nna = df_interactions_nna\n            st.session_state.df_recipes = df_recipes\n            st.session_state.df_recipes_nna = df_recipes_nna\n            st.session_state.df_total_nt = df_total_nt\n            st.session_state.df_total = df_total\n            st.session_state.df_total_court = df_total_court\n            st.session_state.proportion_m = proportion_m\n            st.session_state.proportion_s = proportion_s\n            st.session_state.recipe_analyzer = recipe_analyzer\n            st.session_state.data_loaded = data_loaded\n            logger.info(\"\u2705 Data available in session_state for this user.\")\n\n    home_page = st.Page(\"frontend/pages/dashboard.py\", title=\"Home\", default=True)\n    rating_page = st.Page(\"frontend/pages/rating.py\", title=\"Rating\")\n    # recipe_time_page = st.Page(\"frontend/pages/recipes_analysis.py\", title=\"Recipe Time\")\n    overview_page = st.Page(\"frontend/pages/overview.py\", title=\"Overview\")\n    recipes_analysis_page = st.Page(\n        \"frontend/pages/recipes_analysis.py\",\n        title=\"Recipes Analysis\",\n    )\n    users_analysis_page = st.Page(\n        \"frontend/pages/users_analysis.py\",\n        title=\"Users Analysis\",\n    )\n    trends_page = st.Page(\"frontend/pages/trends.py\", title=\"Trends\")\n\n    pg = st.navigation(\n        [\n            home_page,\n            overview_page,\n            rating_page,\n            trends_page,\n            users_analysis_page,\n            recipes_analysis_page,\n        ],\n    )\n\n    pg.run()\n</code></pre>"},{"location":"reference/mangetamain/backend/__init__/","title":"backend","text":"<p>Backend module for the Mangetamain project.</p> <p>This module contains the data processing logic for the Mangetamain application.</p>"},{"location":"reference/mangetamain/backend/data_processor/","title":"data_processor","text":"<p>Data processing module for the Mangetamain project.</p> <p>This module contains the :class:<code>DataProcessor</code> class which is responsible for loading raw CSV/ZIP datasets, cleaning and splitting recipe data, merging interactions with recipe metadata and computing simple aggregations used by the frontend (proportions, aggregates and parquet exports).</p> <p>The class uses Polars for efficient DataFrame operations and writes the processed results into <code>data/processed/</code> as parquet files.</p>"},{"location":"reference/mangetamain/backend/data_processor/#mangetamain.backend.data_processor.DataProcessor","title":"DataProcessor","text":"<p>Processes and transforms recipe and interaction data.</p> <p>The :class:<code>DataProcessor</code> is a lightweight ETL helper used to build the processed datasets consumed by the Streamlit frontend. It exposes a small sequence of methods that can be called by a runner or CI job:</p> <ul> <li><code>load_data</code>: Load CSV or extract ZIP and read inputs with Polars.</li> <li><code>drop_na</code>: Remove rows with missing or unrealistic values.</li> <li><code>split_minutes</code>: Partition recipes into short/medium/long buckets.</li> <li><code>merge_data</code>: Join interactions with recipe metadata.</li> <li><code>compute_proportions</code>: Compute simple aggregates used by plots.</li> <li><code>save_data</code>: Persist the processed tables to parquet files.</li> </ul> Source code in <code>src/mangetamain/backend/data_processor.py</code> <pre><code>class DataProcessor:\n    \"\"\"Processes and transforms recipe and interaction data.\n\n    The :class:`DataProcessor` is a lightweight ETL helper used to build\n    the processed datasets consumed by the Streamlit frontend. It exposes\n    a small sequence of methods that can be called by a runner or CI job:\n\n    - ``load_data``: Load CSV or extract ZIP and read inputs with Polars.\n    - ``drop_na``: Remove rows with missing or unrealistic values.\n    - ``split_minutes``: Partition recipes into short/medium/long buckets.\n    - ``merge_data``: Join interactions with recipe metadata.\n    - ``compute_proportions``: Compute simple aggregates used by plots.\n    - ``save_data``: Persist the processed tables to parquet files.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_dir: Path = Path(\"data/raw\"),\n        path_interactions: Path = Path(\"data/raw/RAW_interactions.csv\"),\n        path_recipes: Path = Path(\"data/raw/RAW_recipes.csv\"),\n    ) -&gt; None:\n        \"\"\"Initialize the DataProcessor.\n\n        Args:\n            data_dir: Base directory where raw data files live (default ``data/raw``).\n            path_interactions: Path to the interactions CSV file.\n            path_recipes: Path to the recipes CSV file.\n        \"\"\"\n        logger.info(\"Starting to load data.\")\n        self.data_dir = data_dir\n        self.path_interactions = path_interactions\n        self.path_recipes = path_recipes\n        self.df_interactions, self.df_recipes = self.load_data()\n\n    def load_data(self) -&gt; tuple[pl.DataFrame, pl.DataFrame]:\n        \"\"\"Load interactions and recipes data.\n\n        The method accepts either CSV files or ZIP archives containing the\n        CSVs. If CSV files are not present it will look for ``.csv.zip``\n        siblings and extract them into ``data_dir``.\n\n        Returns:\n                A tuple ``(df_interactions, df_recipes)`` of Polars DataFrames.\n        \"\"\"\n        # Check if CSV files exist, otherwise look for ZIP files\n        if not self.path_interactions.exists() or not self.path_recipes.exists():\n            logger.info(\"CSV files not found, checking for ZIP files.\")\n            path_interaction_zip = self.path_interactions.with_suffix(\".csv.zip\")\n            path_recipes_zip = self.path_recipes.with_suffix(\".csv.zip\")\n\n            if not path_interaction_zip.exists() or not path_recipes_zip.exists():\n                logger.error(\n                    f\"CSV and ZIP files not found: {path_interaction_zip} and {path_recipes_zip} not found.\",\n                )\n                raise FileNotFoundError(\n                    \"Neither CSV nor ZIP files found for interactions or recipes.\",\n                )\n            else:\n                logger.info(\"Extracting data from ZIP files.\")\n                with zipfile.ZipFile(path_interaction_zip, \"r\") as zip_ref:\n                    zip_ref.extractall(self.data_dir)\n                with zipfile.ZipFile(path_recipes_zip, \"r\") as zip_ref:\n                    zip_ref.extractall(self.data_dir)\n\n        # Load data from CSV\n\n        try:\n            with open(self.path_interactions, \"rb\") as f:\n                df_interactions = pl.read_csv(f, schema_overrides={\"date\": pl.Datetime})\n                logger.info(\n                    f\"Interactions loaded successfully | Data shape: {df_interactions.shape}.\",\n                )\n            with open(self.path_recipes, \"rb\") as f:\n                df_recipes = pl.read_csv(f, schema_overrides={\"submitted\": pl.Datetime})\n                df_recipes = df_recipes.rename({\"id\": \"recipe_id\"})\n                logger.info(\n                    f\"Recipes loaded successfully | Data shape: {df_recipes.shape}.\",\n                )\n        except Exception as e:\n            logger.error(f\"Error loading CSV files: {e}\")\n            raise\n\n        return df_interactions, df_recipes\n\n    def drop_na(self) -&gt; None:\n        \"\"\"Drop missing or unrealistic records.\n\n        This method filters out interactions without textual reviews and\n        recipes with unrealistic preparation times or zero steps. It\n        updates the instance attributes used by downstream processing.\n        \"\"\"\n        self.df_interactions_nna = self.df_interactions.filter(\n            ~self.df_interactions[\"review\"].is_null(),\n        )\n        logger.info(\n            f\"Interactions after dropping NA | Data shape: {self.df_interactions.shape}.\",\n        )\n        self.df_recipes_nna = self.df_recipes.filter(\n            (self.df_recipes[\"minutes\"] &lt; 60 * 24 * 365)\n            &amp; (self.df_recipes[\"minutes\"] &gt; 0),\n        )\n        logger.info(\n            f\"Recipes after dropping unrealistic times | Data shape: {self.df_recipes.shape}.\",\n        )\n        self.df_recipes_nna = self.df_recipes_nna.filter(\n            self.df_recipes_nna[\"n_steps\"] &gt; 0,\n        )\n        logger.info(\n            f\"Recipes after dropping zero steps | Data shape: {self.df_recipes.shape}.\",\n        )\n\n    def split_minutes(self) -&gt; None:\n        \"\"\"Split recipes into short, medium, and long buckets based on minutes.\n\n        The thresholds are conservative and chosen to separate quick\n        recipes from long projects. Results are stored on the instance as\n        ``df_recipes_nna_short``, ``df_recipes_nna_medium`` and\n        ``df_recipes_nna_long``.\n        \"\"\"\n        self.df_recipes_nna_short = self.df_recipes_nna.filter(\n            self.df_recipes_nna[\"minutes\"] &lt;= MEDIUM_LIM,\n        )\n        self.df_recipes_nna_medium = self.df_recipes_nna.filter(\n            (self.df_recipes_nna[\"minutes\"] &gt; MEDIUM_LIM)\n            &amp; (self.df_recipes_nna[\"minutes\"] &lt;= LONG_LIM),\n        )\n        self.df_recipes_nna_long = self.df_recipes_nna.filter(\n            self.df_recipes_nna[\"minutes\"] &gt; LONG_LIM,\n        )\n        logger.info(\n            f\"Recipes split into short ({self.df_recipes_nna_short.shape}), \"\n            f\"medium ({self.df_recipes_nna_medium.shape}), \"\n            f\"and long ({self.df_recipes_nna_long.shape}).\",\n        )\n\n    def merge_data(self) -&gt; None:\n        \"\"\"Join interactions with recipes on ``recipe_id``.\n\n        Produces ``total`` tables for each duration bucket that are used to\n        compute rating proportions and other aggregates.\n        \"\"\"\n        self.total_nt = self.df_interactions.join(\n            self.df_recipes,\n            on=\"recipe_id\",\n            how=\"inner\",\n        )\n        self.total = self.df_interactions_nna.join(\n            self.df_recipes_nna,\n            on=\"recipe_id\",\n            how=\"inner\",\n        )\n        self.total_short = self.df_interactions_nna.join(\n            self.df_recipes_nna_short,\n            on=\"recipe_id\",\n            how=\"inner\",\n        )\n        self.total_medium = self.df_interactions_nna.join(\n            self.df_recipes_nna_medium,\n            on=\"recipe_id\",\n            how=\"inner\",\n        )\n        self.total_long = self.df_interactions_nna.join(\n            self.df_recipes_nna_long,\n            on=\"recipe_id\",\n            how=\"inner\",\n        )\n        logger.info(f\"Merged data shape: {self.total.shape}.\")\n        logger.info(f\"Merged short recipes data shape: {self.total_short.shape}.\")\n        logger.info(f\"Merged medium recipes data shape: {self.total_medium.shape}.\")\n        logger.info(f\"Merged long recipes data shape: {self.total_long.shape}.\")\n\n    def compute_proportions(self) -&gt; None:\n        \"\"\"Compute 5-star proportions by preparation time and number of steps.\n\n        This method fills ``df_proportion_m`` and ``df_proportion_s`` which\n        contain the proportion of 5-star ratings aggregated by minute and\n        number-of-steps respectively. Results are suitable for plotting in\n        the frontend.\n        \"\"\"\n        # minutes = np.array(sorted(self.df_recipes_nna_court[\"minutes\"].unique()))\n        logger.info(\"Computing proportions of 5-star ratings by minutes\")\n        minutes = np.array(sorted(self.df_recipes_nna_short[\"minutes\"].unique()))\n        comptes = (\n            self.total_short[\"minutes\"]\n            .value_counts()\n            .sort(\"minutes\")[\"count\"]\n            .to_numpy()\n        )\n        proportions = (\n            self.total_short.filter(pl.col(\"rating\") == RATING_MAX)[\"minutes\"]\n            .value_counts()\n            .sort(\"minutes\")[\"count\"]\n            .to_numpy()\n        )\n        proportion_m = proportions / comptes\n\n        # steps = np.array(sorted(self.df_recipes_nna[self.df_recipes_nna[\"n_steps\"] &lt;= 40].n_steps.unique()))\n        logger.info(\"Computing proportions of 5-star ratings by steps\")\n        steps = np.array(\n            sorted(\n                self.df_recipes_nna.filter(pl.col(\"n_steps\") &lt;= NB_STEPS_MAX)[\n                    \"n_steps\"\n                ].unique(),\n            ),\n        )\n        comptes = (\n            self.total.filter(pl.col(\"n_steps\") &lt;= NB_STEPS_MAX)[\"n_steps\"]\n            .value_counts()\n            .sort(\"n_steps\")[\"count\"]\n            .to_numpy()\n        )\n        proportions = (\n            self.total.filter(\n                (pl.col(\"n_steps\") &lt;= NB_STEPS_MAX) &amp; (pl.col(\"rating\") == RATING_MAX),\n            )[\"n_steps\"]\n            .value_counts()\n            .sort(\"n_steps\")[\"count\"]\n            .to_numpy()\n        )\n        proportion_s = proportions / comptes\n\n        logger.info(\"Proportions computed. Loading internally\")\n        self.df_proportion_m = pl.DataFrame(\n            {\n                \"minutes\": minutes.astype(int),\n                \"proportion_m\": proportion_m.astype(float),\n            },\n        )  # type conversion needed for parquet\n        self.df_proportion_s = pl.DataFrame(\n            {\"n_steps\": steps.astype(int), \"proportion_s\": proportion_s.astype(float)},\n        )\n\n    def process_recipes(self) -&gt; None:\n        \"\"\"Create a RecipeAnalyzer instance for NLP and visualization.\n\n        Initializes a :class:`RecipeAnalyzer` with the loaded data and stores\n        it as ``self.recipe_analyzer``. This object provides word cloud generation,\n        TF-IDF analysis, and other recipe text analysis features.\n        \"\"\"\n        self.recipe_analyzer = RecipeAnalyzer(\n            self.df_interactions,\n            self.df_recipes,\n            self.total,\n        )\n\n    def save_data(self) -&gt; None:\n        \"\"\"Persist processed tables to parquet files under ``data/processed/``.\n\n        The output files are:\n        - ``processed_interactions.parquet``\n        - ``processed_recipes.parquet``\n        - ``total.parquet`` (merged interactions)\n        - ``short.parquet`` (merged short recipes)\n        - ``proportion_m.parquet`` and ``proportion_s.parquet``\n        \"\"\"\n        logger.info(\"Starting to save the data in parquet\")\n        save_folder = Path(\"data/processed\")\n        save_folder.mkdir(parents=True, exist_ok=True)\n        logger.info(\"Saving df_interactions\")\n        self.df_interactions.write_parquet(\n            \"data/processed/initial_interactions.parquet\",\n        )\n        self.df_interactions_nna.write_parquet(\n            \"data/processed/processed_interactions.parquet\",\n        )\n        logger.info(\"Done \\n Saving df_recipes\")\n        self.df_recipes.write_parquet(\"data/processed/initial_recipes.parquet\")\n        self.df_recipes_nna.write_parquet(\"data/processed/processed_recipes.parquet\")\n\n        logger.info(\"Done \\n Saving total data\")\n        self.total_nt.write_parquet(\"data/processed/total_nt.parquet\")\n        self.total.write_parquet(\"data/processed/total.parquet\")\n\n        logger.info(\"Done \\n Saving total short data\")\n        self.total_short.write_parquet(\"data/processed/short.parquet\")\n        # self.df_recipes_nna_medium.write_parquet(\"data/processed/medium.parquet\")\n        # self.df_recipes_nna_long.write_parquet(\"data/processed/long.parquet\")\n\n        logger.info(\"Done \\n Saving proportions data\")\n        self.df_proportion_m.write_parquet(\"data/processed/proportion_m.parquet\")\n        self.df_proportion_s.write_parquet(\"data/processed/proportion_s.parquet\")\n\n        logger.info(\"Done \\n Saving recipe analyzer object\")\n\n        self.recipe_analyzer.save(\"data/processed/recipe_analyzer.pkl\")\n\n        logger.info(\"All processed data saved to parquet files.\")\n</code></pre>"},{"location":"reference/mangetamain/backend/data_processor/#mangetamain.backend.data_processor.DataProcessor.__init__","title":"__init__","text":"<pre><code>__init__(\n    data_dir=Path(\"data/raw\"),\n    path_interactions=Path(\"data/raw/RAW_interactions.csv\"),\n    path_recipes=Path(\"data/raw/RAW_recipes.csv\"),\n)\n</code></pre> <p>Initialize the DataProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>Base directory where raw data files live (default <code>data/raw</code>).</p> <code>Path('data/raw')</code> <code>path_interactions</code> <code>Path</code> <p>Path to the interactions CSV file.</p> <code>Path('data/raw/RAW_interactions.csv')</code> <code>path_recipes</code> <code>Path</code> <p>Path to the recipes CSV file.</p> <code>Path('data/raw/RAW_recipes.csv')</code> Source code in <code>src/mangetamain/backend/data_processor.py</code> <pre><code>def __init__(\n    self,\n    data_dir: Path = Path(\"data/raw\"),\n    path_interactions: Path = Path(\"data/raw/RAW_interactions.csv\"),\n    path_recipes: Path = Path(\"data/raw/RAW_recipes.csv\"),\n) -&gt; None:\n    \"\"\"Initialize the DataProcessor.\n\n    Args:\n        data_dir: Base directory where raw data files live (default ``data/raw``).\n        path_interactions: Path to the interactions CSV file.\n        path_recipes: Path to the recipes CSV file.\n    \"\"\"\n    logger.info(\"Starting to load data.\")\n    self.data_dir = data_dir\n    self.path_interactions = path_interactions\n    self.path_recipes = path_recipes\n    self.df_interactions, self.df_recipes = self.load_data()\n</code></pre>"},{"location":"reference/mangetamain/backend/data_processor/#mangetamain.backend.data_processor.DataProcessor.compute_proportions","title":"compute_proportions","text":"<pre><code>compute_proportions()\n</code></pre> <p>Compute 5-star proportions by preparation time and number of steps.</p> <p>This method fills <code>df_proportion_m</code> and <code>df_proportion_s</code> which contain the proportion of 5-star ratings aggregated by minute and number-of-steps respectively. Results are suitable for plotting in the frontend.</p> Source code in <code>src/mangetamain/backend/data_processor.py</code> <pre><code>def compute_proportions(self) -&gt; None:\n    \"\"\"Compute 5-star proportions by preparation time and number of steps.\n\n    This method fills ``df_proportion_m`` and ``df_proportion_s`` which\n    contain the proportion of 5-star ratings aggregated by minute and\n    number-of-steps respectively. Results are suitable for plotting in\n    the frontend.\n    \"\"\"\n    # minutes = np.array(sorted(self.df_recipes_nna_court[\"minutes\"].unique()))\n    logger.info(\"Computing proportions of 5-star ratings by minutes\")\n    minutes = np.array(sorted(self.df_recipes_nna_short[\"minutes\"].unique()))\n    comptes = (\n        self.total_short[\"minutes\"]\n        .value_counts()\n        .sort(\"minutes\")[\"count\"]\n        .to_numpy()\n    )\n    proportions = (\n        self.total_short.filter(pl.col(\"rating\") == RATING_MAX)[\"minutes\"]\n        .value_counts()\n        .sort(\"minutes\")[\"count\"]\n        .to_numpy()\n    )\n    proportion_m = proportions / comptes\n\n    # steps = np.array(sorted(self.df_recipes_nna[self.df_recipes_nna[\"n_steps\"] &lt;= 40].n_steps.unique()))\n    logger.info(\"Computing proportions of 5-star ratings by steps\")\n    steps = np.array(\n        sorted(\n            self.df_recipes_nna.filter(pl.col(\"n_steps\") &lt;= NB_STEPS_MAX)[\n                \"n_steps\"\n            ].unique(),\n        ),\n    )\n    comptes = (\n        self.total.filter(pl.col(\"n_steps\") &lt;= NB_STEPS_MAX)[\"n_steps\"]\n        .value_counts()\n        .sort(\"n_steps\")[\"count\"]\n        .to_numpy()\n    )\n    proportions = (\n        self.total.filter(\n            (pl.col(\"n_steps\") &lt;= NB_STEPS_MAX) &amp; (pl.col(\"rating\") == RATING_MAX),\n        )[\"n_steps\"]\n        .value_counts()\n        .sort(\"n_steps\")[\"count\"]\n        .to_numpy()\n    )\n    proportion_s = proportions / comptes\n\n    logger.info(\"Proportions computed. Loading internally\")\n    self.df_proportion_m = pl.DataFrame(\n        {\n            \"minutes\": minutes.astype(int),\n            \"proportion_m\": proportion_m.astype(float),\n        },\n    )  # type conversion needed for parquet\n    self.df_proportion_s = pl.DataFrame(\n        {\"n_steps\": steps.astype(int), \"proportion_s\": proportion_s.astype(float)},\n    )\n</code></pre>"},{"location":"reference/mangetamain/backend/data_processor/#mangetamain.backend.data_processor.DataProcessor.drop_na","title":"drop_na","text":"<pre><code>drop_na()\n</code></pre> <p>Drop missing or unrealistic records.</p> <p>This method filters out interactions without textual reviews and recipes with unrealistic preparation times or zero steps. It updates the instance attributes used by downstream processing.</p> Source code in <code>src/mangetamain/backend/data_processor.py</code> <pre><code>def drop_na(self) -&gt; None:\n    \"\"\"Drop missing or unrealistic records.\n\n    This method filters out interactions without textual reviews and\n    recipes with unrealistic preparation times or zero steps. It\n    updates the instance attributes used by downstream processing.\n    \"\"\"\n    self.df_interactions_nna = self.df_interactions.filter(\n        ~self.df_interactions[\"review\"].is_null(),\n    )\n    logger.info(\n        f\"Interactions after dropping NA | Data shape: {self.df_interactions.shape}.\",\n    )\n    self.df_recipes_nna = self.df_recipes.filter(\n        (self.df_recipes[\"minutes\"] &lt; 60 * 24 * 365)\n        &amp; (self.df_recipes[\"minutes\"] &gt; 0),\n    )\n    logger.info(\n        f\"Recipes after dropping unrealistic times | Data shape: {self.df_recipes.shape}.\",\n    )\n    self.df_recipes_nna = self.df_recipes_nna.filter(\n        self.df_recipes_nna[\"n_steps\"] &gt; 0,\n    )\n    logger.info(\n        f\"Recipes after dropping zero steps | Data shape: {self.df_recipes.shape}.\",\n    )\n</code></pre>"},{"location":"reference/mangetamain/backend/data_processor/#mangetamain.backend.data_processor.DataProcessor.load_data","title":"load_data","text":"<pre><code>load_data()\n</code></pre> <p>Load interactions and recipes data.</p> <p>The method accepts either CSV files or ZIP archives containing the CSVs. If CSV files are not present it will look for <code>.csv.zip</code> siblings and extract them into <code>data_dir</code>.</p> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>A tuple <code>(df_interactions, df_recipes)</code> of Polars DataFrames.</p> Source code in <code>src/mangetamain/backend/data_processor.py</code> <pre><code>def load_data(self) -&gt; tuple[pl.DataFrame, pl.DataFrame]:\n    \"\"\"Load interactions and recipes data.\n\n    The method accepts either CSV files or ZIP archives containing the\n    CSVs. If CSV files are not present it will look for ``.csv.zip``\n    siblings and extract them into ``data_dir``.\n\n    Returns:\n            A tuple ``(df_interactions, df_recipes)`` of Polars DataFrames.\n    \"\"\"\n    # Check if CSV files exist, otherwise look for ZIP files\n    if not self.path_interactions.exists() or not self.path_recipes.exists():\n        logger.info(\"CSV files not found, checking for ZIP files.\")\n        path_interaction_zip = self.path_interactions.with_suffix(\".csv.zip\")\n        path_recipes_zip = self.path_recipes.with_suffix(\".csv.zip\")\n\n        if not path_interaction_zip.exists() or not path_recipes_zip.exists():\n            logger.error(\n                f\"CSV and ZIP files not found: {path_interaction_zip} and {path_recipes_zip} not found.\",\n            )\n            raise FileNotFoundError(\n                \"Neither CSV nor ZIP files found for interactions or recipes.\",\n            )\n        else:\n            logger.info(\"Extracting data from ZIP files.\")\n            with zipfile.ZipFile(path_interaction_zip, \"r\") as zip_ref:\n                zip_ref.extractall(self.data_dir)\n            with zipfile.ZipFile(path_recipes_zip, \"r\") as zip_ref:\n                zip_ref.extractall(self.data_dir)\n\n    # Load data from CSV\n\n    try:\n        with open(self.path_interactions, \"rb\") as f:\n            df_interactions = pl.read_csv(f, schema_overrides={\"date\": pl.Datetime})\n            logger.info(\n                f\"Interactions loaded successfully | Data shape: {df_interactions.shape}.\",\n            )\n        with open(self.path_recipes, \"rb\") as f:\n            df_recipes = pl.read_csv(f, schema_overrides={\"submitted\": pl.Datetime})\n            df_recipes = df_recipes.rename({\"id\": \"recipe_id\"})\n            logger.info(\n                f\"Recipes loaded successfully | Data shape: {df_recipes.shape}.\",\n            )\n    except Exception as e:\n        logger.error(f\"Error loading CSV files: {e}\")\n        raise\n\n    return df_interactions, df_recipes\n</code></pre>"},{"location":"reference/mangetamain/backend/data_processor/#mangetamain.backend.data_processor.DataProcessor.merge_data","title":"merge_data","text":"<pre><code>merge_data()\n</code></pre> <p>Join interactions with recipes on <code>recipe_id</code>.</p> <p>Produces <code>total</code> tables for each duration bucket that are used to compute rating proportions and other aggregates.</p> Source code in <code>src/mangetamain/backend/data_processor.py</code> <pre><code>def merge_data(self) -&gt; None:\n    \"\"\"Join interactions with recipes on ``recipe_id``.\n\n    Produces ``total`` tables for each duration bucket that are used to\n    compute rating proportions and other aggregates.\n    \"\"\"\n    self.total_nt = self.df_interactions.join(\n        self.df_recipes,\n        on=\"recipe_id\",\n        how=\"inner\",\n    )\n    self.total = self.df_interactions_nna.join(\n        self.df_recipes_nna,\n        on=\"recipe_id\",\n        how=\"inner\",\n    )\n    self.total_short = self.df_interactions_nna.join(\n        self.df_recipes_nna_short,\n        on=\"recipe_id\",\n        how=\"inner\",\n    )\n    self.total_medium = self.df_interactions_nna.join(\n        self.df_recipes_nna_medium,\n        on=\"recipe_id\",\n        how=\"inner\",\n    )\n    self.total_long = self.df_interactions_nna.join(\n        self.df_recipes_nna_long,\n        on=\"recipe_id\",\n        how=\"inner\",\n    )\n    logger.info(f\"Merged data shape: {self.total.shape}.\")\n    logger.info(f\"Merged short recipes data shape: {self.total_short.shape}.\")\n    logger.info(f\"Merged medium recipes data shape: {self.total_medium.shape}.\")\n    logger.info(f\"Merged long recipes data shape: {self.total_long.shape}.\")\n</code></pre>"},{"location":"reference/mangetamain/backend/data_processor/#mangetamain.backend.data_processor.DataProcessor.process_recipes","title":"process_recipes","text":"<pre><code>process_recipes()\n</code></pre> <p>Create a RecipeAnalyzer instance for NLP and visualization.</p> <p>Initializes a :class:<code>RecipeAnalyzer</code> with the loaded data and stores it as <code>self.recipe_analyzer</code>. This object provides word cloud generation, TF-IDF analysis, and other recipe text analysis features.</p> Source code in <code>src/mangetamain/backend/data_processor.py</code> <pre><code>def process_recipes(self) -&gt; None:\n    \"\"\"Create a RecipeAnalyzer instance for NLP and visualization.\n\n    Initializes a :class:`RecipeAnalyzer` with the loaded data and stores\n    it as ``self.recipe_analyzer``. This object provides word cloud generation,\n    TF-IDF analysis, and other recipe text analysis features.\n    \"\"\"\n    self.recipe_analyzer = RecipeAnalyzer(\n        self.df_interactions,\n        self.df_recipes,\n        self.total,\n    )\n</code></pre>"},{"location":"reference/mangetamain/backend/data_processor/#mangetamain.backend.data_processor.DataProcessor.save_data","title":"save_data","text":"<pre><code>save_data()\n</code></pre> <p>Persist processed tables to parquet files under <code>data/processed/</code>.</p> <p>The output files are: - <code>processed_interactions.parquet</code> - <code>processed_recipes.parquet</code> - <code>total.parquet</code> (merged interactions) - <code>short.parquet</code> (merged short recipes) - <code>proportion_m.parquet</code> and <code>proportion_s.parquet</code></p> Source code in <code>src/mangetamain/backend/data_processor.py</code> <pre><code>def save_data(self) -&gt; None:\n    \"\"\"Persist processed tables to parquet files under ``data/processed/``.\n\n    The output files are:\n    - ``processed_interactions.parquet``\n    - ``processed_recipes.parquet``\n    - ``total.parquet`` (merged interactions)\n    - ``short.parquet`` (merged short recipes)\n    - ``proportion_m.parquet`` and ``proportion_s.parquet``\n    \"\"\"\n    logger.info(\"Starting to save the data in parquet\")\n    save_folder = Path(\"data/processed\")\n    save_folder.mkdir(parents=True, exist_ok=True)\n    logger.info(\"Saving df_interactions\")\n    self.df_interactions.write_parquet(\n        \"data/processed/initial_interactions.parquet\",\n    )\n    self.df_interactions_nna.write_parquet(\n        \"data/processed/processed_interactions.parquet\",\n    )\n    logger.info(\"Done \\n Saving df_recipes\")\n    self.df_recipes.write_parquet(\"data/processed/initial_recipes.parquet\")\n    self.df_recipes_nna.write_parquet(\"data/processed/processed_recipes.parquet\")\n\n    logger.info(\"Done \\n Saving total data\")\n    self.total_nt.write_parquet(\"data/processed/total_nt.parquet\")\n    self.total.write_parquet(\"data/processed/total.parquet\")\n\n    logger.info(\"Done \\n Saving total short data\")\n    self.total_short.write_parquet(\"data/processed/short.parquet\")\n    # self.df_recipes_nna_medium.write_parquet(\"data/processed/medium.parquet\")\n    # self.df_recipes_nna_long.write_parquet(\"data/processed/long.parquet\")\n\n    logger.info(\"Done \\n Saving proportions data\")\n    self.df_proportion_m.write_parquet(\"data/processed/proportion_m.parquet\")\n    self.df_proportion_s.write_parquet(\"data/processed/proportion_s.parquet\")\n\n    logger.info(\"Done \\n Saving recipe analyzer object\")\n\n    self.recipe_analyzer.save(\"data/processed/recipe_analyzer.pkl\")\n\n    logger.info(\"All processed data saved to parquet files.\")\n</code></pre>"},{"location":"reference/mangetamain/backend/data_processor/#mangetamain.backend.data_processor.DataProcessor.split_minutes","title":"split_minutes","text":"<pre><code>split_minutes()\n</code></pre> <p>Split recipes into short, medium, and long buckets based on minutes.</p> <p>The thresholds are conservative and chosen to separate quick recipes from long projects. Results are stored on the instance as <code>df_recipes_nna_short</code>, <code>df_recipes_nna_medium</code> and <code>df_recipes_nna_long</code>.</p> Source code in <code>src/mangetamain/backend/data_processor.py</code> <pre><code>def split_minutes(self) -&gt; None:\n    \"\"\"Split recipes into short, medium, and long buckets based on minutes.\n\n    The thresholds are conservative and chosen to separate quick\n    recipes from long projects. Results are stored on the instance as\n    ``df_recipes_nna_short``, ``df_recipes_nna_medium`` and\n    ``df_recipes_nna_long``.\n    \"\"\"\n    self.df_recipes_nna_short = self.df_recipes_nna.filter(\n        self.df_recipes_nna[\"minutes\"] &lt;= MEDIUM_LIM,\n    )\n    self.df_recipes_nna_medium = self.df_recipes_nna.filter(\n        (self.df_recipes_nna[\"minutes\"] &gt; MEDIUM_LIM)\n        &amp; (self.df_recipes_nna[\"minutes\"] &lt;= LONG_LIM),\n    )\n    self.df_recipes_nna_long = self.df_recipes_nna.filter(\n        self.df_recipes_nna[\"minutes\"] &gt; LONG_LIM,\n    )\n    logger.info(\n        f\"Recipes split into short ({self.df_recipes_nna_short.shape}), \"\n        f\"medium ({self.df_recipes_nna_medium.shape}), \"\n        f\"and long ({self.df_recipes_nna_long.shape}).\",\n    )\n</code></pre>"},{"location":"reference/mangetamain/backend/recipe_analyzer/","title":"recipe_analyzer","text":"<p>Recipe analysis module with NLP and visualization capabilities.</p> <p>This module provides the RecipeAnalyzer class for analyzing recipe reviews and ingredients using Natural Language Processing (NLP) techniques. It generates visualizations including word clouds, TF-IDF analysis, Venn diagrams, and polar plots.</p> Key Features <ul> <li>Batch text processing with spaCy for 5-10x performance improvement</li> <li>LRU caching for preprocessed data and generated figures</li> <li>Frequency-based and TF-IDF word cloud generation</li> <li>Comparison visualizations between word extraction methods</li> <li>Polar plots for ingredient frequency analysis</li> <li>Integration with Streamlit for interactive UI components</li> </ul> Dependencies <ul> <li>spacy: NLP processing (requires en_core_web_sm model)</li> <li>polars: High-performance DataFrame operations</li> <li>matplotlib: Plotting and figure generation</li> <li>scikit-learn: TF-IDF vectorization</li> <li>wordcloud: Word cloud generation</li> <li>matplotlib-venn: Venn diagram visualization</li> <li>streamlit: Web UI framework</li> </ul> Note <p>All visualization methods return cached matplotlib.figure.Figure objects to ensure instant rendering on subsequent calls.</p>"},{"location":"reference/mangetamain/backend/recipe_analyzer/#mangetamain.backend.recipe_analyzer.RecipeAnalyzer","title":"RecipeAnalyzer","text":"<p>Analyzer for recipe data with NLP and visualization capabilities.</p> <p>This class provides methods to analyze recipe reviews, extract ingredients, generate word clouds, and create visualizations comparing different text analysis techniques (frequency vs TF-IDF).</p> <p>Attributes:</p> Name Type Description <code>df_recipe</code> <p>DataFrame containing recipe information</p> <code>df_interaction</code> <p>DataFrame containing user interactions and reviews</p> <code>df_total</code> <p>Combined DataFrame with all data</p> <code>nlp</code> <p>spaCy language model for text processing</p> <code>stop_words</code> <p>Set of stop words to filter out</p> <code>top_ingredients</code> <p>Pre-computed DataFrame of most common ingredients</p> <code>_cache</code> <code>dict[str, Any]</code> <p>Dictionary storing preprocessed data and generated figures</p> Source code in <code>src/mangetamain/backend/recipe_analyzer.py</code> <pre><code>class RecipeAnalyzer:\n    \"\"\"Analyzer for recipe data with NLP and visualization capabilities.\n\n    This class provides methods to analyze recipe reviews, extract ingredients,\n    generate word clouds, and create visualizations comparing different text\n    analysis techniques (frequency vs TF-IDF).\n\n    Attributes:\n        df_recipe: DataFrame containing recipe information\n        df_interaction: DataFrame containing user interactions and reviews\n        df_total: Combined DataFrame with all data\n        nlp: spaCy language model for text processing\n        stop_words: Set of stop words to filter out\n        top_ingredients: Pre-computed DataFrame of most common ingredients\n        _cache: Dictionary storing preprocessed data and generated figures\n    \"\"\"\n\n    def __init__(\n        self,\n        df_interactions: pl.DataFrame,\n        df_recipes: pl.DataFrame,\n        df_total: pl.DataFrame,\n    ) -&gt; None:\n        \"\"\"Initialize the RecipeAnalyzer with dataframes.\n\n        Args:\n            df_interactions: DataFrame with user reviews and ratings\n            df_recipes: DataFrame with recipe details\n            df_total: Combined DataFrame with all recipe and interaction data\n        \"\"\"\n        # Store dataframes\n        logger.info(\"Setting up attributes\")\n\n        # Load spaCy model (disable unused components for speed)\n        logger.info(\"Loading spaCy model\")\n        self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n\n        # Initialize stop words\n        logger.info(\"Initializing stop words\")\n        self.stop_words = set(spacy.lang.en.STOP_WORDS)\n        self._extend_stop_words()\n\n        # Pre-compute top ingredients\n        logger.info(\"Computing top ingredients\")\n        self.top_ingredients = self._compute_top_ingredients(df_recipes)\n\n        # Cache for preprocessed data and figures\n        self._cache: dict[str, Any] = {}\n\n        # Pre-process the most common review sets for performance\n        logger.info(\"Preprocessed 500 best reviews\")\n        self._preprocessed_500_best_reviews(df_interactions)\n        logger.info(\"Preprocessed 500 worst reviews\")\n        self._preprocessed_500_worst_reviews(df_interactions)\n        logger.info(\"Preprocessed 500 most reviews\")\n        self._preprocessed_500_most_reviews(df_total)\n        logger.info(\"Preprocessed word cloud\")\n        self._preprocess_word_cloud(100)\n        self._preprocess_comparisons(100, 100)\n\n    def _extend_stop_words(self) -&gt; None:\n        \"\"\"Extend the default stop words with recipe-specific terms.\n\n        Adds common recipe and review words that don't add meaningful\n        information to the analysis (e.g., 'recipe', 'minute', 'good').\n        \"\"\"\n        extra_stop_words = [\n            \"recipe\",\n            \"thank\",\n            \"instead\",\n            \"minute\",\n            \"hour\",\n            \"I\",\n            \"water\",\n            \"bit\",\n            \"definitely\",\n            \"thing\",\n            \"half\",\n            \"way\",\n            \"like\",\n            \"good\",\n            \"great\",\n            \"make\",\n            \"use\",\n            \"get\",\n            \"also\",\n            \"just\",\n            \"would\",\n            \"one\",\n        ]\n\n        self.stop_words.update(extra_stop_words)\n\n    @lru_cache(maxsize=128)  # noqa: B019\n    def _clean_text(self, text: str) -&gt; list[str]:\n        \"\"\"Clean and tokenize a single text string.\n\n        Uses spaCy NLP pipeline to lemmatize, filter stop words, and extract\n        meaningful tokens. Results are cached for performance.\n\n        Args:\n            text: Raw text string to clean\n\n        Returns:\n            List of cleaned and lemmatized tokens\n\n        Note:\n            - Filters out verbs, stop words, and short tokens (&lt; 3 chars)\n            - Results are cached (LRU cache with max 128 entries)\n            - For batch processing, use _clean_texts_batch() instead\n        \"\"\"\n        MIN_TOKEN_LENGTH = 2\n        # Return empty list for invalid input\n        if not isinstance(text, str) or not text.strip():\n            return []\n\n        # Process text with spaCy\n        doc = self.nlp(text.lower())\n\n        # Extract and filter tokens\n        return [\n            token.lemma_\n            for token in doc\n            if (\n                token.is_alpha  # Only alphabetic tokens\n                and token.lemma_ not in self.stop_words  # Not a stop word\n                and len(token.text) &gt; MIN_TOKEN_LENGTH  # At least 3 characters\n                and token.pos_ != \"VERB\"  # Exclude verbs\n            )\n        ]\n\n    def _clean_texts_batch(self, texts: list[str]) -&gt; list[str]:\n        \"\"\"Clean multiple texts in batch (5-10x faster than one-by-one).\n\n        Uses spaCy's pipe() method to process multiple texts efficiently in batches.\n        This is significantly faster than calling _clean_text() in a loop.\n\n        Args:\n            texts: List of text strings to clean\n\n        Returns:\n            Flat list of all cleaned tokens from all texts\n\n        Note:\n            Batch size is set to 50 for optimal performance.\n            Use this instead of looping over _clean_text() for large datasets.\n        \"\"\"\n        MIN_TOKEN_LENGTH = 2\n        # Filter out empty/invalid texts\n        valid_texts = [t.lower() for t in texts if isinstance(t, str) and t.strip()]\n\n        if not valid_texts:\n            return []\n\n        # Process all texts in batch using spaCy's pipe (MUCH faster than loop)\n        all_tokens = []\n        for doc in self.nlp.pipe(valid_texts, batch_size=50):\n            # Extract tokens using same filtering criteria as _clean_text\n            tokens = [\n                token.lemma_\n                for token in doc\n                if (\n                    token.is_alpha\n                    and token.lemma_ not in self.stop_words\n                    and len(token.text) &gt; MIN_TOKEN_LENGTH\n                    and token.pos_ != \"VERB\"\n                )\n            ]\n            all_tokens.extend(tokens)\n\n        return all_tokens\n\n    def _compute_top_ingredients(self, df_recipe: pl.DataFrame) -&gt; pl.DataFrame:\n        \"\"\"Compute the most frequently used ingredients across all recipes.\n\n        Cleans ingredient strings, filters out common non-informative ingredients\n        (salt, water, oil, etc.) and measurement units, then counts occurrences.\n\n        Returns:\n            DataFrame with columns ['ingredients', 'count'] sorted by frequency\n\n        Note:\n            Excluded ingredients are basic items that appear in almost every recipe\n            and don't provide meaningful differentiation.\n        \"\"\"\n        # Define ingredients to exclude (too common or non-specific)\n        excluded = {\n            \"salt\",\n            \"water\",\n            \"oil\",\n            \"sugar\",\n            \"pepper\",\n            \"butter\",\n            \"flour\",\n            \"olive oil\",\n            \"vegetable oil\",\n            \"all-purpose flour\",\n            \"cup\",\n            \"tablespoon\",\n            \"salt and pepper\",\n            \"teaspoon\",\n            \"pound\",\n            \"ounce\",\n            \"gram\",\n            \"kilogram\",\n            \"milliliter\",\n            \"liter\",\n            \"black pepper\",\n        }\n        MIN_LEN = 2\n\n        # Initialize inflect engine for singularization\n\n        # Clean ingredient strings and split into individual items\n        ingredients_cleaned = (\n            df_recipe.with_columns(\n                # Remove brackets and quotes from ingredient list strings\n                pl.col(\"ingredients\").str.replace_all(r\"[\\[\\]']\", \"\").alias(\"cleaned\"),\n            )\n            .select(\n                # Split by comma and explode into separate rows\n                pl.col(\"cleaned\").str.split(\", \").explode().alias(\"ingredients\"),\n            )\n            .filter(\n                # Filter out excluded items and empty/short strings\n                ~pl.col(\"ingredients\").is_in(excluded)\n                &amp; (pl.col(\"ingredients\") != \"\")\n                &amp; (pl.col(\"ingredients\").str.len_chars() &gt; MIN_LEN),\n            )\n        )\n\n        # Count occurrences and sort by frequency\n        ingredients_counts = (\n            ingredients_cleaned.group_by(\"ingredients\")\n            .agg(pl.len().alias(\"count\"))\n            .sort(\"count\", descending=True)\n        )\n\n        return ingredients_counts\n\n    def _preprocessed_500_most_reviews(self, df_total: pl.DataFrame) -&gt; None:\n        \"\"\"Pre-process ingredients from the 500 most-reviewed recipes.\n\n        Identifies recipes with the most reviews, extracts their ingredients,\n        and cleans the text for NLP analysis. Results are cached for performance.\n\n        Note:\n            Uses batch processing for efficient NLP computation.\n            Results are stored in self._cache with key 'preprocessed_500_most_reviews'.\n        \"\"\"\n        logger.info(\"Preprocessing 500 most reviewed recipes...\")\n        cache_key = \"preprocessed_500_most_reviews\"\n\n        # Find the 500 recipes with most reviews\n        most_reviewed_ids = (\n            df_total.group_by(\"recipe_id\")\n            .agg(pl.len().alias(\"nb_reviews\"))\n            .sort(\"nb_reviews\", descending=True)\n            .head(500)\n        )\n\n        # Join with ingredients data - use unique() to avoid duplicates\n        most_reviews_with_ing = most_reviewed_ids.join(\n            df_total.select([\"recipe_id\", \"ingredients\"]).unique(\"recipe_id\"),\n            on=\"recipe_id\",\n            how=\"left\",\n        ).drop_nulls(\"ingredients\")\n\n        # Use batch processing instead of loop (MUCH faster)\n        ingredients_list = most_reviews_with_ing[\"ingredients\"].to_list()\n        logger.info(f\"Processing {len(ingredients_list)} ingredients strings...\")\n        cleaned_reviews = self._clean_texts_batch(ingredients_list)\n\n        logger.info(f\"Most reviews cleaned: {cleaned_reviews[:5]}\")\n        self._cache[cache_key] = cleaned_reviews\n\n    def _preprocessed_500_best_reviews(self, df_interaction: pl.DataFrame) -&gt; None:\n        \"\"\"Preprocess review text from the 500 highest-rated recipe reviews.\n\n        Extracts review text from the top 500 reviews sorted by rating score (5.0 being best).\n        Uses batch processing with spaCy for efficient NLP computation.\n        Results are stored in self._cache with key 'preprocessed_500_best_reviews'.\n        \"\"\"\n        logger.info(\"Preprocessing 500 best reviews...\")\n        cache_key = \"preprocessed_500_best_reviews\"\n\n        best_reviews = (\n            df_interaction.sort(\"rating\", descending=True)\n            .head(500)\n            .select(\"review\")\n            .to_series()\n            .to_list()\n        )\n\n        # Use batch processing instead of loop (MUCH faster)\n        cleaned_reviews = self._clean_texts_batch(best_reviews)\n        self._cache[cache_key] = cleaned_reviews\n\n    def _preprocessed_500_worst_reviews(self, df_interaction: pl.DataFrame) -&gt; None:\n        \"\"\"Preprocess review text from the 500 lowest-rated recipe reviews.\n\n        Extracts review text from the bottom 500 reviews sorted by rating score (1.0 being worst).\n        Uses batch processing with spaCy for efficient NLP computation.\n        Results are stored in self._cache with key 'preprocessed_500_worst_reviews'.\n        \"\"\"\n        logger.info(\"Preprocessing 500 worst reviews...\")\n        cache_key = \"preprocessed_500_worst_reviews\"\n        # if cache_key not in self._cache:\n        worst_reviews = (\n            df_interaction.sort(\"rating\", descending=False)\n            .head(500)\n            .select(\"review\")\n            .to_series()\n            .to_list()\n        )\n\n        # Use batch processing instead of loop (MUCH faster)\n        cleaned_reviews = self._clean_texts_batch(worst_reviews)\n        logger.info(f\"Worst reviews cleaned: {cleaned_reviews[:5]}\")\n        self._cache[cache_key] = cleaned_reviews\n\n    def switch_filter(self, rating_filter: str) -&gt; str:\n        \"\"\"Select appropriate preprocessed data cache based on rating filter.\n\n        Args:\n            rating_filter: Filter type - 'best', 'worst', or 'most' reviewed recipes.\n\n        Returns:\n            str: Cache key corresponding to the selected filter.\n                 Defaults to 'preprocessed_500_best_reviews' if invalid filter provided.\n        \"\"\"\n        if rating_filter == \"best\":\n            cache_key = \"preprocessed_500_best_reviews\"\n            self._cache[cache_key]\n        elif rating_filter == \"worst\":\n            cache_key = \"preprocessed_500_worst_reviews\"\n            self._cache[cache_key]\n        elif rating_filter == \"most\":\n            cache_key = \"preprocessed_500_most_reviews\"\n            self._cache[cache_key]\n        else:\n            cache_key = \"preprocessed_500_best_reviews\"\n            self._cache[cache_key]\n            logger.warning(\n                f\"Invalid rating_filter: {rating_filter}. Using best reviews.\",\n            )\n\n        return cache_key\n\n    def get_top_recipe_ids(\n        self,\n        n: int = 50,\n        rating_filter: str | None = None,\n    ) -&gt; list[str]:\n        \"\"\"Retrieve the first n recipe IDs from the specified rating filter cache.\n\n        Args:\n            n: Number of recipe IDs to return (default: 50).\n            rating_filter: Filter type - 'best', 'worst', or 'most' reviewed recipes.\n\n        Returns:\n            list[str]: List of recipe IDs, length up to n.\n                       Defaults to best reviews if invalid filter provided.\n        \"\"\"\n        cache_key = self.switch_filter(rating_filter or \"best\")\n        recipe_ids = list(self._cache[cache_key][0:n])\n        return recipe_ids\n\n    # could use  df_total\n    # def get_reviews_for_recipes(self, recipe_ids: list[int], df_interaction: pl.DataFrame) -&gt; list[str]:\n    #     \"\"\"Retrieve all review texts for specified recipe IDs.\n\n    #     Args:\n    #         recipe_ids: List of recipe IDs to fetch reviews for.\n\n    #     Returns:\n    #         list[str]: List of review texts matching the provided recipe IDs.\n    #                    Results are cached for repeated queries.\n    #     \"\"\"\n    #     cache_key = f\"reviews_{recipe_ids[:3]!s}_{len(recipe_ids)}\"\n    #     if cache_key not in self._cache:\n    #         self._cache[cache_key] = (\n    #             df_interaction.filter(pl.col(\"recipe_id\").is_in(recipe_ids))\n    #             .select(\"review\")\n    #             .to_series()\n    #             .to_list()\n    #         )\n    #     recipe_review = list(self._cache[cache_key])\n    #     return recipe_review\n\n    def plot_word_cloud(\n        self,\n        wordcloud_nbr_word: int,\n        rating_filter: str,\n        title: str,\n    ) -&gt; Figure:\n        \"\"\"Generate a word cloud visualization from preprocessed review text.\n\n        Creates a word cloud showing the most frequent words in the selected review set.\n        Uses frequency-based word extraction (not TF-IDF).\n\n        Args:\n            wordcloud_nbr_word: Maximum number of words to display in the cloud.\n            rating_filter: Filter type - 'best', 'worst', or 'most' reviewed recipes.\n            title: Title to display on the plot.\n\n        Returns:\n            matplotlib.figure.Figure: Cached figure containing the word cloud visualization.\n                                      Returns figure with \"No text available\" if no data exists.\n        \"\"\"\n        cache_key = f\"word_cloud_{rating_filter!s}_{wordcloud_nbr_word}\"\n        texts = self._cache[self.switch_filter(rating_filter)]\n\n        if cache_key not in self._cache:\n            if not texts:\n                fig, ax = plt.subplots(figsize=(10, 5))\n                ax.text(0.5, 0.5, \"No text available\", ha=\"center\", va=\"center\")\n                ax.set_title(title)\n                self._cache[cache_key] = fig\n                return fig\n\n            word_counts: Counter[str] = Counter(texts)\n            word_freq = dict(word_counts.most_common(wordcloud_nbr_word))\n\n            fig, ax = plt.subplots(figsize=(10, 5))\n            wc = WordCloud(\n                width=800,\n                height=400,\n                background_color=\"white\",\n                max_words=wordcloud_nbr_word,\n                colormap=\"viridis\",\n            ).generate_from_frequencies(word_freq)\n\n            ax.imshow(wc, interpolation=\"bilinear\")\n            ax.axis(\"off\")\n            ax.set_title(title)\n            plt.tight_layout()\n            self._cache[cache_key] = fig\n\n        return self._cache[cache_key]\n\n    def plot_tfidf(\n        self,\n        wordcloud_nbr_word: int,\n        rating_filter: str,\n        title: str,\n    ) -&gt; Figure:\n        \"\"\"Generate a word cloud visualization using TF-IDF word extraction.\n\n        Creates a word cloud showing words with highest TF-IDF scores.\n        TF-IDF identifies words that are distinctive to the selected review set\n        rather than just most frequent.\n\n        Args:\n            wordcloud_nbr_word: Maximum number of words to display in the cloud.\n            rating_filter: Filter type - 'best', 'worst', or 'most' reviewed recipes.\n            title: Title to display on the plot.\n\n        Returns:\n            matplotlib.figure.Figure: Cached figure containing the TF-IDF word cloud.\n                                      Returns figure with \"No text available\" if no data exists.\n\n        Note:\n            Uses preprocessed (already cleaned) tokens from cache. Does NOT re-clean text.\n        \"\"\"\n        cache_key = f\"tfidf_{rating_filter!s}_{wordcloud_nbr_word}\"\n        texts = self._cache[self.switch_filter(rating_filter)]\n\n        if cache_key not in self._cache:\n            # texts is already a list of cleaned tokens, just join them into documents\n            # Group tokens into documents (every N tokens = 1 doc for TF-IDF)\n            if not texts:\n                fig, ax = plt.subplots(figsize=(10, 5))\n                ax.text(0.5, 0.5, \"No text available\", ha=\"center\", va=\"center\")\n                ax.set_title(title)\n                self._cache[cache_key] = fig\n                return fig\n\n            # Create documents by grouping tokens (since texts is already cleaned)\n            doc_size = max(1, len(texts) // 100)  # Aim for ~100 documents\n            docs = [\n                \" \".join(texts[i : i + doc_size])\n                for i in range(0, len(texts), doc_size)\n            ]\n\n            vectorizer = TfidfVectorizer(\n                max_features=wordcloud_nbr_word,\n                stop_words=\"english\",\n                ngram_range=(1, 2),\n            )\n\n            vectorizer.fit_transform(docs)\n            feature_names = vectorizer.get_feature_names_out()\n            scores = vectorizer.idf_\n            word_freq = dict(zip(feature_names, scores, strict=False))\n\n            fig, ax = plt.subplots(figsize=(10, 5))\n            wc = WordCloud(\n                width=800,\n                height=400,\n                background_color=\"white\",\n                max_words=wordcloud_nbr_word,\n                colormap=\"plasma\",\n            ).generate_from_frequencies(word_freq)\n\n            ax.imshow(wc, interpolation=\"bilinear\")\n            ax.axis(\"off\")\n            ax.set_title(title)\n            plt.tight_layout()\n            self._cache[cache_key] = fig\n        return self._cache[cache_key]\n\n    def compare_frequency_and_tfidf(\n        self,\n        recipe_count: int,\n        wordcloud_nbr_word: int,\n        rating_filter: str,\n        title: str,\n    ) -&gt; Figure:\n        \"\"\"Generate a Venn diagram comparing frequency-based vs TF-IDF word extraction.\n\n        Creates a visualization showing the overlap between top words identified by\n        raw frequency counting vs TF-IDF scoring. Helps identify which words are\n        distinctive (TF-IDF) vs merely common (frequency).\n\n        Args:\n            recipe_count: Number of recipes to analyze (currently unused in implementation).\n            wordcloud_nbr_word: Maximum features for TF-IDF vectorizer.\n            rating_filter: Filter type - 'best', 'worst', or 'most' reviewed recipes.\n            title: Title to display on the Venn diagram.\n\n        Returns:\n            matplotlib.figure.Figure: Cached figure containing the Venn diagram.\n                                      Shows top 20 words from each method and their overlap.\n        \"\"\"\n        cache_key = f\"compare_{recipe_count}_{wordcloud_nbr_word}_{title}\"\n        VENN_NBR = 20\n\n        if cache_key not in self._cache:\n            cleaned = self._cache[self.switch_filter(rating_filter)]\n            if not cleaned:\n                fig, ax = plt.subplots(figsize=(8, 8))\n                ax.text(0.5, 0.5, \"No text available\", ha=\"center\", va=\"center\")\n                ax.set_title(title)\n                self._cache[cache_key] = fig\n                return fig\n\n            # Raw frequency\n            freq_counts: Counter[str] = Counter(cleaned)\n            freq_top = {w for w, _ in freq_counts.most_common(VENN_NBR)}\n\n            # TF-IDF\n            vectorizer = TfidfVectorizer(\n                max_features=wordcloud_nbr_word,\n                stop_words=\"english\",\n            )\n            vectorizer.fit_transform(cleaned)\n            tfidf_top = set(vectorizer.get_feature_names_out()[:VENN_NBR])\n\n            fig, ax = plt.subplots(figsize=(8, 8))\n            venn2(\n                [freq_top, tfidf_top],\n                set_labels=(\"Raw Frequency\", \"TF-IDF\"),\n                set_colors=(\"skyblue\", \"salmon\"),\n                alpha=0.7,\n                ax=ax,\n            )\n            ax.set_title(title)\n\n            # Legend\n            only_freq = freq_top - tfidf_top\n            only_tfidf = tfidf_top - freq_top\n            common = freq_top &amp; tfidf_top\n\n            legend_text = (\n                f\"Only Frequency: {len(only_freq)}\\n\"\n                f\"Only TF-IDF: {len(only_tfidf)}\\n\"\n                f\"Common: {len(common)}\"\n            )\n            ax.text(0.5, -0.15, legend_text, ha=\"center\", transform=ax.transAxes)\n\n            plt.tight_layout()\n            self._cache[cache_key] = fig\n        return self._cache[cache_key]\n\n    def plot_top_ingredients(self, top_n: int = 20) -&gt; Figure:\n        \"\"\"Generate a polar plot showing the most common ingredients.\n\n        Creates a radar/polar chart visualizing ingredient frequency across recipes.\n        The radial distance represents the count of recipes using each ingredient.\n\n        Args:\n            top_n: Number of top ingredients to display (default: 20).\n\n        Returns:\n            matplotlib.figure.Figure: Cached polar plot figure showing ingredient distribution.\n                                      Returns figure with \"No ingredients found\" if no data exists.\n        \"\"\"\n        cache_key = f\"top_ingredients_{top_n}\"\n        if cache_key not in self._cache:\n            ingredients_counts = self.top_ingredients.head(top_n)\n\n            if ingredients_counts.height == 0:\n                fig, ax = plt.subplots(\n                    figsize=(8, 8),\n                    subplot_kw={\"projection\": \"polar\"},\n                )\n                ax.text(0.5, 0.5, \"No ingredients found\", ha=\"center\", va=\"center\")\n                self._cache[cache_key] = fig\n                return fig\n\n            labels = ingredients_counts[\"ingredients\"].to_list()\n            values = ingredients_counts[\"count\"].to_list()\n            angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n            values += values[:1]\n            angles += angles[:1]\n\n            fig, ax = plt.subplots(figsize=(8, 8), subplot_kw={\"projection\": \"polar\"})\n            ax.plot(angles, values, linewidth=2, color=\"blue\")\n            ax.fill(angles, values, alpha=0.3, color=\"skyblue\")\n            ax.set_xticks(angles[:-1])\n            ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n            ax.set_yticklabels([])\n            ax.set_title(f\"Top {top_n} ingredients\")\n            plt.tight_layout()\n            self._cache[cache_key] = fig\n\n        return self._cache[cache_key]\n\n    def _preprocess_word_cloud(self, wordcloud_nbr_word: int) -&gt; None:\n        categories = [\n            (\"Most reviewed recipes\", \"most\"),\n            (\"Best rated recipes\", \"best\"),\n            (\"Worst rated recipes\", \"worst\"),\n        ]\n        for _i, (title, filter_type) in enumerate(categories):\n            self.plot_word_cloud(\n                wordcloud_nbr_word,\n                filter_type,\n                f\"Frequency - {title}\",\n            )\n            self.plot_tfidf(\n                wordcloud_nbr_word,\n                filter_type,\n                f\"TF-IDF - {title}\",\n            )\n\n    def display_wordclouds(self, wordcloud_nbr_word: int) -&gt; None:\n        \"\"\"Render a Streamlit UI with 6 word cloud visualizations.\n\n        Creates a 2x3 grid showing frequency-based and TF-IDF word clouds for:\n        - Most reviewed recipes\n        - Best rated recipes\n        - Worst rated recipes\n\n        All plots are cached for instant display on subsequent renders.\n\n        Args:\n            wordcloud_nbr_word: Maximum number of words to display in each cloud.\n        \"\"\"\n        st.subheader(\"\ud83d\udde3\ufe0f WordClouds (6 charts)\")\n\n        categories = [\n            (\"Most reviewed recipes\", \"most\"),\n            (\"Best rated recipes\", \"best\"),\n            (\"Worst rated recipes\", \"worst\"),\n        ]\n\n        # 2x3 grid for the 6 wordclouds\n        for _i, (title, filter_type) in enumerate(categories):\n            st.markdown(title)\n            cols = st.columns(2)\n\n            with (\n                cols[0],\n                st.spinner(f\"Generating WordCloud (Frequency) for {title}...\"),\n            ):\n                fig = self.plot_word_cloud(\n                    wordcloud_nbr_word,\n                    filter_type,\n                    f\"Frequency - {title}\",\n                )\n                st.pyplot(fig)\n\n            with cols[1], st.spinner(f\"Generating WordCloud (TF-IDF) for {title}...\"):\n                fig = self.plot_tfidf(\n                    wordcloud_nbr_word,\n                    filter_type,\n                    f\"TF-IDF - {title}\",\n                )\n                st.pyplot(fig)\n\n    def _preprocess_comparisons(\n        self,\n        recipe_count: int,\n        wordcloud_nbr_word: int,\n    ) -&gt; None:\n        \"\"\"Pre-generate and cache all Venn diagram comparison figures.\n\n        Generates comparison visualizations for all three categories (most, best,\n        worst) to populate the cache. This speeds up subsequent display calls.\n\n        Args:\n            recipe_count: Number of recipes to analyze for comparisons.\n            wordcloud_nbr_word: Maximum features for TF-IDF vectorization.\n        \"\"\"\n        categories = [\n            (\"Most reviewed recipes\", \"most\"),\n            (\"Best rated recipes\", \"best\"),\n            (\"Worst rated recipes\", \"worst\"),\n        ]\n        for _i, (title, filter_type) in enumerate(categories):\n            self.compare_frequency_and_tfidf(\n                recipe_count,\n                wordcloud_nbr_word,\n                filter_type,\n                f\"Comparison - {title}\",\n            )\n\n    # Function to display comparisons\n    def display_comparisons(\n        self,\n        recipe_count: int,\n        wordcloud_nbr_word: int,\n    ) -&gt; None:\n        \"\"\"Render a Streamlit UI with 3 Venn diagram comparisons.\n\n        Creates Venn diagrams comparing frequency vs TF-IDF word extraction for:\n        - Most reviewed recipes\n        - Best rated recipes\n        - Worst rated recipes\n\n        Shows which words are identified by both methods (common), only frequency,\n        or only TF-IDF. All plots are cached for instant display.\n\n        Args:\n            recipe_count: Number of recipes to analyze (passed to comparison method).\n            wordcloud_nbr_word: Maximum features for TF-IDF vectorization.\n        \"\"\"\n        st.subheader(\"\ud83d\udd04 Frequency/TF-IDF Comparisons (3 charts)\")\n\n        categories = [\n            (\"Most reviewed recipes\", \"most\"),\n            (\"Best rated recipes\", \"best\"),\n            (\"Worst rated recipes\", \"worst\"),\n        ]\n\n        # 1x3 grid for the 3 comparisons\n        for _i, (title, filter_type) in enumerate(categories):\n            with st.spinner(f\"Comparison for {title}...\"):\n                fig = self.compare_frequency_and_tfidf(\n                    recipe_count,\n                    wordcloud_nbr_word,\n                    filter_type,\n                    f\"Comparison - {title}\",\n                )\n                st.pyplot(fig)\n\n    def __getstate__(self) -&gt; dict[str, Any]:\n        \"\"\"Prepare object for pickling - exclude spaCy model.\n\n        The spaCy NLP model cannot be pickled directly, so we exclude it\n        from the serialized state. It will be reloaded in __setstate__.\n\n        Returns:\n            dict[str, Any]: Object state dictionary without the spaCy model.\n        \"\"\"\n        state = self.__dict__.copy()\n        state[\"nlp\"] = None  # Don't pickle the spaCy model\n        return state\n\n    def __setstate__(self, state: dict[str, Any]) -&gt; None:\n        \"\"\"Restore object after unpickling - reload spaCy model.\n\n        Args:\n            state: The object state dictionary from pickle.\n        \"\"\"\n        self.__dict__.update(state)\n        # Reload the spaCy model with same configuration as __init__\n\n        self.nlp = None  # spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n\n    def save(self, filepath: str) -&gt; None:\n        \"\"\"Save the RecipeAnalyzer instance to disk using pickle.\n\n        Args:\n            filepath: Path where to save the analyzer (e.g., 'analyzer.pkl').\n\n        Note:\n            The spaCy model is excluded from serialization and reloaded on load.\n        \"\"\"\n        with open(filepath, \"wb\") as f:\n            pickle.dump(self, f)\n        logger.info(f\"RecipeAnalyzer saved to {filepath}\")\n\n    @staticmethod\n    def load(filepath: str) -&gt; \"RecipeAnalyzer\":\n        \"\"\"Load a RecipeAnalyzer instance from disk.\n\n        Args:\n            filepath: Path to the saved analyzer file.\n\n        Returns:\n            RecipeAnalyzer: Loaded analyzer instance with spaCy model reloaded.\n        \"\"\"\n        logger.info(f\"Loading RecipeAnalyzer from {filepath}...\")\n        with open(filepath, \"rb\") as f:\n            analyzer: RecipeAnalyzer = pickle.load(f)\n        logger.info(f\"RecipeAnalyzer loaded from {filepath}\")\n        return analyzer\n</code></pre>"},{"location":"reference/mangetamain/backend/recipe_analyzer/#mangetamain.backend.recipe_analyzer.RecipeAnalyzer.__getstate__","title":"__getstate__","text":"<pre><code>__getstate__()\n</code></pre> <p>Prepare object for pickling - exclude spaCy model.</p> <p>The spaCy NLP model cannot be pickled directly, so we exclude it from the serialized state. It will be reloaded in setstate.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Object state dictionary without the spaCy model.</p> Source code in <code>src/mangetamain/backend/recipe_analyzer.py</code> <pre><code>def __getstate__(self) -&gt; dict[str, Any]:\n    \"\"\"Prepare object for pickling - exclude spaCy model.\n\n    The spaCy NLP model cannot be pickled directly, so we exclude it\n    from the serialized state. It will be reloaded in __setstate__.\n\n    Returns:\n        dict[str, Any]: Object state dictionary without the spaCy model.\n    \"\"\"\n    state = self.__dict__.copy()\n    state[\"nlp\"] = None  # Don't pickle the spaCy model\n    return state\n</code></pre>"},{"location":"reference/mangetamain/backend/recipe_analyzer/#mangetamain.backend.recipe_analyzer.RecipeAnalyzer.__init__","title":"__init__","text":"<pre><code>__init__(df_interactions, df_recipes, df_total)\n</code></pre> <p>Initialize the RecipeAnalyzer with dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>df_interactions</code> <code>DataFrame</code> <p>DataFrame with user reviews and ratings</p> required <code>df_recipes</code> <code>DataFrame</code> <p>DataFrame with recipe details</p> required <code>df_total</code> <code>DataFrame</code> <p>Combined DataFrame with all recipe and interaction data</p> required Source code in <code>src/mangetamain/backend/recipe_analyzer.py</code> <pre><code>def __init__(\n    self,\n    df_interactions: pl.DataFrame,\n    df_recipes: pl.DataFrame,\n    df_total: pl.DataFrame,\n) -&gt; None:\n    \"\"\"Initialize the RecipeAnalyzer with dataframes.\n\n    Args:\n        df_interactions: DataFrame with user reviews and ratings\n        df_recipes: DataFrame with recipe details\n        df_total: Combined DataFrame with all recipe and interaction data\n    \"\"\"\n    # Store dataframes\n    logger.info(\"Setting up attributes\")\n\n    # Load spaCy model (disable unused components for speed)\n    logger.info(\"Loading spaCy model\")\n    self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n\n    # Initialize stop words\n    logger.info(\"Initializing stop words\")\n    self.stop_words = set(spacy.lang.en.STOP_WORDS)\n    self._extend_stop_words()\n\n    # Pre-compute top ingredients\n    logger.info(\"Computing top ingredients\")\n    self.top_ingredients = self._compute_top_ingredients(df_recipes)\n\n    # Cache for preprocessed data and figures\n    self._cache: dict[str, Any] = {}\n\n    # Pre-process the most common review sets for performance\n    logger.info(\"Preprocessed 500 best reviews\")\n    self._preprocessed_500_best_reviews(df_interactions)\n    logger.info(\"Preprocessed 500 worst reviews\")\n    self._preprocessed_500_worst_reviews(df_interactions)\n    logger.info(\"Preprocessed 500 most reviews\")\n    self._preprocessed_500_most_reviews(df_total)\n    logger.info(\"Preprocessed word cloud\")\n    self._preprocess_word_cloud(100)\n    self._preprocess_comparisons(100, 100)\n</code></pre>"},{"location":"reference/mangetamain/backend/recipe_analyzer/#mangetamain.backend.recipe_analyzer.RecipeAnalyzer.__setstate__","title":"__setstate__","text":"<pre><code>__setstate__(state)\n</code></pre> <p>Restore object after unpickling - reload spaCy model.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict[str, Any]</code> <p>The object state dictionary from pickle.</p> required Source code in <code>src/mangetamain/backend/recipe_analyzer.py</code> <pre><code>def __setstate__(self, state: dict[str, Any]) -&gt; None:\n    \"\"\"Restore object after unpickling - reload spaCy model.\n\n    Args:\n        state: The object state dictionary from pickle.\n    \"\"\"\n    self.__dict__.update(state)\n    # Reload the spaCy model with same configuration as __init__\n\n    self.nlp = None  # spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n</code></pre>"},{"location":"reference/mangetamain/backend/recipe_analyzer/#mangetamain.backend.recipe_analyzer.RecipeAnalyzer.compare_frequency_and_tfidf","title":"compare_frequency_and_tfidf","text":"<pre><code>compare_frequency_and_tfidf(\n    recipe_count, wordcloud_nbr_word, rating_filter, title\n)\n</code></pre> <p>Generate a Venn diagram comparing frequency-based vs TF-IDF word extraction.</p> <p>Creates a visualization showing the overlap between top words identified by raw frequency counting vs TF-IDF scoring. Helps identify which words are distinctive (TF-IDF) vs merely common (frequency).</p> <p>Parameters:</p> Name Type Description Default <code>recipe_count</code> <code>int</code> <p>Number of recipes to analyze (currently unused in implementation).</p> required <code>wordcloud_nbr_word</code> <code>int</code> <p>Maximum features for TF-IDF vectorizer.</p> required <code>rating_filter</code> <code>str</code> <p>Filter type - 'best', 'worst', or 'most' reviewed recipes.</p> required <code>title</code> <code>str</code> <p>Title to display on the Venn diagram.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: Cached figure containing the Venn diagram.                       Shows top 20 words from each method and their overlap.</p> Source code in <code>src/mangetamain/backend/recipe_analyzer.py</code> <pre><code>def compare_frequency_and_tfidf(\n    self,\n    recipe_count: int,\n    wordcloud_nbr_word: int,\n    rating_filter: str,\n    title: str,\n) -&gt; Figure:\n    \"\"\"Generate a Venn diagram comparing frequency-based vs TF-IDF word extraction.\n\n    Creates a visualization showing the overlap between top words identified by\n    raw frequency counting vs TF-IDF scoring. Helps identify which words are\n    distinctive (TF-IDF) vs merely common (frequency).\n\n    Args:\n        recipe_count: Number of recipes to analyze (currently unused in implementation).\n        wordcloud_nbr_word: Maximum features for TF-IDF vectorizer.\n        rating_filter: Filter type - 'best', 'worst', or 'most' reviewed recipes.\n        title: Title to display on the Venn diagram.\n\n    Returns:\n        matplotlib.figure.Figure: Cached figure containing the Venn diagram.\n                                  Shows top 20 words from each method and their overlap.\n    \"\"\"\n    cache_key = f\"compare_{recipe_count}_{wordcloud_nbr_word}_{title}\"\n    VENN_NBR = 20\n\n    if cache_key not in self._cache:\n        cleaned = self._cache[self.switch_filter(rating_filter)]\n        if not cleaned:\n            fig, ax = plt.subplots(figsize=(8, 8))\n            ax.text(0.5, 0.5, \"No text available\", ha=\"center\", va=\"center\")\n            ax.set_title(title)\n            self._cache[cache_key] = fig\n            return fig\n\n        # Raw frequency\n        freq_counts: Counter[str] = Counter(cleaned)\n        freq_top = {w for w, _ in freq_counts.most_common(VENN_NBR)}\n\n        # TF-IDF\n        vectorizer = TfidfVectorizer(\n            max_features=wordcloud_nbr_word,\n            stop_words=\"english\",\n        )\n        vectorizer.fit_transform(cleaned)\n        tfidf_top = set(vectorizer.get_feature_names_out()[:VENN_NBR])\n\n        fig, ax = plt.subplots(figsize=(8, 8))\n        venn2(\n            [freq_top, tfidf_top],\n            set_labels=(\"Raw Frequency\", \"TF-IDF\"),\n            set_colors=(\"skyblue\", \"salmon\"),\n            alpha=0.7,\n            ax=ax,\n        )\n        ax.set_title(title)\n\n        # Legend\n        only_freq = freq_top - tfidf_top\n        only_tfidf = tfidf_top - freq_top\n        common = freq_top &amp; tfidf_top\n\n        legend_text = (\n            f\"Only Frequency: {len(only_freq)}\\n\"\n            f\"Only TF-IDF: {len(only_tfidf)}\\n\"\n            f\"Common: {len(common)}\"\n        )\n        ax.text(0.5, -0.15, legend_text, ha=\"center\", transform=ax.transAxes)\n\n        plt.tight_layout()\n        self._cache[cache_key] = fig\n    return self._cache[cache_key]\n</code></pre>"},{"location":"reference/mangetamain/backend/recipe_analyzer/#mangetamain.backend.recipe_analyzer.RecipeAnalyzer.display_comparisons","title":"display_comparisons","text":"<pre><code>display_comparisons(recipe_count, wordcloud_nbr_word)\n</code></pre> <p>Render a Streamlit UI with 3 Venn diagram comparisons.</p> <p>Creates Venn diagrams comparing frequency vs TF-IDF word extraction for: - Most reviewed recipes - Best rated recipes - Worst rated recipes</p> <p>Shows which words are identified by both methods (common), only frequency, or only TF-IDF. All plots are cached for instant display.</p> <p>Parameters:</p> Name Type Description Default <code>recipe_count</code> <code>int</code> <p>Number of recipes to analyze (passed to comparison method).</p> required <code>wordcloud_nbr_word</code> <code>int</code> <p>Maximum features for TF-IDF vectorization.</p> required Source code in <code>src/mangetamain/backend/recipe_analyzer.py</code> <pre><code>def display_comparisons(\n    self,\n    recipe_count: int,\n    wordcloud_nbr_word: int,\n) -&gt; None:\n    \"\"\"Render a Streamlit UI with 3 Venn diagram comparisons.\n\n    Creates Venn diagrams comparing frequency vs TF-IDF word extraction for:\n    - Most reviewed recipes\n    - Best rated recipes\n    - Worst rated recipes\n\n    Shows which words are identified by both methods (common), only frequency,\n    or only TF-IDF. All plots are cached for instant display.\n\n    Args:\n        recipe_count: Number of recipes to analyze (passed to comparison method).\n        wordcloud_nbr_word: Maximum features for TF-IDF vectorization.\n    \"\"\"\n    st.subheader(\"\ud83d\udd04 Frequency/TF-IDF Comparisons (3 charts)\")\n\n    categories = [\n        (\"Most reviewed recipes\", \"most\"),\n        (\"Best rated recipes\", \"best\"),\n        (\"Worst rated recipes\", \"worst\"),\n    ]\n\n    # 1x3 grid for the 3 comparisons\n    for _i, (title, filter_type) in enumerate(categories):\n        with st.spinner(f\"Comparison for {title}...\"):\n            fig = self.compare_frequency_and_tfidf(\n                recipe_count,\n                wordcloud_nbr_word,\n                filter_type,\n                f\"Comparison - {title}\",\n            )\n            st.pyplot(fig)\n</code></pre>"},{"location":"reference/mangetamain/backend/recipe_analyzer/#mangetamain.backend.recipe_analyzer.RecipeAnalyzer.display_wordclouds","title":"display_wordclouds","text":"<pre><code>display_wordclouds(wordcloud_nbr_word)\n</code></pre> <p>Render a Streamlit UI with 6 word cloud visualizations.</p> <p>Creates a 2x3 grid showing frequency-based and TF-IDF word clouds for: - Most reviewed recipes - Best rated recipes - Worst rated recipes</p> <p>All plots are cached for instant display on subsequent renders.</p> <p>Parameters:</p> Name Type Description Default <code>wordcloud_nbr_word</code> <code>int</code> <p>Maximum number of words to display in each cloud.</p> required Source code in <code>src/mangetamain/backend/recipe_analyzer.py</code> <pre><code>def display_wordclouds(self, wordcloud_nbr_word: int) -&gt; None:\n    \"\"\"Render a Streamlit UI with 6 word cloud visualizations.\n\n    Creates a 2x3 grid showing frequency-based and TF-IDF word clouds for:\n    - Most reviewed recipes\n    - Best rated recipes\n    - Worst rated recipes\n\n    All plots are cached for instant display on subsequent renders.\n\n    Args:\n        wordcloud_nbr_word: Maximum number of words to display in each cloud.\n    \"\"\"\n    st.subheader(\"\ud83d\udde3\ufe0f WordClouds (6 charts)\")\n\n    categories = [\n        (\"Most reviewed recipes\", \"most\"),\n        (\"Best rated recipes\", \"best\"),\n        (\"Worst rated recipes\", \"worst\"),\n    ]\n\n    # 2x3 grid for the 6 wordclouds\n    for _i, (title, filter_type) in enumerate(categories):\n        st.markdown(title)\n        cols = st.columns(2)\n\n        with (\n            cols[0],\n            st.spinner(f\"Generating WordCloud (Frequency) for {title}...\"),\n        ):\n            fig = self.plot_word_cloud(\n                wordcloud_nbr_word,\n                filter_type,\n                f\"Frequency - {title}\",\n            )\n            st.pyplot(fig)\n\n        with cols[1], st.spinner(f\"Generating WordCloud (TF-IDF) for {title}...\"):\n            fig = self.plot_tfidf(\n                wordcloud_nbr_word,\n                filter_type,\n                f\"TF-IDF - {title}\",\n            )\n            st.pyplot(fig)\n</code></pre>"},{"location":"reference/mangetamain/backend/recipe_analyzer/#mangetamain.backend.recipe_analyzer.RecipeAnalyzer.get_top_recipe_ids","title":"get_top_recipe_ids","text":"<pre><code>get_top_recipe_ids(n=50, rating_filter=None)\n</code></pre> <p>Retrieve the first n recipe IDs from the specified rating filter cache.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of recipe IDs to return (default: 50).</p> <code>50</code> <code>rating_filter</code> <code>str | None</code> <p>Filter type - 'best', 'worst', or 'most' reviewed recipes.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of recipe IDs, length up to n.        Defaults to best reviews if invalid filter provided.</p> Source code in <code>src/mangetamain/backend/recipe_analyzer.py</code> <pre><code>def get_top_recipe_ids(\n    self,\n    n: int = 50,\n    rating_filter: str | None = None,\n) -&gt; list[str]:\n    \"\"\"Retrieve the first n recipe IDs from the specified rating filter cache.\n\n    Args:\n        n: Number of recipe IDs to return (default: 50).\n        rating_filter: Filter type - 'best', 'worst', or 'most' reviewed recipes.\n\n    Returns:\n        list[str]: List of recipe IDs, length up to n.\n                   Defaults to best reviews if invalid filter provided.\n    \"\"\"\n    cache_key = self.switch_filter(rating_filter or \"best\")\n    recipe_ids = list(self._cache[cache_key][0:n])\n    return recipe_ids\n</code></pre>"},{"location":"reference/mangetamain/backend/recipe_analyzer/#mangetamain.backend.recipe_analyzer.RecipeAnalyzer.load","title":"load  <code>staticmethod</code>","text":"<pre><code>load(filepath)\n</code></pre> <p>Load a RecipeAnalyzer instance from disk.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the saved analyzer file.</p> required <p>Returns:</p> Name Type Description <code>RecipeAnalyzer</code> <code>RecipeAnalyzer</code> <p>Loaded analyzer instance with spaCy model reloaded.</p> Source code in <code>src/mangetamain/backend/recipe_analyzer.py</code> <pre><code>@staticmethod\ndef load(filepath: str) -&gt; \"RecipeAnalyzer\":\n    \"\"\"Load a RecipeAnalyzer instance from disk.\n\n    Args:\n        filepath: Path to the saved analyzer file.\n\n    Returns:\n        RecipeAnalyzer: Loaded analyzer instance with spaCy model reloaded.\n    \"\"\"\n    logger.info(f\"Loading RecipeAnalyzer from {filepath}...\")\n    with open(filepath, \"rb\") as f:\n        analyzer: RecipeAnalyzer = pickle.load(f)\n    logger.info(f\"RecipeAnalyzer loaded from {filepath}\")\n    return analyzer\n</code></pre>"},{"location":"reference/mangetamain/backend/recipe_analyzer/#mangetamain.backend.recipe_analyzer.RecipeAnalyzer.plot_tfidf","title":"plot_tfidf","text":"<pre><code>plot_tfidf(wordcloud_nbr_word, rating_filter, title)\n</code></pre> <p>Generate a word cloud visualization using TF-IDF word extraction.</p> <p>Creates a word cloud showing words with highest TF-IDF scores. TF-IDF identifies words that are distinctive to the selected review set rather than just most frequent.</p> <p>Parameters:</p> Name Type Description Default <code>wordcloud_nbr_word</code> <code>int</code> <p>Maximum number of words to display in the cloud.</p> required <code>rating_filter</code> <code>str</code> <p>Filter type - 'best', 'worst', or 'most' reviewed recipes.</p> required <code>title</code> <code>str</code> <p>Title to display on the plot.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: Cached figure containing the TF-IDF word cloud.                       Returns figure with \"No text available\" if no data exists.</p> Note <p>Uses preprocessed (already cleaned) tokens from cache. Does NOT re-clean text.</p> Source code in <code>src/mangetamain/backend/recipe_analyzer.py</code> <pre><code>def plot_tfidf(\n    self,\n    wordcloud_nbr_word: int,\n    rating_filter: str,\n    title: str,\n) -&gt; Figure:\n    \"\"\"Generate a word cloud visualization using TF-IDF word extraction.\n\n    Creates a word cloud showing words with highest TF-IDF scores.\n    TF-IDF identifies words that are distinctive to the selected review set\n    rather than just most frequent.\n\n    Args:\n        wordcloud_nbr_word: Maximum number of words to display in the cloud.\n        rating_filter: Filter type - 'best', 'worst', or 'most' reviewed recipes.\n        title: Title to display on the plot.\n\n    Returns:\n        matplotlib.figure.Figure: Cached figure containing the TF-IDF word cloud.\n                                  Returns figure with \"No text available\" if no data exists.\n\n    Note:\n        Uses preprocessed (already cleaned) tokens from cache. Does NOT re-clean text.\n    \"\"\"\n    cache_key = f\"tfidf_{rating_filter!s}_{wordcloud_nbr_word}\"\n    texts = self._cache[self.switch_filter(rating_filter)]\n\n    if cache_key not in self._cache:\n        # texts is already a list of cleaned tokens, just join them into documents\n        # Group tokens into documents (every N tokens = 1 doc for TF-IDF)\n        if not texts:\n            fig, ax = plt.subplots(figsize=(10, 5))\n            ax.text(0.5, 0.5, \"No text available\", ha=\"center\", va=\"center\")\n            ax.set_title(title)\n            self._cache[cache_key] = fig\n            return fig\n\n        # Create documents by grouping tokens (since texts is already cleaned)\n        doc_size = max(1, len(texts) // 100)  # Aim for ~100 documents\n        docs = [\n            \" \".join(texts[i : i + doc_size])\n            for i in range(0, len(texts), doc_size)\n        ]\n\n        vectorizer = TfidfVectorizer(\n            max_features=wordcloud_nbr_word,\n            stop_words=\"english\",\n            ngram_range=(1, 2),\n        )\n\n        vectorizer.fit_transform(docs)\n        feature_names = vectorizer.get_feature_names_out()\n        scores = vectorizer.idf_\n        word_freq = dict(zip(feature_names, scores, strict=False))\n\n        fig, ax = plt.subplots(figsize=(10, 5))\n        wc = WordCloud(\n            width=800,\n            height=400,\n            background_color=\"white\",\n            max_words=wordcloud_nbr_word,\n            colormap=\"plasma\",\n        ).generate_from_frequencies(word_freq)\n\n        ax.imshow(wc, interpolation=\"bilinear\")\n        ax.axis(\"off\")\n        ax.set_title(title)\n        plt.tight_layout()\n        self._cache[cache_key] = fig\n    return self._cache[cache_key]\n</code></pre>"},{"location":"reference/mangetamain/backend/recipe_analyzer/#mangetamain.backend.recipe_analyzer.RecipeAnalyzer.plot_top_ingredients","title":"plot_top_ingredients","text":"<pre><code>plot_top_ingredients(top_n=20)\n</code></pre> <p>Generate a polar plot showing the most common ingredients.</p> <p>Creates a radar/polar chart visualizing ingredient frequency across recipes. The radial distance represents the count of recipes using each ingredient.</p> <p>Parameters:</p> Name Type Description Default <code>top_n</code> <code>int</code> <p>Number of top ingredients to display (default: 20).</p> <code>20</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: Cached polar plot figure showing ingredient distribution.                       Returns figure with \"No ingredients found\" if no data exists.</p> Source code in <code>src/mangetamain/backend/recipe_analyzer.py</code> <pre><code>def plot_top_ingredients(self, top_n: int = 20) -&gt; Figure:\n    \"\"\"Generate a polar plot showing the most common ingredients.\n\n    Creates a radar/polar chart visualizing ingredient frequency across recipes.\n    The radial distance represents the count of recipes using each ingredient.\n\n    Args:\n        top_n: Number of top ingredients to display (default: 20).\n\n    Returns:\n        matplotlib.figure.Figure: Cached polar plot figure showing ingredient distribution.\n                                  Returns figure with \"No ingredients found\" if no data exists.\n    \"\"\"\n    cache_key = f\"top_ingredients_{top_n}\"\n    if cache_key not in self._cache:\n        ingredients_counts = self.top_ingredients.head(top_n)\n\n        if ingredients_counts.height == 0:\n            fig, ax = plt.subplots(\n                figsize=(8, 8),\n                subplot_kw={\"projection\": \"polar\"},\n            )\n            ax.text(0.5, 0.5, \"No ingredients found\", ha=\"center\", va=\"center\")\n            self._cache[cache_key] = fig\n            return fig\n\n        labels = ingredients_counts[\"ingredients\"].to_list()\n        values = ingredients_counts[\"count\"].to_list()\n        angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n        values += values[:1]\n        angles += angles[:1]\n\n        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw={\"projection\": \"polar\"})\n        ax.plot(angles, values, linewidth=2, color=\"blue\")\n        ax.fill(angles, values, alpha=0.3, color=\"skyblue\")\n        ax.set_xticks(angles[:-1])\n        ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n        ax.set_yticklabels([])\n        ax.set_title(f\"Top {top_n} ingredients\")\n        plt.tight_layout()\n        self._cache[cache_key] = fig\n\n    return self._cache[cache_key]\n</code></pre>"},{"location":"reference/mangetamain/backend/recipe_analyzer/#mangetamain.backend.recipe_analyzer.RecipeAnalyzer.plot_word_cloud","title":"plot_word_cloud","text":"<pre><code>plot_word_cloud(wordcloud_nbr_word, rating_filter, title)\n</code></pre> <p>Generate a word cloud visualization from preprocessed review text.</p> <p>Creates a word cloud showing the most frequent words in the selected review set. Uses frequency-based word extraction (not TF-IDF).</p> <p>Parameters:</p> Name Type Description Default <code>wordcloud_nbr_word</code> <code>int</code> <p>Maximum number of words to display in the cloud.</p> required <code>rating_filter</code> <code>str</code> <p>Filter type - 'best', 'worst', or 'most' reviewed recipes.</p> required <code>title</code> <code>str</code> <p>Title to display on the plot.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: Cached figure containing the word cloud visualization.                       Returns figure with \"No text available\" if no data exists.</p> Source code in <code>src/mangetamain/backend/recipe_analyzer.py</code> <pre><code>def plot_word_cloud(\n    self,\n    wordcloud_nbr_word: int,\n    rating_filter: str,\n    title: str,\n) -&gt; Figure:\n    \"\"\"Generate a word cloud visualization from preprocessed review text.\n\n    Creates a word cloud showing the most frequent words in the selected review set.\n    Uses frequency-based word extraction (not TF-IDF).\n\n    Args:\n        wordcloud_nbr_word: Maximum number of words to display in the cloud.\n        rating_filter: Filter type - 'best', 'worst', or 'most' reviewed recipes.\n        title: Title to display on the plot.\n\n    Returns:\n        matplotlib.figure.Figure: Cached figure containing the word cloud visualization.\n                                  Returns figure with \"No text available\" if no data exists.\n    \"\"\"\n    cache_key = f\"word_cloud_{rating_filter!s}_{wordcloud_nbr_word}\"\n    texts = self._cache[self.switch_filter(rating_filter)]\n\n    if cache_key not in self._cache:\n        if not texts:\n            fig, ax = plt.subplots(figsize=(10, 5))\n            ax.text(0.5, 0.5, \"No text available\", ha=\"center\", va=\"center\")\n            ax.set_title(title)\n            self._cache[cache_key] = fig\n            return fig\n\n        word_counts: Counter[str] = Counter(texts)\n        word_freq = dict(word_counts.most_common(wordcloud_nbr_word))\n\n        fig, ax = plt.subplots(figsize=(10, 5))\n        wc = WordCloud(\n            width=800,\n            height=400,\n            background_color=\"white\",\n            max_words=wordcloud_nbr_word,\n            colormap=\"viridis\",\n        ).generate_from_frequencies(word_freq)\n\n        ax.imshow(wc, interpolation=\"bilinear\")\n        ax.axis(\"off\")\n        ax.set_title(title)\n        plt.tight_layout()\n        self._cache[cache_key] = fig\n\n    return self._cache[cache_key]\n</code></pre>"},{"location":"reference/mangetamain/backend/recipe_analyzer/#mangetamain.backend.recipe_analyzer.RecipeAnalyzer.save","title":"save","text":"<pre><code>save(filepath)\n</code></pre> <p>Save the RecipeAnalyzer instance to disk using pickle.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path where to save the analyzer (e.g., 'analyzer.pkl').</p> required Note <p>The spaCy model is excluded from serialization and reloaded on load.</p> Source code in <code>src/mangetamain/backend/recipe_analyzer.py</code> <pre><code>def save(self, filepath: str) -&gt; None:\n    \"\"\"Save the RecipeAnalyzer instance to disk using pickle.\n\n    Args:\n        filepath: Path where to save the analyzer (e.g., 'analyzer.pkl').\n\n    Note:\n        The spaCy model is excluded from serialization and reloaded on load.\n    \"\"\"\n    with open(filepath, \"wb\") as f:\n        pickle.dump(self, f)\n    logger.info(f\"RecipeAnalyzer saved to {filepath}\")\n</code></pre>"},{"location":"reference/mangetamain/backend/recipe_analyzer/#mangetamain.backend.recipe_analyzer.RecipeAnalyzer.switch_filter","title":"switch_filter","text":"<pre><code>switch_filter(rating_filter)\n</code></pre> <p>Select appropriate preprocessed data cache based on rating filter.</p> <p>Parameters:</p> Name Type Description Default <code>rating_filter</code> <code>str</code> <p>Filter type - 'best', 'worst', or 'most' reviewed recipes.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Cache key corresponding to the selected filter.  Defaults to 'preprocessed_500_best_reviews' if invalid filter provided.</p> Source code in <code>src/mangetamain/backend/recipe_analyzer.py</code> <pre><code>def switch_filter(self, rating_filter: str) -&gt; str:\n    \"\"\"Select appropriate preprocessed data cache based on rating filter.\n\n    Args:\n        rating_filter: Filter type - 'best', 'worst', or 'most' reviewed recipes.\n\n    Returns:\n        str: Cache key corresponding to the selected filter.\n             Defaults to 'preprocessed_500_best_reviews' if invalid filter provided.\n    \"\"\"\n    if rating_filter == \"best\":\n        cache_key = \"preprocessed_500_best_reviews\"\n        self._cache[cache_key]\n    elif rating_filter == \"worst\":\n        cache_key = \"preprocessed_500_worst_reviews\"\n        self._cache[cache_key]\n    elif rating_filter == \"most\":\n        cache_key = \"preprocessed_500_most_reviews\"\n        self._cache[cache_key]\n    else:\n        cache_key = \"preprocessed_500_best_reviews\"\n        self._cache[cache_key]\n        logger.warning(\n            f\"Invalid rating_filter: {rating_filter}. Using best reviews.\",\n        )\n\n    return cache_key\n</code></pre>"},{"location":"reference/mangetamain/utils/__init__/","title":"utils","text":"<p>Module with multiple functions and classes for general utilities.</p>"},{"location":"reference/mangetamain/utils/helper/","title":"helper","text":"<p>Helper functions for data loading with progress indication in Streamlit.</p> <p>This module provides cached data loading functions for CSV and Parquet files, as well as utilities to initialize Streamlit session state from preprocessed datasets. All functions use Polars for efficient DataFrame operations and integrate with Streamlit's caching and UI feedback mechanisms.</p>"},{"location":"reference/mangetamain/utils/helper/#mangetamain.utils.helper.custom_exception_handler","title":"custom_exception_handler","text":"<pre><code>custom_exception_handler(exception)\n</code></pre> <p>Handle exceptions with logging and user-friendly Streamlit display.</p> <p>Parameters:</p> Name Type Description Default <code>exception</code> <code>Exception</code> <p>The exception instance to handle.</p> required Note <p>This function logs the full exception traceback and displays a user-friendly error message in the Streamlit UI instead of showing the raw Python traceback.</p> Source code in <code>src/mangetamain/utils/helper.py</code> <pre><code>def custom_exception_handler(exception: Exception) -&gt; None:\n    \"\"\"Handle exceptions with logging and user-friendly Streamlit display.\n\n    Args:\n        exception: The exception instance to handle.\n\n    Note:\n        This function logs the full exception traceback and displays a\n        user-friendly error message in the Streamlit UI instead of showing\n        the raw Python traceback.\n    \"\"\"\n    import streamlit as st  # noqa: PLC0415\n\n    from mangetamain.utils.logger import get_logger  # noqa: PLC0415\n\n    logger = get_logger()\n    logger.error(f\"An error occurred: {exception}\")\n    st.error(\"An unexpected error occurred. Please contact support.\")\n</code></pre>"},{"location":"reference/mangetamain/utils/helper/#mangetamain.utils.helper.load_csv_with_progress","title":"load_csv_with_progress","text":"<pre><code>load_csv_with_progress(file_path)\n</code></pre> <p>Read a CSV file into a Polars DataFrame while showing a Streamlit spinner.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the CSV file to read.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A tuple (df, load_time) where <code>df</code> is the loaded Polars DataFrame and</p> <code>float</code> <p><code>load_time</code> is the elapsed time in seconds.</p> Source code in <code>src/mangetamain/utils/helper.py</code> <pre><code>@st.cache_data  # type: ignore[misc]\ndef load_csv_with_progress(file_path: str) -&gt; tuple[pl.DataFrame, float]:\n    \"\"\"Read a CSV file into a Polars DataFrame while showing a Streamlit spinner.\n\n    Args:\n      file_path: Path to the CSV file to read.\n\n    Returns:\n      A tuple (df, load_time) where ``df`` is the loaded Polars DataFrame and\n      ``load_time`` is the elapsed time in seconds.\n    \"\"\"\n    start_time = time.time()\n    with st.spinner(f\"Loading data from {file_path}...\"):\n        df = pl.read_csv(file_path)\n    load_time = time.time() - start_time\n    logger.info(\n        f\"Data loaded successfully from {file_path} in {load_time:.2f} seconds.\",\n    )\n    return df, load_time\n</code></pre>"},{"location":"reference/mangetamain/utils/helper/#mangetamain.utils.helper.load_data_from_parquet_and_pickle","title":"load_data_from_parquet_and_pickle","text":"<pre><code>load_data_from_parquet_and_pickle()\n</code></pre> <p>Load ALL application data ONCE and cache it globally across all users.</p> <p>This function is called once per application lifecycle. The first user will trigger the data loading (90s), but all subsequent users will get instant access (&lt;0.01s) thanks to @st.cache_resource.</p> <p>The function reads several precomputed parquet files and returns the resulting Polars DataFrames / Series as a tuple.</p> <p>Files expected under <code>data/processed/</code>: - initial_interactions.parquet - processed_interactions.parquet - initial_recipes.parquet - processed_recipes.parquet - total_nt.parquet - total.parquet - short.parquet - proportion_m.parquet - proportion_s.parquet - recipe_analyzer.pkl</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple containing:</p> <code>DataFrame</code> <ul> <li>df_interactions: Initial interactions DataFrame</li> </ul> <code>DataFrame</code> <ul> <li>df_interactions_nna: Processed interactions DataFrame</li> </ul> <code>DataFrame</code> <ul> <li>df_recipes: Initial recipes DataFrame</li> </ul> <code>DataFrame</code> <ul> <li>df_recipes_nna: Processed recipes DataFrame</li> </ul> <code>DataFrame</code> <ul> <li>df_total_nt: Total non-tagged DataFrame</li> </ul> <code>DataFrame</code> <ul> <li>df_total: Total DataFrame</li> </ul> <code>Series</code> <ul> <li>df_total_court: Short DataFrame</li> </ul> <code>Series</code> <ul> <li>proportion_m: Series of proportion_m</li> </ul> <code>RecipeAnalyzer | None</code> <ul> <li>proportion_s: Series of proportion_s</li> </ul> <code>bool</code> <ul> <li>recipe_analyzer: RecipeAnalyzer instance</li> </ul> <code>tuple[DataFrame, DataFrame, DataFrame, DataFrame, DataFrame, DataFrame, DataFrame, Series, Series, RecipeAnalyzer | None, bool]</code> <ul> <li>data_loaded: Success flag</li> </ul> Source code in <code>src/mangetamain/utils/helper.py</code> <pre><code>@st.cache_resource(show_spinner=False)  # type: ignore[misc]\ndef load_data_from_parquet_and_pickle() -&gt; tuple[\n    pl.DataFrame,\n    pl.DataFrame,\n    pl.DataFrame,\n    pl.DataFrame,\n    pl.DataFrame,\n    pl.DataFrame,\n    pl.DataFrame,\n    pl.Series,\n    pl.Series,\n    RecipeAnalyzer | None,\n    bool,\n]:\n    \"\"\"Load ALL application data ONCE and cache it globally across all users.\n\n    This function is called once per application lifecycle. The first user will\n    trigger the data loading (90s), but all subsequent users will get instant\n    access (&lt;0.01s) thanks to @st.cache_resource.\n\n    The function reads several precomputed parquet files and returns the\n    resulting Polars DataFrames / Series as a tuple.\n\n    Files expected under ``data/processed/``:\n    - initial_interactions.parquet\n    - processed_interactions.parquet\n    - initial_recipes.parquet\n    - processed_recipes.parquet\n    - total_nt.parquet\n    - total.parquet\n    - short.parquet\n    - proportion_m.parquet\n    - proportion_s.parquet\n    - recipe_analyzer.pkl\n\n    Returns:\n        Tuple containing:\n        - df_interactions: Initial interactions DataFrame\n        - df_interactions_nna: Processed interactions DataFrame\n        - df_recipes: Initial recipes DataFrame\n        - df_recipes_nna: Processed recipes DataFrame\n        - df_total_nt: Total non-tagged DataFrame\n        - df_total: Total DataFrame\n        - df_total_court: Short DataFrame\n        - proportion_m: Series of proportion_m\n        - proportion_s: Series of proportion_s\n        - recipe_analyzer: RecipeAnalyzer instance\n        - data_loaded: Success flag\n    \"\"\"\n    logger.info(\"\ud83d\udd04 Starting data load (this happens ONCE globally)...\")\n    start_time = time.time()\n\n    try:\n        # Load all parquet files with individual timing\n        df_interactions = load_parquet_with_progress(\n            \"data/processed/initial_interactions.parquet\",\n        )\n        gc.collect()  # Free memory between large loads\n\n        df_interactions_nna = load_parquet_with_progress(\n            \"data/processed/processed_interactions.parquet\",\n        )\n        gc.collect()\n\n        df_recipes = load_parquet_with_progress(\n            \"data/processed/initial_recipes.parquet\",\n        )\n        gc.collect()\n\n        df_recipes_nna = load_parquet_with_progress(\n            \"data/processed/processed_recipes.parquet\",\n        )\n        gc.collect()\n\n        df_total_nt = load_parquet_with_progress(\n            \"data/processed/total_nt.parquet\",\n        )\n        gc.collect()\n\n        df_total = load_parquet_with_progress(\n            \"data/processed/total.parquet\",\n        )\n        gc.collect()\n\n        df_total_court = load_parquet_with_progress(\n            \"data/processed/short.parquet\",\n        )\n        gc.collect()\n\n        proportion_m = load_parquet_with_progress(\n            \"data/processed/proportion_m.parquet\",\n        )[\"proportion_m\"]\n\n        proportion_s = load_parquet_with_progress(\n            \"data/processed/proportion_s.parquet\",\n        )[\"proportion_s\"]\n\n        # Load the recipe_analyzer object from pickle file\n        logger.info(\"Loading recipe_analyzer.pkl...\")\n        t = time.time()\n        try:\n            recipe_analyzer = RecipeAnalyzer.load(\n                \"data/processed/recipe_analyzer.pkl\",\n            )\n            elapsed = time.time() - t\n            logger.info(f\"\u2705 recipe_analyzer.pkl loaded in {elapsed:.2f}s\")\n        except Exception as pickle_error:\n            logger.warning(\n                f\"Failed to load pickle: {pickle_error}. Attempting fallback...\",\n            )\n            # Fallback: recreate if pickle fails (optional)\n            recipe_analyzer = None\n            elapsed = time.time() - t\n            logger.info(f\"\u26a0\ufe0f recipe_analyzer fallback in {elapsed:.2f}s\")\n\n        data_loaded = True\n        total_time = time.time() - start_time\n        logger.info(\n            f\"\u2705 ALL DATA LOADED successfully in {total_time:.2f}s \"\n            f\"(cached globally for all users)\",\n        )\n\n    except Exception as e:\n        logger.error(\n            f\"\u274c Error loading data: {e}, please run backend/dataprocessor \"\n            f\"first to initialize application data.\",\n        )\n        st.error(\n            f\"Error loading data: {e}, please run backend/dataprocessor \"\n            f\"first to initialize application data.\",\n        )\n        # Return empty data on error\n        data_loaded = False\n        df_interactions = pl.DataFrame()\n        df_interactions_nna = pl.DataFrame()\n        df_recipes = pl.DataFrame()\n        df_recipes_nna = pl.DataFrame()\n        df_total_nt = pl.DataFrame()\n        df_total = pl.DataFrame()\n        df_total_court = pl.DataFrame()\n        proportion_m = pl.Series()\n        proportion_s = pl.Series()\n        recipe_analyzer = None\n\n    return (\n        df_interactions,\n        df_interactions_nna,\n        df_recipes,\n        df_recipes_nna,\n        df_total_nt,\n        df_total,\n        df_total_court,\n        proportion_m,\n        proportion_s,\n        recipe_analyzer,\n        data_loaded,\n    )\n</code></pre>"},{"location":"reference/mangetamain/utils/helper/#mangetamain.utils.helper.load_parquet_with_progress","title":"load_parquet_with_progress","text":"<pre><code>load_parquet_with_progress(file_path)\n</code></pre> <p>Read a Parquet file into a Polars DataFrame (cached globally with zero-copy).</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the Parquet file to read.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Polars DataFrame loaded from the specified parquet file.</p> Source code in <code>src/mangetamain/utils/helper.py</code> <pre><code>@st.cache_resource(show_spinner=False)  # type: ignore[misc]\ndef load_parquet_with_progress(file_path: str) -&gt; pl.DataFrame:\n    \"\"\"Read a Parquet file into a Polars DataFrame (cached globally with zero-copy).\n\n    Args:\n      file_path: Path to the Parquet file to read.\n\n    Returns:\n      A Polars DataFrame loaded from the specified parquet file.\n    \"\"\"\n    t = time.time()\n    df = pl.read_parquet(file_path)\n    elapsed = time.time() - t\n    logger.info(f\"\u2705 {file_path} loaded in {elapsed:.2f}s - Shape: {df.shape}\")\n    return df\n</code></pre>"},{"location":"reference/mangetamain/utils/logger/","title":"logger","text":"<p>Lightweight logging utility with colored console output and file handlers.</p>"},{"location":"reference/mangetamain/utils/logger/#mangetamain.utils.logger.BaseLogger","title":"BaseLogger","text":"<p>Lightweight application logger with file and console handlers.</p> <p>BaseLogger is a small wrapper around Python's standard logging that configures a file handler and a console handler (with colors). It is implemented as a simple singleton so multiple instantiations return the same configured logger instance.</p> Source code in <code>src/mangetamain/utils/logger.py</code> <pre><code>class BaseLogger:\n    \"\"\"Lightweight application logger with file and console handlers.\n\n    BaseLogger is a small wrapper around Python's standard logging that\n    configures a file handler and a console handler (with colors).\n    It is implemented as a simple singleton so multiple instantiations\n    return the same configured logger instance.\n    \"\"\"\n\n    _instance = None  # Singleton\n\n    def __new__(cls, input_name: Path | str | None = None) -&gt; \"BaseLogger\":\n        \"\"\"Init or return the singleton logger instance.\"\"\"\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._init_logger(input_name)\n        return cls._instance\n\n    def _init_logger(self, input_name: Path | str | None = None) -&gt; None:\n        \"\"\"Initialize the internal logger, handlers and formatters.\n\n        Args:\n          input_name: Optional name used to create or locate the log\n            file (defaults to 'app').\n        \"\"\"\n        self._has_error = False\n        self.base_folder = Path(\"logs\")\n\n        self.logger = logging.getLogger(\"SystemLogger\")\n        self.logger.setLevel(logging.DEBUG)\n        self.logger.propagate = False\n\n        file_formatter = logging.Formatter(\n            fmt=\"[%(asctime)s] [%(levelname)s] [%(filename)s:%(lineno)d] %(message)s\",\n            datefmt=\"%H:%M:%S\",\n        )\n\n        console_formatter = ColoredFormatter(\n            fmt=\"[%(asctime)s] [%(levelname)s] [%(filename)s:%(lineno)d] %(message)s\",\n            datefmt=\"%H:%M:%S\",\n        )\n\n        # File handler\n        self.log_path, file_handler = self._setup_handler(input_name)\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(file_formatter)\n\n        # Console handler\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(logging.INFO)\n        console_handler.setFormatter(console_formatter)\n\n        if not self.logger.handlers:\n            self.logger.addHandler(file_handler)\n            self.logger.addHandler(console_handler)\n\n    def _setup_handler(\n        self,\n        input_name: Path | str | None = None,\n    ) -&gt; tuple[Path, logging.FileHandler]:\n        \"\"\"Create and return a file handler for application logs.\n\n        Args:\n          input_name: Optional identifier used to name the log file.\n\n        Returns:\n          A tuple containing the Path to the log file and the\n          configured file handler instance.\n        \"\"\"\n        os.makedirs(self.base_folder, exist_ok=True)\n        log_path = self.base_folder / f\"{(input_name or 'app')}.log\"\n        file_handler = logging.FileHandler(log_path, encoding=\"utf-8\")\n        return log_path, file_handler\n\n    def info(self, msg: str) -&gt; None:\n        \"\"\"Log an informational message.\n\n        Args:\n          msg: Message to log at INFO level.\n        \"\"\"\n        self.logger.info(msg, stacklevel=2)\n\n    def debug(self, msg: str) -&gt; None:\n        \"\"\"Log a debug-level message.\n\n        Args:\n          msg: Message to log at DEBUG level.\n        \"\"\"\n        self.logger.debug(msg, stacklevel=2)\n\n    def warning(self, msg: str) -&gt; None:\n        \"\"\"Log a warning-level message.\n\n        Args:\n          msg: Message to log at WARNING level.\n        \"\"\"\n        self.logger.warning(msg, stacklevel=2)\n\n    def error(self, msg: str) -&gt; None:\n        \"\"\"Log an error message and mark that an error occurred.\n\n        Args:\n          msg: Message to log at ERROR level.\n        \"\"\"\n        self._has_error = True\n        self.logger.error(msg, stacklevel=2)\n\n    def critical(self, msg: str) -&gt; None:\n        \"\"\"Log a critical-level message.\n\n        Args:\n          msg: Message to log at CRITICAL level.\n        \"\"\"\n        self.logger.critical(msg, stacklevel=2)\n\n    def get_log_path(self) -&gt; Path:\n        \"\"\"Return the current log file path.\n\n        Returns:\n          Path: Path to the active log file.\n        \"\"\"\n        return self.log_path\n\n    def has_errors(self) -&gt; bool:\n        \"\"\"Return whether an error has been logged during runtime.\n\n        Returns:\n          bool: True if any error was logged, otherwise False.\n        \"\"\"\n        return self._has_error\n</code></pre>"},{"location":"reference/mangetamain/utils/logger/#mangetamain.utils.logger.BaseLogger.__new__","title":"__new__","text":"<pre><code>__new__(input_name=None)\n</code></pre> <p>Init or return the singleton logger instance.</p> Source code in <code>src/mangetamain/utils/logger.py</code> <pre><code>def __new__(cls, input_name: Path | str | None = None) -&gt; \"BaseLogger\":\n    \"\"\"Init or return the singleton logger instance.\"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n        cls._instance._init_logger(input_name)\n    return cls._instance\n</code></pre>"},{"location":"reference/mangetamain/utils/logger/#mangetamain.utils.logger.BaseLogger.critical","title":"critical","text":"<pre><code>critical(msg)\n</code></pre> <p>Log a critical-level message.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Message to log at CRITICAL level.</p> required Source code in <code>src/mangetamain/utils/logger.py</code> <pre><code>def critical(self, msg: str) -&gt; None:\n    \"\"\"Log a critical-level message.\n\n    Args:\n      msg: Message to log at CRITICAL level.\n    \"\"\"\n    self.logger.critical(msg, stacklevel=2)\n</code></pre>"},{"location":"reference/mangetamain/utils/logger/#mangetamain.utils.logger.BaseLogger.debug","title":"debug","text":"<pre><code>debug(msg)\n</code></pre> <p>Log a debug-level message.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Message to log at DEBUG level.</p> required Source code in <code>src/mangetamain/utils/logger.py</code> <pre><code>def debug(self, msg: str) -&gt; None:\n    \"\"\"Log a debug-level message.\n\n    Args:\n      msg: Message to log at DEBUG level.\n    \"\"\"\n    self.logger.debug(msg, stacklevel=2)\n</code></pre>"},{"location":"reference/mangetamain/utils/logger/#mangetamain.utils.logger.BaseLogger.error","title":"error","text":"<pre><code>error(msg)\n</code></pre> <p>Log an error message and mark that an error occurred.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Message to log at ERROR level.</p> required Source code in <code>src/mangetamain/utils/logger.py</code> <pre><code>def error(self, msg: str) -&gt; None:\n    \"\"\"Log an error message and mark that an error occurred.\n\n    Args:\n      msg: Message to log at ERROR level.\n    \"\"\"\n    self._has_error = True\n    self.logger.error(msg, stacklevel=2)\n</code></pre>"},{"location":"reference/mangetamain/utils/logger/#mangetamain.utils.logger.BaseLogger.get_log_path","title":"get_log_path","text":"<pre><code>get_log_path()\n</code></pre> <p>Return the current log file path.</p> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path to the active log file.</p> Source code in <code>src/mangetamain/utils/logger.py</code> <pre><code>def get_log_path(self) -&gt; Path:\n    \"\"\"Return the current log file path.\n\n    Returns:\n      Path: Path to the active log file.\n    \"\"\"\n    return self.log_path\n</code></pre>"},{"location":"reference/mangetamain/utils/logger/#mangetamain.utils.logger.BaseLogger.has_errors","title":"has_errors","text":"<pre><code>has_errors()\n</code></pre> <p>Return whether an error has been logged during runtime.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if any error was logged, otherwise False.</p> Source code in <code>src/mangetamain/utils/logger.py</code> <pre><code>def has_errors(self) -&gt; bool:\n    \"\"\"Return whether an error has been logged during runtime.\n\n    Returns:\n      bool: True if any error was logged, otherwise False.\n    \"\"\"\n    return self._has_error\n</code></pre>"},{"location":"reference/mangetamain/utils/logger/#mangetamain.utils.logger.BaseLogger.info","title":"info","text":"<pre><code>info(msg)\n</code></pre> <p>Log an informational message.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Message to log at INFO level.</p> required Source code in <code>src/mangetamain/utils/logger.py</code> <pre><code>def info(self, msg: str) -&gt; None:\n    \"\"\"Log an informational message.\n\n    Args:\n      msg: Message to log at INFO level.\n    \"\"\"\n    self.logger.info(msg, stacklevel=2)\n</code></pre>"},{"location":"reference/mangetamain/utils/logger/#mangetamain.utils.logger.BaseLogger.warning","title":"warning","text":"<pre><code>warning(msg)\n</code></pre> <p>Log a warning-level message.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Message to log at WARNING level.</p> required Source code in <code>src/mangetamain/utils/logger.py</code> <pre><code>def warning(self, msg: str) -&gt; None:\n    \"\"\"Log a warning-level message.\n\n    Args:\n      msg: Message to log at WARNING level.\n    \"\"\"\n    self.logger.warning(msg, stacklevel=2)\n</code></pre>"},{"location":"reference/mangetamain/utils/logger/#mangetamain.utils.logger.ColoredFormatter","title":"ColoredFormatter","text":"<p>               Bases: <code>Formatter</code></p> <p>A logging formatter that adds ANSI color codes to console output.</p> <p>This formatter applies simple ANSI color codes to the formatted message according to the record level name (DEBUG, INFO, WARNING, ERROR, CRITICAL). It improves readability when logs are viewed in a terminal that supports ANSI escapes.</p> Source code in <code>src/mangetamain/utils/logger.py</code> <pre><code>class ColoredFormatter(logging.Formatter):\n    \"\"\"A logging formatter that adds ANSI color codes to console output.\n\n    This formatter applies simple ANSI color codes to the formatted\n    message according to the record level name (DEBUG, INFO, WARNING,\n    ERROR, CRITICAL). It improves readability when logs are viewed in\n    a terminal that supports ANSI escapes.\n    \"\"\"\n\n    COLORS: ClassVar[dict[str, str]] = {\n        \"DEBUG\": \"\\033[0;36m\",  # Cyan\n        \"INFO\": \"\\033[0;32m\",  # Green\n        \"WARNING\": \"\\033[0;33m\",  # Yellow\n        \"ERROR\": \"\\033[0;31m\",  # Red\n        \"CRITICAL\": \"\\033[0;37m\\033[41m\",  # White on Red BG\n        \"RESET\": \"\\033[0m\",  # Reset\n    }\n\n    def format(self, record: logging.LogRecord) -&gt; str:\n        \"\"\"Format the record and wrap it in color escape sequences.\n\n        Args:\n          record: The logging.Record to format.\n\n        Returns:\n          The formatted log string with ANSI color codes applied.\n        \"\"\"\n        msg = (\n            self.COLORS.get(record.levelname, self.COLORS[\"RESET\"])\n            + super().format(record)\n            + self.COLORS[\"RESET\"]\n        )\n        return msg\n</code></pre>"},{"location":"reference/mangetamain/utils/logger/#mangetamain.utils.logger.ColoredFormatter.format","title":"format","text":"<pre><code>format(record)\n</code></pre> <p>Format the record and wrap it in color escape sequences.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>LogRecord</code> <p>The logging.Record to format.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted log string with ANSI color codes applied.</p> Source code in <code>src/mangetamain/utils/logger.py</code> <pre><code>def format(self, record: logging.LogRecord) -&gt; str:\n    \"\"\"Format the record and wrap it in color escape sequences.\n\n    Args:\n      record: The logging.Record to format.\n\n    Returns:\n      The formatted log string with ANSI color codes applied.\n    \"\"\"\n    msg = (\n        self.COLORS.get(record.levelname, self.COLORS[\"RESET\"])\n        + super().format(record)\n        + self.COLORS[\"RESET\"]\n    )\n    return msg\n</code></pre>"},{"location":"reference/mangetamain/utils/logger/#mangetamain.utils.logger.RotLogger","title":"RotLogger","text":"<p>               Bases: <code>BaseLogger</code></p> <p>Logger using a rotating file handler to bound log file size.</p> <p>RotLogger uses :class:<code>logging.handlers.RotatingFileHandler</code> to keep logs from growing indefinitely by rotating them when they exceed a configured size.</p> Source code in <code>src/mangetamain/utils/logger.py</code> <pre><code>class RotLogger(BaseLogger):\n    \"\"\"Logger using a rotating file handler to bound log file size.\n\n    RotLogger uses :class:`logging.handlers.RotatingFileHandler` to\n    keep logs from growing indefinitely by rotating them when they\n    exceed a configured size.\n    \"\"\"\n\n    def _setup_handler(\n        self,\n        input_name: Path | str | None = None,\n    ) -&gt; tuple[Path, logging.FileHandler]:\n        \"\"\"Create a rotating file handler for application logs.\n\n        Args:\n          input_name: Optional identifier for the log folder.\n\n        Returns:\n          A tuple (log_path, file_handler) of the configured rotating handler.\n        \"\"\"\n        log_dir = self.base_folder / (input_name or \"app\")\n        os.makedirs(log_dir, exist_ok=True)\n        log_path = log_dir / f\"{(input_name or 'app')}.log\"\n\n        file_handler = RotatingFileHandler(\n            log_path,\n            maxBytes=5 * 1024 * 1024,\n            backupCount=5,\n            encoding=\"utf-8\",\n        )\n\n        return log_path, file_handler\n</code></pre>"},{"location":"reference/mangetamain/utils/logger/#mangetamain.utils.logger.TimeLogger","title":"TimeLogger","text":"<p>               Bases: <code>BaseLogger</code></p> <p>Logger that writes timestamped log files into a per-run folder.</p> <p>This variant of :class:<code>BaseLogger</code> creates a new timestamped file for each run inside <code>logs/&lt;input_name&gt;/</code> which is useful when keeping separate logs per execution is desired.</p> Source code in <code>src/mangetamain/utils/logger.py</code> <pre><code>class TimeLogger(BaseLogger):\n    \"\"\"Logger that writes timestamped log files into a per-run folder.\n\n    This variant of :class:`BaseLogger` creates a new timestamped file\n    for each run inside ``logs/&lt;input_name&gt;/`` which is useful when\n    keeping separate logs per execution is desired.\n    \"\"\"\n\n    def _setup_handler(\n        self,\n        input_name: Path | str | None = None,\n    ) -&gt; tuple[Path, logging.FileHandler]:\n        \"\"\"Create a timestamped file handler for a single run.\n\n        Args:\n          input_name: Optional identifier used to name the folder.\n\n        Returns:\n          A tuple (log_path, file_handler) where log_path is the Path to the\n          created log file and file_handler is the configured handler.\n        \"\"\"\n        timestamp = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n        log_dir = self.base_folder / (input_name or \"app\")\n        os.makedirs(log_dir, exist_ok=True)\n        log_path = log_dir / f\"{timestamp}.log\"\n\n        file_handler = logging.FileHandler(log_path, encoding=\"utf-8\")\n        return log_path, file_handler\n</code></pre>"},{"location":"reference/mangetamain/utils/logger/#mangetamain.utils.logger.get_logger","title":"get_logger","text":"<pre><code>get_logger()\n</code></pre> <p>Return the module-level configured logger instance.</p> <p>Returns:</p> Name Type Description <code>RotLogger</code> <code>RotLogger</code> <p>The shared logger instance used by the application.</p> Source code in <code>src/mangetamain/utils/logger.py</code> <pre><code>def get_logger() -&gt; RotLogger:\n    \"\"\"Return the module-level configured logger instance.\n\n    Returns:\n      RotLogger: The shared logger instance used by the application.\n    \"\"\"\n    return logger\n</code></pre>"}]}